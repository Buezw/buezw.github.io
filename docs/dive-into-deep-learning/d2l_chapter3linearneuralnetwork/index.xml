<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 3. Linear Neural Network on Buezwqwg</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/</link><description>Recent content in Chapter 3. Linear Neural Network on Buezwqwg</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>youremail@example.com (Buezwqwg)</managingEditor><webMaster>youremail@example.com (Buezwqwg)</webMaster><copyright>© 2025 Buezwqwg</copyright><lastBuildDate>Tue, 23 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L 3.1 Linear Regression</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/</guid><description>&lt;blockquote>
&lt;p>Last Edit 4/15/24&lt;/p>
&lt;/blockquote>
&lt;p>Regression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/feature.png"/></item><item><title>D2L 3.2 Object-Oriented Design for Implementation</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/</guid><description>&lt;p>从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/feature.png"/></item><item><title>D2L 3.3 A concise implementation of linear regression</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/</guid><description>&lt;p>本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/feature.png"/></item><item><title>D2L 3.4 Softmax Regression</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/</guid><description>&lt;p>回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/feature.png"/></item><item><title>D2L 3.5 Image classification datasets</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/</guid><description>&lt;p>MNIST数据集 (&lt;a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90"title="LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., &amp;amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324." target="_blank">LeCun &lt;em>et al.&lt;/em>, 1998&lt;/a>) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (&lt;a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189"title="Xiao, H., Rasul, K., &amp;amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747." target="_blank">Xiao &lt;em>et al.&lt;/em>, 2017&lt;/a>)。&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/feature.png"/></item><item><title>D2L 3.6 Implementation of softmax regression from scratch</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/</guid><description>&lt;h2 class="relative group">3.6.1 初始化模型参数
&lt;div id="361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;ul>
&lt;li>和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量&lt;/li>
&lt;li>在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10&lt;/li>
&lt;/ul>
&lt;h3 class="relative group">初始化权重w
&lt;div id="%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h3>
&lt;ul>
&lt;li>与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>num_inputs = 784
num_outputs = 10
W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
&lt;/code>&lt;/pre>
&lt;h2 class="relative group">3.6.2 定义softmax操作
&lt;div id="362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;ul>
&lt;li>实现softmax操作由三个步骤组成&lt;/li>
&lt;li>对每个项求幂&lt;/li>
&lt;li>对每一行求和，得到其规范化常数&lt;/li>
&lt;li>每一行除以其规范化常数，保持结果的和为1
$$softmax(X)&lt;em>{ij}=\frac{exp(X&lt;/em>{ij})}{\sum_kexp(X_{ik})}$$&lt;/li>
&lt;li>分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自&lt;a href="https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29" target="_blank">统计物理学&lt;/a>中一个模拟粒子群分布的方程&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>def softmax(X):
X_exp = torch.exp(X)
partition = X_exp.sum(1, keepdim=True)
return X_exp / partition # 这里应用了广播机制
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>keepdim=True: 在进行张量操作时，保持原始张量的维度&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/feature.png"/></item></channel></rss>