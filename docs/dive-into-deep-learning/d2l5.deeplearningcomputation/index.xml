<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>D2L 5. Deep Learning Computation on Buezwqwg</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/</link><description>Recent content in D2L 5. Deep Learning Computation on Buezwqwg</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>youremail@example.com (Buezwqwg)</managingEditor><webMaster>youremail@example.com (Buezwqwg)</webMaster><copyright>© 2025 Buezwqwg</copyright><lastBuildDate>Sun, 19 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/index.xml" rel="self" type="application/rss+xml"/><item><title>D2L 5.4 Custom Layer</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.4customlayer/</link><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.4customlayer/</guid><description>&lt;blockquote>
&lt;p>Last Edit: 1/19/25&lt;/p>
&lt;/blockquote>
&lt;p>在整体网络中，存在一些不同的层，他们都是专门用来处理不同事件的，这也令自定义层变得有必要&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.4customlayer/feature.png"/></item><item><title>D2L 5.3 Deferred Initialization</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.3deferredinitialization/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.3deferredinitialization/</guid><description>&lt;blockquote>
&lt;p>Last Edit: 1/18/25&lt;/p>
&lt;/blockquote>
&lt;p>Deferred Initialization是指模型的某些参数在模型创建时并不会立即被初始化，而是会在第一次接收到输入数据时，根据输入数据的实际形状动态地完成初始化
需要知道的是延后初始化的&lt;strong>核心目标&lt;/strong> 就是为了解决 &lt;strong>输入维度未知&lt;/strong> 的问题，而模型内部层之间的维度通常是事先定义好的&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.3deferredinitialization/feature.png"/></item><item><title>D2L 5.2 Parameter Management</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.2parametermanagement/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.2parametermanagement/</guid><description>&lt;blockquote>
&lt;p>Last Edit: 1/17/25&lt;/p>
&lt;/blockquote>
&lt;p>在训练的过程中，目标是找到使得Cost Function最小化的Parameters，而有些时候需要提取其中一层的参数检查或者移动到其他环境下，这就需要访问参数&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.2parametermanagement/feature.png"/></item><item><title>D2 5.1 Layer &amp; Block</title><link>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><author>youremail@example.com (Buezwqwg)</author><guid>https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/</guid><description>&lt;blockquote>
&lt;p>Last Edit: 12/21/24&lt;/p>
&lt;/blockquote>
&lt;h1 class="relative group">Layer 层
&lt;div id="layer-%E5%B1%82" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#layer-%E5%B1%82" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h1>
&lt;ul>
&lt;li>对于一个Layer来说，其接受一组输入（通常是矢量化的），通过调整参数后生成相应的输出&lt;/li>
&lt;li>对于一个Softmax回归，其模型本身就是一个Layer&lt;/li>
&lt;/ul>
&lt;h1 class="relative group">Block 块
&lt;div id="block-%E5%9D%97" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#block-%E5%9D%97" aria-label="Anchor">#&lt;/a>
&lt;/span>
&lt;/h1>
&lt;ul>
&lt;li>在神经网络中，Block是一种通用的抽象概念，用来描述网络中的组件，可以是一个简单的单层，也可以是由多层组成的模块，甚至是整个模型本身&lt;/li>
&lt;li>块的主要目的是对神经网络的结构进行分层抽象，方便构建和复用复杂的网络&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure>
&lt;img
class="my-0 rounded-md"
loading="lazy"
srcset="
/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/D2L5.1Layer&amp;amp;Block_hu399865873791596830.png 330w,
/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/D2L5.1Layer&amp;amp;Block_hu13772976285383848655.png 660w,
/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/D2L5.1Layer&amp;amp;Block_hu17735818070818276903.png 1024w,
/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/D2L5.1Layer&amp;amp;Block_hu13633986511892309580.png 2x"
src="https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/D2L5.1Layer&amp;amp;Block_hu13772976285383848655.png"
alt="Img"
/>
&lt;/figure>
&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/dive-into-deep-learning/d2l5.deeplearningcomputation/d2l5.1layerblock/feature.png"/></item></channel></rss>