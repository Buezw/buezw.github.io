


[{"content":"","date":"Jan 9 2025","externalUrl":null,"permalink":"/","section":"Buezwqwg","summary":"","title":"Buezwqwg","type":"page"},{"content":"","date":"Jan 9 2025","externalUrl":null,"permalink":"/tags/c/","section":"Tags","summary":"","title":"C","type":"tags"},{"content":"","date":"Jan 9 2025","externalUrl":null,"permalink":"/tags/docs/","section":"Tags","summary":"","title":"Docs","type":"tags"},{"content":"","date":"Jan 9 2025","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"https://learningc.org/cover\n","date":"Jan 9 2025","externalUrl":null,"permalink":"/docs/learning-programming-with-c/","section":"Docs","summary":"\u003cp\u003e\u003ca href=\"https://learningc.org/cover\" target=\"_blank\"\u003ehttps://learningc.org/cover\u003c/a\u003e\u003c/p\u003e","title":"Learning Programming with C","type":"docs"},{"content":"","date":"Jan 9 2025","externalUrl":null,"permalink":"/tags/lpc/","section":"Tags","summary":"","title":"LPC","type":"tags"},{"content":" Last Edit: 1/9/25\n2.1 Double data type for real numbers #\r在程序中用分数代表数字 2.1.1 Convert Inches to Centimeters #\r// Description: This program convert inches to centimeters #include \u0026lt;stdio.h\u0026gt; int main(void){ // Declare variables const double InchesToCm = 2.54; double inputInches, outputCm; // Prompt user for input printf(\u0026#34;Enter the number of inches to convert to cm: \u0026#34;); scanf(\u0026#34;%lf\u0026#34;, \u0026amp;inputInches); // Convert inches to centimeters outputCm = inputInches * InchesToCm; // Display output in 2 decimal places printf(\u0026#34;The number of centimeters is %.2lf\\n\u0026#34;, outputCm); return 0; } const是一个关键字，指示变量是常量。不能在整个代码中更改该变量 int main(void){ const double InchesToCm = 2.54; InchesToCm = 2.51; } 这样操作将会报错，因为InchesToCm是一个不可以更改的Constant\ndouble 是一种数据类型，指示变量是小数 What would happen if a number with decimal is stored in an int? 当赋值一个小数给int的时候，小数部分将被 Truncated 截断，只保留整数部分\n%lf 这是一个格式说明符，指示输入是小数 .2 表示该值应以 2 位小数打印 2.1.2 Summary #\rint：整数数据类型，Format Specifier是%d double：小数数据类型，Format Specifier是%lf 2.2 Data types and representation #\r不同的数据类型在Memory中的储存方式都不同 2.2.1 Integers #\rint使用32位存储，其中31位用于表示整数本身，一位为Sign Bit Sign bit为0是说明整数是正数，为1说明是负数 由于有整数可以有31位，其在正数的范围为0到$2^{31}-1$，在负数的范围为$-2^{31}$到-1 Other Integers Representation #\rshort：16位整数 unsigned int：使用32位，没有符号位，表示范围是0到$2^{32} - 1$ long：通常使用64位（8个字节） long long：也是使用64位（8个字节） 2.2.2. Floating point or real numbers #\rfloat的储存方法类似于科学计数法，其写成$m\\times 10^e$的形式 其中m是尾数，是一个介于1到10的数字，e是指数，表示数字的大小 Two float Representation #\rfloat使用32位，即4bytes double使用64位，即8bytes，由于精度是float的两倍，也叫Double data type双精度 2.2.3. Characters #\r要表示一个字符（如字母、符号或数字），可以使用 char 数据类型。常见的字符包括 A, B, 1, 9, @, # 等 #include \u0026lt;stdio.h\u0026gt; int main(void){ char firstInitial = \u0026#39;S\u0026#39;; printf(\u0026#34;My first initial is %c.\\n\u0026#34;, firstInitial); return 0; } The format specifier for char is %c char 类型使用8bits（1bytes）来存储每个字符 其可以于ASCII编码对应范围是0到$2^7-1$ ASCII 标准使用7位来表示字符，第8位是多余的，因此它被设置为0以兼容字节存储结构。这是因为ASCII最初设计时，只有7位用于字符表示，8位的字节结构是为了适应现代计算机的存储需求 2.2.4. Boolean #\r布尔类型用于表示逻辑值，即 true 或 false。在C语言中，true 被表示为 1，而 false 被表示为 0 尽管布尔类型只需要1个bit来表示其值，但由于内存的组织结构，每个内存单元（cell）通常存储1byte，因此布尔类型在内存中实际占用1byte #include \u0026lt;stdbool.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(void){ bool isRaining = true; printf(\u0026#34;Is it raining? %d\\n\u0026#34;, isRaining); return 0; } Boolean没有专门针对的格式说明符，采用%d来打印值 使用布尔类型时，需要包含 \u0026lt;stdbool.h\u0026gt; 库。没有这个库，编译器无法识别 bool 类型 ex. #\r假设n是正整数 bool isPositive = n \u0026gt; 0; \u0026gt; True bool isPositive = n; \u0026gt; True or False,any non-zero number is considered as `true` bool isPositive = n \u0026gt; 0 != 0; \u0026gt; True bool isPositive = n \u0026lt;= 0 != 1; \u0026gt; n\u0026lt;=0 is 0, 0 != 1 -\u0026gt; 1 or True 2.2.5. Declaring Vs. Initializing Variables #\rDeclaring Variables是告诉编译器使用某个变量。在C语言中，声明一个变量的语法是int var; 这样，编译器知道了一个类型为 int 的变量，名为 var。此时，编译器为变量保留了内存空间，但此变量尚未被赋值 变量声明后如果没有赋值，它就是Uninitialized Variables 未初始化变量，这意味着变量没有存储任何有效的值，只是占据了一块内存 如果你声明了一个变量 var，但没有给它赋值，那么它的值可能是一个随机值，例如 174739296（这只是一个示例值，实际结果因每次运行而异）。每次运行时，这个值可能会不同 #include \u0026lt;stdio.h\u0026gt; int main(void) { int var; printf(\u0026#34;Value of uninitialized variable \\\u0026#34;var\\\u0026#34;: %d\\n\u0026#34;, var); int var2 = 0; printf(\u0026#34;Value of initialized variable \\\u0026#34;var\\\u0026#34;: %d\\n\u0026#34;, var2); return 0; } 编译器会发出警告，指出未初始化的变量 var 在使用时可能会导致不确定的行为。警告信息类似于：variable ‘var’ is uninitialized when used here [-Wuninitialized] 为了避免这种警告，最佳做法是声明变量并初始化它，例如：int var = 0; 2.2.6. Taking in input from the user using scanf #\rMutiple Numbers in mutiple variables #\r#include \u0026lt;stdio.h\u0026gt; int main(void) { int num1 = 0, num2 = 0; double dnum1 = 0, dnum2 = 0; printf(\u0026#34;Enter a number: \u0026#34;); scanf(\u0026#34;%d %lf %d %lf\u0026#34;, \u0026amp;num1, \u0026amp;dnum1, \u0026amp;num2, \u0026amp;dnum2); printf(\u0026#34;Numbers entered: %d %lf %d %lf\\n\u0026#34;, num1, dnum1, num2, dnum2); return 0; } \u0026gt; 1 1.2 3 3.4 \u0026gt; Enter a number: 1 1.2 3 3.4 Numbers entered: 1 1.200000 3 3.400000 可以使用 一个 scanf 来接收多个输入，并将它们分别存储在多个变量中。输入的各个数值通过分隔符（如空格、回车或制表符）分隔 Numbers and Characters #\r#include \u0026lt;stdio.h\u0026gt; int main(void) { char idChar; int idNum; printf(\u0026#34;Enter your ID: \u0026#34;); scanf(\u0026#34;%c %d\u0026#34;, \u0026amp;idChar, \u0026amp;idNum); printf(\u0026#34;ID entered: %c%d\\n\u0026#34;, idChar, idNum); return 0; } \u0026gt; S1321234 \u0026gt; Enter your ID: S1321234 ID entered: S1321234 你可以在同一行中使用 scanf 接收字符和数字。比如，用户输入一个以字符开头，后面跟随数字的ID。 使用 %c 来接收字符，接着用 %d 来接收数字。scanf 会自动区分字符和数字，不需要在字符和数字之间添加分隔符 Take in characters and ignoring leading spaces #\r#include \u0026lt;stdio.h\u0026gt; int main(void) { char c1, c2, c3, c4, c5, c6, c7; printf(\u0026#34;Enter license plate letters and numbers: \u0026#34;); scanf(\u0026#34;%c %c %c %c %c %c %c\u0026#34;, \u0026amp;c1, \u0026amp;c2, \u0026amp;c3, \u0026amp;c4, \u0026amp;c5, \u0026amp;c6, \u0026amp;c7); printf(\u0026#34;Licence plate entered: %c%c%c%c-%c%c%c\\n\u0026#34;, c1, c2, c3, c4, c5, c6,c7); return 0; } 在这段代码中，为了忽略输入字符之间的空格，使用了 scanf 函数中的 %c 格式说明符之间加入空格的方法。这样，scanf 在读取每个字符时会自动跳过空格 Common mistake: Spaces after format specifiers #\r#include \u0026lt;stdio.h\u0026gt; int main(void) { double dnum1 = 0; printf(\u0026#34;Enter a number: \u0026#34;); scanf(\u0026#34; %lf \u0026#34;, \u0026amp;dnum1); printf(\u0026#34;Number entered: %.2lf\\n\u0026#34;, dnum1); return 0; } scanf 使用了一个格式说明符 %lf 后跟一个空格。这种情况下，程序会在接收到一个数字输入后继续等待，直到遇到非空格的输入。这是因为 scanf 的行为是读取输入直到满足格式要求，而空格在格式说明符之后会导致它等待下一个非空白字符 ","date":"Jan 9 2025","externalUrl":null,"permalink":"/docs/learning-programming-with-c/lpc2.dataoperations/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 1/9/25\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e2.1 Double data type for real numbers \r\n    \u003cdiv id=\"21-double-data-type-for-real-numbers\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#21-double-data-type-for-real-numbers\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cul\u003e\n\u003cli\u003e在程序中用分数代表数字\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e2.1.1 Convert Inches to Centimeters \r\n    \u003cdiv id=\"211-convert-inches-to-centimeters\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#211-convert-inches-to-centimeters\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// Description: This program convert inches to centimeters\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"cp\"\u003e#include\u003c/span\u003e \u003cspan class=\"cpf\"\u003e\u0026lt;stdio.h\u0026gt;\u003c/span\u003e\u003cspan class=\"cp\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"p\"\u003e){\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"c1\"\u003e// Declare variables\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e  \u003cspan class=\"k\"\u003econst\u003c/span\u003e \u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"n\"\u003eInchesToCm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e2.54\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"n\"\u003einputInches\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eoutputCm\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"c1\"\u003e// Prompt user for input\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e  \u003cspan class=\"nf\"\u003eprintf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;Enter the number of inches to convert to cm: \u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"nf\"\u003escanf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;%lf\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003einputInches\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"c1\"\u003e// Convert inches to centimeters\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e  \u003cspan class=\"n\"\u003eoutputCm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003einputInches\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eInchesToCm\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"c1\"\u003e// Display output in 2 decimal places\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e  \u003cspan class=\"nf\"\u003eprintf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;The number of centimeters is %.2lf\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eoutputCm\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003ccode\u003econst\u003c/code\u003e是一个关键字，指示变量是常量。不能在整个代码中更改该变量\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e\u003cspan class=\"p\"\u003e){\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"k\"\u003econst\u003c/span\u003e \u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"n\"\u003eInchesToCm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e2.54\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003eInchesToCm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e2.51\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cblockquote\u003e\n\u003cp\u003e这样操作将会报错，因为\u003ccode\u003eInchesToCm\u003c/code\u003e是一个不可以更改的Constant\u003c/p\u003e","title":"LPC 2. Data \u0026 Operations","type":"docs"},{"content":"","date":"Jan 9 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Last Edit: 1/8/25\n1.2 Binary representation in memory #\rBinary to Decimal Number 二进制转十进制 #\rBinary到Decimal Number的转换通过位数和值相乘得到 Decimal Number to Binary 十进制转二进制 #\rDecimal Number通过除法的余数得到二进制 Number of Bits to represent x #\r需要n个二进制位数来表达一个\\(2^n\\)的Decimal Number 要表示 256 个数字，我们需要 8 位。要表示 512 个数字，我们需要 9 位。要表示 1024 个数字，我们需要 10 位 Memory organized way 内存管理方式 #\rMemory通过Cells的方式管理，每一个Cell储存了一个byte 字节 而每一个Cell包换他的Address 地址，这使得Mmory Byte 内存字节是Byte-Addressable的 当采用32个Bits来表达Cell的Address的时候，我们可以储存\\(2^{32}\\)个Bytes 字节 A byte is a group of 8 bits. A kilobyte (KB) is 1024 bytes. A megabyte (MB) is 1024 kilobytes. A gigabyte (GB) is 1024 megabytes. A terabyte (TB) is 1024 gigabytes. \\(2^{32}\\) Bytes也就是4个Gigabytes 现代计算机是64-bits的，也就是说它们的Memory Length可以达到\\(2^{64}\\)位 Hexadecimal \u0026amp; Binary 十六进制和二进制 #\r已知Hexadecimal和Binary的对应表为 0 = 0000, 1 = 0001, 2 = 0010, 3 = 0011, 4 = 0100, 5 = 0101, 6 = 0110, 7 = 0111, 8 = 1000, 9 = 1001, A = 1010, B = 1011, C = 1100, D = 1101, E = 1110, F = 1111 一个Hexadecimal \\(c_1c_2\\)的本质为\\(16^1c_1+16^0c_2\\) 举例来说一个Hexadecimal \\(3A\\)的Decimal Number就是\\(316^1+A16^0=316+101=58\\) 更简单的Hexadecimal直接转换到Binary Number的办法就是拼接 3 转换为 0011 A 转换为 1010 将它们拼接：3A = 0011 + 1010 = 00111010 1.4 Write Simple C Programs 编写简单的 C 程序 #\r// This program prints the message \u0026#34;Hello World!\u0026#34; on the screen. ##include \u0026lt;stdio.h\u0026gt; int main(void){ printf(\u0026#34;Hello World!\\n\u0026#34;); return 0; } #include \u0026lt;stdio.h\u0026gt;允许访问与输入（如键盘）和输出（如监视器）设备接口的功能。这些函数包括 和 。printf``scanf main 是 C 程序的入口点。所有 C 程序都需要 main 函数 在执行程序时调用。它返回一个整数值。该值表示程序执行成功。任何其他值都表示程序失败 printf 是将字符串打印到屏幕的函数 \\ is called an escape character 转义字符 ，\\n is a special character that indicates a new line Input 输入 #\r##include \u0026lt;stdio.h\u0026gt; int main(void){ int numPizzas, numSlices; printf(\u0026#34;How many pizzas do you have?\\n\u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;numPizzas); numSlices = numPizzas * 8; printf(\u0026#34;You have %d slices in %d pizza.\\n\u0026#34;, numSlices, numPizzas); return 0; } int numPizzas, numSlices;声明两个类型的变量int is a data type that represents integers scanf(\u0026quot;%d\u0026quot;, \u0026amp;numPizzas);将获取用户输入并将其分配给 variable \u0026amp; Address-of Operator（取地址符） 是为了将变量的地址传递给 scanf pass-by-value 按值传递 #\r在 C 语言中，函数的参数传递默认是按值传递（pass-by-value）。这意味着：\n当你调用一个函数时，传递的实际上是变量值的副本，而不是变量本身。 因此，如果不通过地址传递，函数无法直接修改原始变量的值。 Escape Sequences 转义序列 #\r转义字符是由反斜杠 \\ 开头的一组特殊字符，用于表示一些特殊含义。 \\n 表示换行。 \\t 表示制表符。 \\\\ 表示反斜杠本身。 \\\u0026quot; 表示双引号。 ","date":"Jan 8 2025","externalUrl":null,"permalink":"/docs/learning-programming-with-c/lpc1.introtoprogrammingcomputers/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 1/8/25\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e1.2 Binary representation in memory \r\n    \u003cdiv id=\"12-binary-representation-in-memory\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#12-binary-representation-in-memory\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eBinary to Decimal Number 二进制转十进制 \r\n    \u003cdiv id=\"binary-to-decimal-number-%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%BD%AC%E5%8D%81%E8%BF%9B%E5%88%B6\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#binary-to-decimal-number-%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%BD%AC%E5%8D%81%E8%BF%9B%E5%88%B6\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003eBinary到Decimal Number的转换通过位数和值相乘得到\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /docs/learning-programming-with-c/lpc1.introtoprogrammingcomputers/LPC1.IntrotoProgrammingComputers_hu4440032987239514809.png 330w,\r\n        /docs/learning-programming-with-c/lpc1.introtoprogrammingcomputers/LPC1.IntrotoProgrammingComputers_hu15081779412421026257.png 660w,\r\n        /docs/learning-programming-with-c/lpc1.introtoprogrammingcomputers/LPC1.IntrotoProgrammingComputers_hu17787250618502931940.png 1024w,\r\n        /docs/learning-programming-with-c/lpc1.introtoprogrammingcomputers/LPC1.IntrotoProgrammingComputers_hu12766134372818378964.png 2x\"\r\n        src=\"/docs/learning-programming-with-c/lpc1.introtoprogrammingcomputers/LPC1.IntrotoProgrammingComputers_hu15081779412421026257.png\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"LPC 1. Intro to Programming Computers","type":"docs"},{"content":"","date":"Dec 20 2024","externalUrl":null,"permalink":"/tags/computer-science/","section":"Tags","summary":"","title":"Computer Science","type":"tags"},{"content":"","date":"Dec 20 2024","externalUrl":null,"permalink":"/series/d2l/","section":"Series","summary":"","title":"D2L","type":"series"},{"content":"","date":"Dec 20 2024","externalUrl":null,"permalink":"/tags/d2l/","section":"Tags","summary":"","title":"D2L","type":"tags"},{"content":" Last Edit: 12/20/24\n“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”\nPerceptron 感知机 #\r一种单层神经网络模型，用于Binary Classification $$o = \\sigma\\left(\\langle w, x \\rangle + b\\right)~~~~ \\sigma(x) = \\begin{cases} 1 \u0026amp; \\text{if } x \u0026gt; 0 \\ -1 \u0026amp; \\text{otherwise} \\end{cases}$$ Binary Classification 二分类问题 #\r两个可能的值的问题，例如「正类」（1）和「负类」（0） Training 训练 #\rinitialize w = 0 and b = 0\rrepeat\rfor each (xi, yi) in the training data:\rif yi * (⟨w, xi⟩ + b) ≤ 0 then\rw ← w + yi * xi\rb ← b + yi\rend if\rend for\runtil all points are classified correctly initialize w = 0 and b = 0：初始化weight和bias if yi * (⟨w, xi⟩ + b) ≤ 0：如果分类与预测不符 在Perceptron中并没有明确的Optimize Method，但可以隐式定义一个仅与分类错误的点有关的数据的损失，也就是上面小于零情况下的 $$L(w, b) = -\\sum_{x_i \\in M} y_i (w \\cdot x_i + b)$$ 由于\\(y_i (w \\cdot x_i + b)\\)本身是负的，取负之后，这部分损失就变成了正 这意味着误分类样本对损失的贡献是增加的，因为我们希望最小化正的损失值 而对于weight和bias分别的Gradient为 $$\\nabla_w L(w, b) = -\\sum_{x_i \\in M} y_i x_i$$ $$\\nabla_b L(w, b) = -\\sum_{x_i \\in M} y_i$$ 对应了伪代码中的w ← w + yi * xi 与 b ← b + yi 完整代码如下 import numpy as np w = np.zeros(2) b = 0.0 n_epoch = 11 X = np.array([ [0.5, 1.5], [1.0, 1.0], [1.5, 0.5], [2.0, 1.0], [2.5, 1.5], [3.0, 3.0], [3.5, 3.5], [4.0, 4.5], [4.5, 5.0], [5.0, 5.5]]) y = np.array([-1, -1, -1, -1, -1, 1, 1, 1, 1, 1]) for epoch in range(n_epochs): for i in range(len(X)): if y[i] * (np.dot(w, X[i]) + b) \u0026lt;= 0: w += y[i] * X[i] b += y[i] else: continue def predict(X, w, b): return np.sign(np.dot(X, w) + b) predictions = predict(X, w, b) print(\u0026#34;Predictions:\u0026#34;, predictions) print(\u0026#34;Actual labels:\u0026#34;, y) \u0026gt; Predictions: [-1. -1. -1. -1. -1. 1. 1. 1. 1. 1.] \u0026gt; Actual labels: [-1 -1 -1 -1 -1 1 1 1 1 1] XOR Problem #\rXOR（异或）逻辑门是一个二输入逻辑门，其输出只在两个输入不同时为1（即当输入是(0,1)或(1,0)时）。其逻辑如下：\n0 XOR 0 = 0\n0 XOR 1 = 1\n1 XOR 0 = 1\n1 XOR 1 = 0\n线性模型，如感知机，是基于线性方程的，它试图找到一个权重向量和偏差，以便通过一个超平面来分割数据点。\n对于 XOR 问题，无论如何调整线性模型的参数，都无法得到一个能够将这四个点分开的单一直线\n为了解决XOR问题，我们可以使用非线性模型。最常见的方法是使用神经网络，尤其是多层感知机（MLP）。通过添加一个或多个隐藏层，神经网络能够学习非线性函数\n单调假设与线性模型的局限性 #\r单调假设：在一个线性模型中，特征与输出之间的关系是单调的 线性模型的局限：虽然线性模型简单且易于理解，但很多现实世界的关系是非线性的 Hidden Layer 隐藏层 #\r我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制 对于线性网络来说，每一层都是线性的Affine transformation 仿射变换 $$H = XW^{(1)} + b^{(1)}, O = HW^{(2)} + b^{(2)}$$\n这样即使构造了多层的模型，其实际上还是只等于一个Affine Transformation $$O = (XW^{(1)} + b^{(1)})W^{(2)} + b^{(2)} = XW^{(1)}W^{(2)} + b^{(1)}W^{(2)} + b^{(2)} = XW + b$$\n而为了发挥多层框架的潜力，就需要在Affine Transformation后应用一个Non-Linear的Activation Function激活Output\n一般来说在Activation之后便不能将其退化为Linear Model $$H = \\sigma(XW^{(1)} + b^{(1)}), O = HW^{(2)} + b^{(2)}$$\n为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 一层叠一层，从而产生更有表达能力的模型\nActivation Function 激活函数 #\rActivation function 通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的 通过加入了更“DEEP”的层数，MLP理论可以拟合任意连续函数 Weierstrass Approximation Theorem #\r在知道了Weierstrass Approximation Theorem后，也就是证明该 $$B_n(x) = \\sum_{i=0}^n f\\left(\\frac{i}{n}\\right) \\binom{n}{i} x^i (1-x)^{n-i}$$ Bernstein Polynomial，\\(B_n(f, x)\\)在区间\\([0, 1]\\)上以任意精度逼近\\(f(x)\\) 通过说明MLP如何通过从Activation Function构造Polynomial，最终证明MLP如何实现函数的理论任意精度逼近 Activation Function that has Tyler Series #\rWeierstrass Approximation Theorem指出，任意定义在闭区间 \\([a, b]\\)上的连续函数\\(f\\)都可以被多项式函数以任意精度逼近。即，对于任意\\(\\varepsilon \u0026gt; 0\\)，存在一个多项式\\(P(x)\\)，使得 $$|f(x) - P(x)| \u0026lt; \\varepsilon \\quad \\forall x \\in [a, b]$$ 为了证明单隐层神经网络能够逼近任意多项式，我们考虑如下多项式： $$P(x) = \\sum_{k=0}^n a_k x^k$$ 其中\\(a_k\\)是多项式系数，n是多项式的次数。 目标是要构造一个单隐层神经网络\\(F(x) = \\sum_{j=1}^m \\alpha_j \\sigma(w_j x + b_j)\\)，使得\\(F(x)\\)能够逼近\\(P(x)\\)以任意精度 选择合适的非线性激活函数是关键。假设\\(\\sigma\\)在某个区间内具有泰勒展开： $$\\sigma(z) = \\sum_{k=0}^\\infty c_k z^k$$ 其中\\(c_k\\)是泰勒级数的系数。典型的激活函数如 sigmoid、tanh 等都满足在某个区间内可展开为幂级数 $$\\sigma(x) = \\frac{1}{2} + \\frac{x}{4} - \\frac{x^3}{48} + \\frac{x^5}{480} + \\cdots$$ $$\\tanh(x) = x - \\frac{x^3}{3} + \\frac{2x^5}{15} - \\frac{17x^7}{315} + \\cdots$$ 由于\\(P(x)\\)是多项式，我们需要构造网络的输出\\(F(x)\\)来逼近\\(P(x)\\)具体步骤如下： 对于每个高阶项\\(x^k\\)，利用激活函数的非线性性质，通过组合多个隐藏单元来逼近。具体来说，可以通过调整\\(w_j\\)和\\(b_j\\)，使得多个\\(\\sigma(w_j x + b_j)\\)的组合能够近似\\(x^k\\) 一个二次多项式的例子便是 $$F(x) = \\underbrace{\\sigma(b_1) \\cdot \\alpha_1}_{\\text{常数项}} + \\underbrace{\\sigma(w_2x + b_2) \\cdot \\alpha_2}_{\\text{线性项}} + \\underbrace{\\sigma(w_{3,1}x + b_{3,1}) \\cdot \\alpha_{3,1} + \\sigma(w_{3,2}x + b_{3,2}) \\cdot \\alpha_{3,2}}_{\\text{二次项}}$$\r由于多项式是各阶项的线性组合，单隐层网络通过线性组合隐藏层的输出即可实现对多项式的逼近 $$F(x) = \\sum_{j=1}^m \\alpha_j \\sigma(w_j x + b_j) \\approx \\sum_{k=0}^n a_k x^k = P(x)$$ 上述证明假设激活函数\\(\\sigma\\)能够通过适当组合逼近多项式项。某些激活函数（如ReLU）虽然非多项式，但由于其分段线性性质，也具备强大的逼近能力。 Activation Function that doesn\u0026rsquo;t have Tyler Series #\r首先要说明的就是上面所提到的那句话，\u0026ldquo;如果微妙的边界条件很重要，我们很可能是在研究数学而非工程.\u0026rdquo; ReLU函数定义为： $$\\sigma(z) = \\max(0, z)$$ $$\\sigma(z) = \\begin{cases} 0, \u0026amp; z \\leq 0 \\ z, \u0026amp; z \u0026gt; 0 \\end{cases}$$ 分段线性函数能够在不同的区间内表现出不同的线性特征 这种特性允许神经网络通过组合多个ReLU单元，在输入空间中划分出多个线性区域，每个区域内的网络输出都是一个线性函数 通过增加隐藏单元数，可以在输入空间中创建更多的线性区间，从而逼近复杂的非线性函数 $$F(x) = \\sum_{j=1}^m \\alpha_j \\sigma(w_j x + b_j) = \\sum_{j=1}^m \\alpha_j \\max(0, w_j x + b_j)$$ 每个隐藏单元\\(\\sigma(w_j x + b_j)\\)在\\(w_j x + b_j = 0\\)处产生一个“折点”，即输入\\(x = -\\frac{b_j}{w_j}\\)处 通过调整不同单元的权重\\(w_j\\)和偏置\\(b_j\\)，可以在输入空间中创建多个折点，将输入空间划分为多个线性区间。 5分钟理解激活函数让神经网络能拟合任何函数 - 知乎\n","date":"Dec 20 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1multilayerperceptron/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 12/20/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”\u003c/p\u003e","title":"D2L 4.1 Multilayer Perceptron","type":"docs"},{"content":" Last Edit: 12/20/24\n使用纯MLP参加https://www.kaggle.com/competitions/titanic的Competition\n# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here\u0026#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \u0026#34;../input/\u0026#34; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(\u0026#39;/kaggle/input/d/heptapod/titanic/train_and_test2.csv\u0026#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \u0026#34;Save \u0026amp; Run All\u0026#34; # You can also write temporary files to /kaggle/temp/, but they won\u0026#39;t be saved outside of the current session train_path = \u0026#39;/kaggle/input/titanic/train.csv\u0026#39; test_path = \u0026#39;/kaggle/input/titanic/test.csv\u0026#39; train_data = pd.read_csv(train_path) test_data = pd.read_csv(test_path) data = pd.concat([train_data, test_data], sort=False).reset_index(drop=True) display(data) # 填补Age的缺失值 data[\u0026#39;Age\u0026#39;].fillna(data[\u0026#39;Age\u0026#39;].median(), inplace=True) # 填补Fare的缺失值 data[\u0026#39;Fare\u0026#39;].fillna(data[\u0026#39;Fare\u0026#39;].median(), inplace=True) display(data[\u0026#39;Fare\u0026#39;]) data = data[[\u0026#39;Survived\u0026#39;,\u0026#39;Pclass\u0026#39;,\u0026#39;Sex\u0026#39;,\u0026#39;Age\u0026#39;,\u0026#39;SibSp\u0026#39;,\u0026#39;Parch\u0026#39;,\u0026#39;Fare\u0026#39;]] data[\u0026#39;Sex\u0026#39;] = data[\u0026#39;Sex\u0026#39;].map({\u0026#39;male\u0026#39;: 0, \u0026#39;female\u0026#39;: 1}) print(data) train_data = data.iloc[:891].copy() test_data = data.iloc[891:].copy() X_train = train_data.drop(\u0026#39;Survived\u0026#39;, axis=1) y_train = train_data[\u0026#39;Survived\u0026#39;].astype(int) X_test = test_data.drop(\u0026#39;Survived\u0026#39;, axis=1).copy() print(X_test) from sklearn.neural_network import MLPClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, classification_report # 将训练集分为训练子集和验证子集 X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42) # 初始化MLPClassifier mlp = MLPClassifier(hidden_layer_sizes=(100,), # 一个隐藏层，100个神经元 activation=\u0026#39;relu\u0026#39;, # 激活函数为ReLU solver=\u0026#39;adam\u0026#39;, # 优化器为Adam max_iter=1000, # 最大迭代次数 random_state=42) # 训练模型 mlp.fit(X_tr, y_tr) # 在验证集上进行预测 y_pred = mlp.predict(X_val) # 计算准确率 accuracy = accuracy_score(y_val, y_pred) print(f\u0026#34;\\n验证集准确率：{accuracy:.4f}\u0026#34;) # 查看分类报告 print(\u0026#34;\\n分类报告：\u0026#34;) print(classification_report(y_val, y_pred)) y_test = mlp.predict(X_test) result = mlp.predict(X_test) X_test[\u0026#39;Survived\u0026#39;] = result passenger_ids = np.arange(891, 1309) X_test[\u0026#39;Passengerid\u0026#39;] = passenger_ids X_test = X_test[\u0026#39;Survived\u0026#39;] print(X_test) X_test.to_csv(\u0026#39;submission.csv\u0026#39;, index=False) print(\u0026#34;提交文件 \u0026#39;submission.csv\u0026#39; 已生成。\u0026#34;) ","date":"Dec 20 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2exampleofmlp/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 12/20/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e使用纯MLP参加https://www.kaggle.com/competitions/titanic的Competition\u003c/p\u003e","title":"D2L 4.2 Example of MLP","type":"docs"},{"content":"","date":"Dec 20 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" Last Edit: 12/19/24\nWeierstrass Approximation Theorem #\r每一个定义在闭区间\\([a,b]\\)上的实值连续函数都可以被多项式序列在整个区间上一致逼近。 换句话说，给定任意的连续函数\\(f: [a, b] \\to \\mathbb{R}\\)和任意小的正数\\(\\epsilon\\)，都存在一个多项式\\(P(x)\\)，使得对所有\\(x \\in [a, b]\\)都有\\(|f(x) - P(x)| \u0026lt; \\epsilon\\) Bernstein\u0026rsquo;s Proof 1912 #\r采用离散的Convolution $$f(x)\\approx\\sum^n_{i=0}f(x_i)w(x_i)$$ 其满足\\(\\sum_i(x_i)=1\\)，离\\(x\\)越近的地方\\(w(x_i)\\)越大 Binomial Distribution 二项分布 #\r一种离散概率分布，用于模型在固定次数的独立试验中每次试验成功的次数 其质量概率函数为 $$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$ p：单次独立事件的成功概率 k：实验中事件成功的次数 n：实验的总事件的数量 Interpretation #\r这样理解，先不管\\(\\binom{n}{k}\\)，假设一个成功率为\\(60%\\)的事件，其总共实验次数为5次，也就是\\(p=0.6,n=5\\) 现在当\\(k=5\\)的时候，Binomial Distribution表示的概率为\\(0.6^5\\)，也就是说对于一个概率为0.6的事件，其独立测试五次后都成功的概率为\\(0.6^5\\)，这就是最简单的概率 当\\(k=3\\)时，概率质量函数为 $$\\binom{5}{3} 0.6^3 (1-0.6)^{5-3}$$ 也就是说，5次实验，每一个5次实验中3次成功的概率为\\(0.6^3 (1-0.6)^{5-3}\\) 而在5次实验中这些成功的和失败的实验都可能出现在不同的位置，而这些中的成功的事件的位置可以是 $$123,124,125,134,135,145,234,235,245,345$$ 这10种情况，也就是出现5次中3次的会有10中情况，所以乘以10 Bernstein Polynomial 伯恩斯坦多项式 #\r$$B_n(x) = \\sum_{i=0}^n f\\left(\\frac{i}{n}\\right) \\binom{n}{i} x^i (1-x)^{n-i}$$\n用加权平均的方式（基于二项分布）生成新的多项式\\(B_n(x)\\)，作为\\(f(x)\\)的近似，实际上就是一个离散的Convolution Similarity to Convolution #\r$$(f * g)(x) = \\sum_{k} f(k) g(x-k)$$\n可以发现两者的区别就在于Bernstein Poly的Weight是基于Binomial Distribution的 并且采样点不再是连续的输入而是离散且固定的值 Expectation #\r$$B_n(x) = \\mathbb{E}\\left[f\\left(\\frac{X}{n}\\right)\\right]$$\n最终可以得到Bernstein Polynomial的期望值在\\(n\\rightarrow \\infty\\)的情况下是就是\\(f(x)\\) 也就是说可以通过一致收敛性，说明\\(B_n(f, x)\\)在区间\\([0, 1]\\)上以任意精度逼近\\(f(x)\\) ","date":"Dec 19 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/weierstrassapproximation/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 12/19/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eWeierstrass Approximation Theorem \r\n    \u003cdiv id=\"weierstrass-approximation-theorem\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#weierstrass-approximation-theorem\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e每一个定义在闭区间\\([a,b]\\)上的实值连续函数都可以被多项式序列在整个区间上一致逼近。\u003c/li\u003e\n\u003cli\u003e换句话说，给定任意的连续函数\\(f: [a, b] \\to \\mathbb{R}\\)和任意小的正数\\(\\epsilon\\)，都存在一个多项式\\(P(x)\\)，使得对所有\\(x \\in [a, b]\\)都有\\(|f(x) - P(x)| \u0026lt; \\epsilon\\)\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eBernstein\u0026rsquo;s Proof 1912 \r\n    \u003cdiv id=\"bernsteins-proof-1912\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#bernsteins-proof-1912\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e采用离散的Convolution\n$$f(x)\\approx\\sum^n_{i=0}f(x_i)w(x_i)$$\u003c/li\u003e\n\u003cli\u003e其满足\\(\\sum_i(x_i)=1\\)，离\\(x\\)越近的地方\\(w(x_i)\\)越大\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eBinomial Distribution 二项分布 \r\n    \u003cdiv id=\"binomial-distribution-%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#binomial-distribution-%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e一种离散概率分布，用于模型在固定次数的独立试验中每次试验成功的次数\u003c/li\u003e\n\u003cli\u003e其质量概率函数为\n$$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\u003c/li\u003e\n\u003cli\u003ep：单次独立事件的成功概率\u003c/li\u003e\n\u003cli\u003ek：实验中事件成功的次数\u003c/li\u003e\n\u003cli\u003en：实验的总事件的数量\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eInterpretation \r\n    \u003cdiv id=\"interpretation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#interpretation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e这样理解，先不管\\(\\binom{n}{k}\\)，假设一个成功率为\\(60%\\)的事件，其总共实验次数为5次，也就是\\(p=0.6,n=5\\)\u003c/li\u003e\n\u003cli\u003e现在当\\(k=5\\)的时候，Binomial Distribution表示的概率为\\(0.6^5\\)，也就是说对于一个概率为0.6的事件，其独立测试五次后都成功的概率为\\(0.6^5\\)，这就是最简单的概率\u003c/li\u003e\n\u003cli\u003e当\\(k=3\\)时，概率质量函数为\n$$\\binom{5}{3} 0.6^3 (1-0.6)^{5-3}$$\u003c/li\u003e\n\u003cli\u003e也就是说，5次实验，每一个5次实验中3次成功的概率为\\(0.6^3 (1-0.6)^{5-3}\\)\u003c/li\u003e\n\u003cli\u003e而在5次实验中这些成功的和失败的实验都可能出现在不同的位置，而这些中的成功的事件的位置可以是\n$$123,124,125,134,135,145,234,235,245,345$$\u003c/li\u003e\n\u003cli\u003e这10种情况，也就是出现5次中3次的会有10中情况，所以乘以10\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eBernstein Polynomial 伯恩斯坦多项式 \r\n    \u003cdiv id=\"bernstein-polynomial-%E4%BC%AF%E6%81%A9%E6%96%AF%E5%9D%A6%E5%A4%9A%E9%A1%B9%E5%BC%8F\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#bernstein-polynomial-%E4%BC%AF%E6%81%A9%E6%96%AF%E5%9D%A6%E5%A4%9A%E9%A1%B9%E5%BC%8F\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e$$B_n(x) = \\sum_{i=0}^n f\\left(\\frac{i}{n}\\right) \\binom{n}{i} x^i (1-x)^{n-i}$$\u003c/p\u003e","title":"D2L Weierstrass Approximation Theorem","type":"docs"},{"content":"","date":"Dec 15 2024","externalUrl":null,"permalink":"/tags/ma/","section":"Tags","summary":"","title":"MA","type":"tags"},{"content":" Last Edit: 12/15/24\nReal Number 实数 #\rRational Number 有理数 #\r整数，有限位小数，无限循环小数，分数 只要是能被表达为 $$\\frac{p}{q},p,q\\in \\mathbb z,q\\neq 0$$ 的数都叫做Rational Number 也就是说可以被任意两个Nature Number通过加减乘除所得到的数都被称为Rational Number（做除法的时候分母不能为零） Irrational Number 无理数 #\rPythagoras Theorem 毕达哥拉斯定理 #\r在一个直角三角形中，直角边对面的斜边（最长边）的平方等于两个直角边的平方和 Contradiction #\r在当时并没有Irrational Number的定义，但是表示一个两个直角边长度为一的直角三角形的斜边的时候却出现了问题，即\\(1^2+1^2=x^2\\)，无法通过一个Rational Number，也就是两个Nature Number的任意四则运算求出这个x Proof that sqrt 2 isn\u0026rsquo;t Rational Number #\rProof By Contradiction 假设\\(\\sqrt 2\\)是一个Rational Number，则有\\(2=(\\frac{p}{q})^2\\)，其中p和q是互素的 即\\(p^2=2q^2\\)，已知\\(2q^2\\)为一个Even Number，则等号另一边的\\(p\\)也必为一个Even Number（Odd Number的平方为Odd Number） 既然p是一个Even Number，则他可以被2整除，即\\(p^2\\)可以被4整除，同理可以得到\\(2|q^2\\) 那既然p和q都是偶数，很明显他们不可能互素，Contradict，故假设不成立 于是证明了Irrational Number的存在 Define Real Number #\r首先要知道的是，根号的本质是一个服务幂而创造出的代数运算，其能表达出的Irrational Number的个数几乎可以忽略不计 所以定义实数的第一步就是构造出所有Irrational Number，第二步则是定义全序列关系，第三步为定义代数运算，第四步研究拓扑结构（稠密性） Dedekind Cut #\r设数集的一个划分\\({\\alpha,\\beta }\\)，其中\n\\(\\alpha,\\beta\\neq \\emptyset\\)，即两个划分必须为有元素\n向下封闭：\\(\\forall x,y\\in k, x\u0026lt;y,y\\in \\alpha \\Rightarrow x\\in \\alpha\\)\n\\(\\alpha\\)中无最大元素：\\(\\forall x\\in \\alpha,\\exists y\\in \\alpha~ st.~y\u0026gt;x\\)\n满足以上条件的Cut则称为k上的一个Dedekind Cut，记做\\(\\alpha|\\beta\\) ，其中\\(\\alpha,\\beta\\)分别称为Dedekind Cut的Lower Set和Upper Set\n每一个Dedekind Cut都确定了一个Real Number，其为一个存在无限过程的集合，具体来说有对于一个Set，其无最大元素的定义便是一个无限的过程，所以即使Dedekind Cut的Lower Set是一个集合，其实际上表示的是一个Real Number\n再次对于上面的\\(x^2=2\\)做分析，假设其正根为\\(x_0\\)，令 $$\\alpha = {a \\in \\mathbb{Q} : a \u0026lt; x_0}~~~~~ \\beta = {a \\in \\mathbb{Q} : a \u0026gt; x_0}$$ ![[MA2.RealNumber.png]]\n则集合\\(\\alpha\\)便就是一个表达\\(\\sqrt2\\)的方法\nReal Number Set Definition #\r有理数集\\(\\mathbb Q\\)上的所有Dedekind Cut的Lower Set的Set称为Set of Real Numbers，记做\\(\\mathbb R\\) 其中的每一个Dedekind Cut的Lower Set表示一个Real Number Sequence Relationship #\r定义了Real Number后，需要将他们排列，具体来说需要将由Dedekind Cut所确定的Lower Sets做排列，有 $$\\alpha_1\\leq\\alpha_2=\\alpha1\\subseteq\\alpha _2$$ 但是左边是一个全序集，而右边是偏序集 证明右边是全序集可以通过向下封闭的性质，即 $$\\forall \\alpha_1\\leq \\alpha_2~\\exists\\forall x\\in\\alpha_1\\Rightarrow x \\in\\alpha_2$$ 则可以证明出Real Number Set\\(\\mathbb R\\)是一个全序集 Summation #\r通过两个Dedekind Cut相加定义出一个新的Dedekind Cut $$\\alpha+\\beta={a+b,a\\in\\alpha,b\\in\\beta}$$ 现在需要证明这个定义Well-defined Proof #\r只需证明\\(\\alpha + \\beta\\)是一个 Dedekind Cut的Lower Set，也就是证明其1.向下封闭，2.没有最大元素 (i) 显然\\(\\alpha + \\beta \\neq \\emptyset\\)。任取\\(c \\in (\\alpha + \\beta)\\)，令\\(c = a + b\\)，其中\\(a \\in \\alpha, b \\in \\beta\\)。若\\(c\u0026rsquo; \u0026lt; c\\)，则存在\\(d \u0026gt; 0\\)满足\\(c\u0026rsquo; = c - d = (a + b) - d = (a - d) + b\\)，由于\\(a - d \u0026lt; a\\)，故\\(a - d \\in \\alpha\\)。这表明\\(c\u0026rsquo; \\in (\\alpha + \\beta)\\)，于是可知\\(\\alpha + \\beta\\)向下封闭 (ii) 由于\\(\\alpha\\)和\\(\\beta\\)中都没有最大元素，因此一定存在\\(a\u0026rsquo; \\in \\alpha, b\u0026rsquo; \\in \\beta\\)满足\\(a\u0026rsquo; \u0026gt; a, b\u0026rsquo; \u0026gt; b\\)，于是\\((a\u0026rsquo; + b\u0026rsquo;) \\in (\\alpha + \\beta)\\)且\\(a\u0026rsquo; + b\u0026rsquo; \u0026gt; a + b\\)，于是可知\\(\\alpha + \\beta\\)中也没有最大元素。 综上可述\\(\\alpha+\\beta\\in \\mathbb R\\) Law of Operation #\r加法结合律 #\r对任意的\\(\\alpha, \\beta, \\gamma \\in \\mathbb{R}\\)，有\\((\\alpha + \\beta) + \\gamma = \\alpha + (\\beta + \\gamma)\\) 加法交换律 #\r对任意的\\(\\alpha, \\beta \\in \\mathbb{R}\\)，有\\(\\alpha + \\beta = \\beta + \\alpha\\) 加法零元 #\r对于任意的\\(\\alpha \\in \\mathbb{R}\\)，存在一个零元\\(0^\\)使得\\(\\alpha + 0^ = 0^* + \\alpha = \\alpha\\) 加法负元 #\r对于任意的\\(\\alpha \\in \\mathbb{R}\\)，存在一个负元\\(\\beta\\)使得\\(\\alpha + \\beta = \\beta + \\alpha = 0^*\\) Completeness of Real Number Field 实数域的完备性 #\rDense 稠密 #\r设S是一个集合，X是一个包含S的更大的空间。我们说S在X中稠密，如果对于X中任意的点x，在x的任意小的邻域中，总能找到至少一个属于S的点 $$\\forall x \\in X, \\forall \\epsilon \u0026gt; 0, \\exists s \\in S \\ \\text{st.} \\ |s - x| \u0026lt; \\epsilon$$\nReal Number Field\u0026rsquo;s Density 实数域的稠密性 #\r对于任意𝛼,𝛽∈R,若𝛼\u0026lt; 𝛽,则一定存在𝛾∈R满足\\(\\alpha \u0026lt; \\gamma \u0026lt;\\beta\\) Proof #\r令𝛾=(𝛼+𝛽)/2.由于𝛼 \u0026lt; 𝛽,故 $$2𝛼 \u0026lt; 𝛼+𝛽 \u0026lt;2𝛽 ⇐⇒ 𝛼\u0026lt; \\frac{𝛼+𝛽} {2} \u0026lt;𝛽 ⇐⇒ 𝛼\u0026lt;𝛾\u0026lt;\\beta$$ Dedekind Theorem In Rational Number Field 有理数的戴德金分割 #\r有理数域\\(\\mathbb Q\\)上的Dedekind Cut可能会出现Upper Set中无最小元素的情况.这说明有理数域存在空隙\nex. sqrt{2} 在 Q中的分割 #\r考虑实数\\(\\sqrt{2}\\)（它是无理数，不属于\\(\\mathbb{Q}\\)），我们定义：\n\\(A = { q \\in \\mathbb{Q} \\mid q^2 \u0026lt; 2 }\\) （所有小于\\(\\sqrt{2}\\) 的有理数） \\(B = { q \\in \\mathbb{Q} \\mid q^2 \u0026gt; 2 }\\) （所有大于\\(\\sqrt{2}\\) 的有理数） 可以验证：\n\\(A \\cup B = \\mathbb{Q}\\)且\\(A \\cap B = \\emptyset\\) \\(a \u0026lt; b\\) 对于任意\\(a \\in A, b \\in B\\) 但是Upper Set B中不存在最小元素，因为对于任意\\(b \\in B\\)，都可以找到一个更小的\\(b\u0026rsquo; \\in B\\)\n这说明在\\(\\mathbb{Q}\\)中，\\(\\sqrt{2}\\)这样的点无法被有理数表示，导致了“空隙”的存在。\nDedekind Theorem in Real Number Field 实数的戴德金分割 #\r对于实数域R上的任一Dedekind Cut \\(𝐴| 𝐵\\), Upper Set 𝐵中都有最小元素 Proof #\r简单来说，给定实数域上的一个Dedekind Cut(A|B)。\n若A有最大元，则这个最大元即属于B，因此B有最小元。 若A无最大元，则A中可找出一列有理数向上递增逼近分割点。若该分割点存在于A中，则逼近过程能产生一个最大元与分割矛盾；若分割点不在A中，就会落在B中，从而成为B的最小元。 总而言之，无论A是否有最大元，B中总能找到一个最小元\nThe limit principle 界 #\rDedekind Cut 𝐴 | 𝐵 中, Lower Set 𝐴的任一元素都小于𝐵中任一元素，从直观上看, 𝐴是有‘‘上界的”,而𝐵是有‘‘下界的”.\nBounded 有界的 #\r设非空集合𝐸⊆R.若存在𝑀\u0026gt;0使得|𝑥|\u0026lt;𝑀 (∀𝑥∈𝐸),则称𝐸是有界的(bounded) Supremum 上确界 #\r集合E的一个数M被称为其上确界（supremum），如果满足以下两个条件\nM是E的上界 Upper bound： $$\\forall x \\in E, \\quad x \\leq M$$ M是所有上界中的最小值，也就是上确界 Supremum： 对于任意\\(\\varepsilon \u0026gt; 0\\)，都存在\\(x_\\varepsilon \\in E\\)，使得\\(M - \\varepsilon \u0026lt; x_\\varepsilon \\leq M\\) Infimum 下确界 #\r集合E的一个数m被称为其下确界（infimum），如果满足以下两个条件：\nm是E的下界（lower bound）： $$\\forall x \\in E, \\quad m \\leq x$$ m是所有下界中的最大值： 对于任意\\(\\varepsilon \u0026gt; 0\\)，都存在\\(x_\\varepsilon \\in E\\)，使得\\(m\\leq x_\\varepsilon \u0026lt; m + \\varepsilon\\) Existence of Supremum \u0026amp; Infimum 上，下确界的存在性 #\r对于一个实数集的子集\\(E\\subseteq\\mathbb R\\)，其根据Real Number的 Completeness一定存在确界 ex. Supremum \u0026amp; Infimum #\r$$E ={ \\frac{1}{n} : n \\in \\mathbb{N}^* }$$\n对于E来说，\\(supE=1,infE=0\\) Least-upper-bound property, LUB 最小上界性 #\r定义：如果一个非空的实数集合S在实数集中有上界，那么S必定在实数集中有最小的上界（简称为确界）\nDedekind Theorem和确界原理是等价的 Heine-Borel Theorem #\r有限闭区间的任一开覆盖都存在一个有限子覆盖 Open Cover 开覆盖 #\r设Set \\(E\\subseteq \\mathbb R\\)和一族开区间\\({I_\\lambda:\\lambda\\in \\Lambda }\\)，\\(\\Lambda\\)是一个指标集，若 $$E \\subseteq \\bigcup_{\\lambda \\in \\Lambda} I_\\lambda.$$ 则\\({I_\\lambda:\\lambda\\in \\Lambda }\\)是E的一个Open Cover，也就是一个开区间的集合的并集能够覆盖满整个集合E，记作\\(C_E\\) 若E有一个Open Cover \\(C_E\u0026rsquo;\\subseteq C_E\\)，则称\\(C_E\u0026rsquo;\\)为\\(C_E\\)的Subcover 子覆盖 过这个Cover里只有有限个Open Set，则称他为Finite Subcover 有限子覆盖 Set\u0026rsquo;s Cardinality 集合的基数 #\rCardinality 基数 #\r对于一个Finite Set A来说，他的基数是一个可以数出来的数字，即可以在\\(\\mathbb Z\\)中找到一个数表示他的Cardinality，计作cardA或是\\(|A|\\) 我们约定\\(card \\emptyset =0\\) 同时也可以发现两个Card相同的Set之间一定存在一个Bijective 对于一个Infinite Set，即使无法直接数出他的Card，但可以通过Bijective的角度刻画 Equivalency of Set 集合的对等 #\r设集合A，B若存在一个A到B的双射 , 则称A与B对等 (equivalent), 记作A ∼ B Finite Set 有限集 #\r设集合A. 若A= ∅ , 或存在\\(n∈ N^∗\\), 使得集合 \\({ 1 , 2 , · · · , n}\\) ∼ A , 则称集合A为有限集 (finite set) Finite Set的任一Subset仍是一个Finite Set Equivalency of Integer Set and Nature Number Set 整数集和自然数集的对等 #\r考虑以下问题，自然数集和整数集的Cardinality是否相等？ 从直觉上看\\(\\mathbb N \\subset \\mathbb Z\\)，看似自然数的总数比整数少，但并非如此 前面说明了，如果两个Set之间可以建立一个Bijective的关系，则说明两个Set是Equivalent的，现在把Integer Set的所有元素排成一列 $$0,1,-1,2,-2,3,-3,\\cdots$$ 通过以下函数建立\\(f:\\mathbb N\\rightarrow \\mathbb Z\\) $$f(n) = \\begin{cases} -\\frac{n}{2}, \u0026amp; \\text{n is Even} \\ \\frac{n+1}{2}, \u0026amp; \\text{n is Odd} \\end{cases}$$ 可以发现 $$n = 1 \\to 0,n = 2 \\to -n,n = 3 \\to 1,n = 4 \\to -2,n = 5 \\to 2,n = 6 \\to -3 $$ 于是可知\\(\\mathbb N\\) ~ \\(\\mathbb Z\\) Countable Set 可数集 #\r设Infinite Set A，若\\(A\\) ~ \\(\\mathbb N\\)，集存在一个A到N的Bijective Relationship，则称A为Countable 可数的，这样的Set也被称为Countable Set Countable Set的Cardinality称为Countable Cardinality 可数基数，记做\\(\\aleph_0\\) Aleph 阿列夫\nRational Number Set is a Countable Set #\r同理只要将Rational Number通过\\(\\frac{p}{q}\\)那样排成一排然后和\\(\\mathbb N\\)建立Bijective Relationship就行 $$ \\left[ \\begin{array}{cccc} \\frac{1}{1} \u0026amp; \\frac{1}{2} \u0026amp; \\frac{1}{3} \u0026amp; \\cdots \\ \\frac{2}{1} \u0026amp; \\frac{2}{2} \u0026amp; \\frac{2}{3} \u0026amp; \\cdots \\ \\frac{3}{1} \u0026amp; \\frac{3}{2} \u0026amp; \\frac{3}{3} \u0026amp; \\cdots \\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\end{array} \\right] $$ 具体排列方式不限，但可以知道Rational Number Set的Cardinality也同为\\(\\aleph_0\\) Uncountable Set 不可数集 #\r下面给出一个不可数集的例子，其实既然知道了有理数集是Infinite Countable Set，那Uncountable Set很明显就是Irrational Number或者的Subset了 ex. Interval \\([0,1)\\) is Uncountable Set #\r假设区间\\([0,1)\\)中的所有实数是可数的，那么我们可以将这些实数按序列排列如下： $$x_1, x_2, x_3, \\ldots$$\n每一个Real Number \\(x_i\\)都可以表示为小数形式 $$\\begin{align} x_1 = 0.a_{11}a_{12}a_{13}a_{14} \\cdots \\ x_2 = 0.a_{21}a_{22}a_{23}a_{24} \\cdots \\ x_3 = 0.a_{31}a_{32}a_{33}a_{34} \\cdots \\ \\vdots \\end{align} $$ 假设前三个实数 $$ \\begin{array}{c|cccc} \u0026amp; \\text{第1位} \u0026amp; \\text{第2位} \u0026amp; \\text{第3位} \u0026amp; \\text{第4位} \\ \\hline x_1 \u0026amp; 3 \u0026amp; 1 \u0026amp; 4 \u0026amp; 1 \\ x_2 \u0026amp; 5 \u0026amp; 9 \u0026amp; 2 \u0026amp; 6 \\ x_3 \u0026amp; 5 \u0026amp; 3 \u0026amp; 5 \u0026amp; 8 \\ \\end{array} $$ 康托尔的对角线论证法要求我们构造一个新的实数y，其小数部分的每一位都与列表中第i个数\\(x_i\\)的第i位不同，通过这样的构造，y与列表中的每个数\\(x_i\\)至少在第i位上不同，也就是说前面的假设：我们可以将实数按序列排列成一个序列不成立，因为永远存在一个y不在列表中 这证明了\\([0,1)\\)的实数集合是不可数的 这种证明方法也叫做Cantor的Diagonal Process Real Number Set is Uncountable 实数集是不可数集 #\r通过一个Bijective Relationship $$f(x)=-cot(\\pi x)$$ 其Domain为\\((0,1)\\)，Range为\\(\\mathbb R\\)，即f为\\((0,1)和\\mathbb R\\)的一个Bijective Relationship 因此\\((0,1)\\) ~ \\(\\mathbb R\\) 已知\\((0,1)\\)是一个Uncountable Set，即证\\(\\mathbb R\\)也是Uncountable的 Continuum 连续统 #\r和Real Number Set等势的Set称为continuum 连续统 Continuum的Cardinality为\\(\\aleph_1\\) Continuum hypothesis 连续统假设 #\r1874 年 Cantor 提出猜想 : 不存在基数介于\\(ℵ_0\\)和\\(ℵ_1\\)的集合 . 这就是著名的连续统假设 ","date":"Dec 15 2024","externalUrl":null,"permalink":"/docs/mathematicalanalysis/ma2.realnumber/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 12/15/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eReal Number 实数 \r\n    \u003cdiv id=\"real-number-%E5%AE%9E%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#real-number-%E5%AE%9E%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eRational Number 有理数 \r\n    \u003cdiv id=\"rational-number-%E6%9C%89%E7%90%86%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#rational-number-%E6%9C%89%E7%90%86%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e整数，有限位小数，无限循环小数，分数\u003c/li\u003e\n\u003cli\u003e只要是能被表达为\n$$\\frac{p}{q},p,q\\in \\mathbb z,q\\neq 0$$\u003c/li\u003e\n\u003cli\u003e的数都叫做Rational Number\u003c/li\u003e\n\u003cli\u003e也就是说可以被任意两个Nature Number通过加减乘除所得到的数都被称为Rational Number（做除法的时候分母不能为零）\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eIrrational Number 无理数 \r\n    \u003cdiv id=\"irrational-number-%E6%97%A0%E7%90%86%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#irrational-number-%E6%97%A0%E7%90%86%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\r\n\r\n\u003ch4 class=\"relative group\"\u003ePythagoras Theorem 毕达哥拉斯定理 \r\n    \u003cdiv id=\"pythagoras-theorem-%E6%AF%95%E8%BE%BE%E5%93%A5%E6%8B%89%E6%96%AF%E5%AE%9A%E7%90%86\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#pythagoras-theorem-%E6%AF%95%E8%BE%BE%E5%93%A5%E6%8B%89%E6%96%AF%E5%AE%9A%E7%90%86\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h4\u003e\r\n\u003cul\u003e\n\u003cli\u003e在一个直角三角形中，直角边对面的斜边（最长边）的平方等于两个直角边的平方和\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch4 class=\"relative group\"\u003eContradiction \r\n    \u003cdiv id=\"contradiction\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#contradiction\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h4\u003e\r\n\u003cul\u003e\n\u003cli\u003e在当时并没有Irrational Number的定义，但是表示一个两个直角边长度为一的直角三角形的斜边的时候却出现了问题，即\\(1^2+1^2=x^2\\)，无法通过一个Rational Number，也就是两个Nature Number的任意四则运算求出这个x\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch4 class=\"relative group\"\u003eProof that sqrt 2 isn\u0026rsquo;t Rational Number \r\n    \u003cdiv id=\"proof-that-sqrt-2-isnt-rational-number\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#proof-that-sqrt-2-isnt-rational-number\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h4\u003e\r\n\u003cul\u003e\n\u003cli\u003eProof By Contradiction\u003c/li\u003e\n\u003cli\u003e假设\\(\\sqrt 2\\)是一个Rational Number，则有\\(2=(\\frac{p}{q})^2\\)，其中p和q是互素的\u003c/li\u003e\n\u003cli\u003e即\\(p^2=2q^2\\)，已知\\(2q^2\\)为一个Even Number，则等号另一边的\\(p\\)也必为一个Even Number（Odd Number的平方为Odd Number）\u003c/li\u003e\n\u003cli\u003e既然p是一个Even Number，则他可以被2整除，即\\(p^2\\)可以被4整除，同理可以得到\\(2|q^2\\)\u003c/li\u003e\n\u003cli\u003e那既然p和q都是偶数，很明显他们不可能互素，Contradict，故假设不成立\u003c/li\u003e\n\u003cli\u003e于是证明了Irrational Number的存在\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eDefine Real Number \r\n    \u003cdiv id=\"define-real-number\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#define-real-number\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e首先要知道的是，根号的本质是一个服务幂而创造出的代数运算，其能表达出的Irrational Number的个数几乎可以忽略不计\u003c/li\u003e\n\u003cli\u003e所以定义实数的第一步就是构造出所有Irrational Number，第二步则是定义全序列关系，第三步为定义代数运算，第四步研究拓扑结构（稠密性）\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eDedekind Cut \r\n    \u003cdiv id=\"dedekind-cut\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#dedekind-cut\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e设数集的一个划分\\({\\alpha,\\beta }\\)，其中\u003c/p\u003e","title":"MA 2. RealNumber","type":"docs"},{"content":"","date":"Nov 25 2024","externalUrl":null,"permalink":"/tags/chemistry/","section":"Tags","summary":"","title":"Chemistry","type":"tags"},{"content":"","date":"Nov 25 2024","externalUrl":null,"permalink":"/series/ecms/","section":"Series","summary":"","title":"ECMS","type":"series"},{"content":"","date":"Nov 25 2024","externalUrl":null,"permalink":"/tags/ecms/","section":"Tags","summary":"","title":"ECMS","type":"tags"},{"content":" Last Edit: 11/25/24\nThe finger of time #\r在上一章中讨论了Solide Crystalline Nacl的形成，我们知道它在室温和压力下是稳定的 同理当我们看到手机从人手上掉下来并不会感到惊讶 通过The Second Law Of Thermodynamics，我们可以意识到时间的流逝 The Second Law of Thermodynamics 热力学第二定律 #\rThe entropy of the Universe increases during any spontaneous process Spontaneous 自发的 #\rSpontaneous, here, means that the process proceeds on its own, without the need for an input of energy 代表了该过程自行进行，不需要能量输入 例如在空气中点燃一张纸，他将会燃烧，我面对如此现象并不会感到惊讶 Entropy 熵 #\r前面反复的介绍的Second Law的重要性，而Entropy的存在将定义如何使用它 熵是一个衡量系统混乱程度的物理量，系统的熵越高，系统的无序程度就越高 $$\\Delta S = \\frac{q_{REV}}{T}$$ S is the entropy 熵 \\(q_{rev}\\)​ is the heat transferred 传递的热量 T is the thermodynamic temperature 热力学温度，单位为Kelvin The Thermodynamic Alphabet 热力学字母表 #\rReversibility 可逆性 #\r对于rev的下标，这代表了Heat is transferred reversibly 但实际上Reversibility仅存在于Concept中，因为真实的物理过程总会有一些不可逆的因素，如摩擦，热损失等 System Surroundings and the Universe #\r周围的一切都被叫做Surroundings 而System和Surrounding一起构成了Universe 带回到The Second law of thermodynamic中，有 $$\\Delta S_{universe} = \\Delta S_{system} + \\Delta S_{surroundings} \u0026gt; 0$$ 所以唯一的判断Spontaneous的要求即为当Entropy change for universe must be positive The First Law of Thermodynamics 热力学第一定律 #\r在一个孤立系统中，能量既不能被创造也不能被消灭，能量只能从一种形式转换为另一种形式，或者从一个物体传递到另一个物体 $$ΔU=q+w$$ Internal Energy 内能 #\rInternal Energy是一个System的总能量，包括所有可能的能量形式，如动能、势能、化学能等 在听到内能的时候，可能会联想到Fuel, battery or a Quantity of Nuclear Fuel，但他们都不是完全的 对于一个System其中存在着无数种能影响内能的能量形式，这也使得研究System的Internal Energy的Absolute Value变得难以测定以至于不具有实际意义 所以所研究的Internal Energy更多的是一种Change of Energy 而Change，是建立在所谓的控制变量法 Potential Energy - Water Bottle #\r考虑上面的水平，要计算它的Internal Energy，我们会本能的选择一个Obvious Reference Surface从而计算瓶子相对于该平面的Potential Energy 同时，我们不需要考虑其动能或者是燃烧瓶子所释放的能量 这强调了在特定问题中选择关注特定类型能量的重要性 Similarly, in thermodynamics we\u0026rsquo;ll need to define a logical reference point to measure changes in energy Logical Reference Point - The Standard State #\r在热力学和化学中，需要一个清晰、一致的基准来测量能量变化。比如，当讨论燃烧汽油（主要成分为辛烷）所需的能量时，我们需要一个基准状态作为起点 这段话在讨论热力学和化学中如何选择一个参考点，称为“Standard State”，来测量和比较化学反应和能量变化 标准状态是在特定条件下（通常是25°C和105帕斯卡压力），一种纯元素的最稳定形式 State Functions and Path Functions 状态和路径函数 #\r再次考虑前面水瓶的例子 $$Water Bottle_{on table} → Water Bottle_{on high shelf}$$ State Function 状态函数 #\rIt does not depend on how we got to the final state, all that matters is what that state is 它不取决于我们如何达到最终状态，重要的是那个状态是什么 另一个例子为Temperature，你不需要查阅过去的温度来计算现在的，只需要测量当前温度便可以， 这就是一种State Function 庆幸的是Internal Energy只能是一种State Function，这意味着内能的变化仅取决于系统的初始状态和最终状态。无论系统是通过何种过程从初始状态转变到最终状态，内能的变化总量是固定的 Path Function 路径函数 #\r如果问题变成了，把水瓶从桌子上移到高架子上有多困难 做的工作取决 how you got there - it depends on the path Closed Versus Isolated Systems 封闭系统与孤立系统 #\r我们可以研究Boundaries开放且物质穿过它们的系统，但这不是我们现在需要考虑的 我们需要考虑systems where matter is not allowed to pass the boundaries的系统 Isolated System 隔离式系统 #\rNo heat is exchanged with the surroundings $$ΔU=q+w,q=w=0\\RightarrowΔU=0$$ Closed system 封闭系统 #\rHeat may pass the boundaries $$ΔU=q+w$$ - q is the heat transferring into the system w is the work done on the system Sign convention在这里很重要，heat in and work on are positive Enthalpy 焓 #\r焓（Enthalpy），符号为H，是热力学中的一个重要概念，用于描述系统在一定压力下的总热含量 焓是一个状态函数，它与系统的内能、压力和体积关系密切。焓的定义是 $$H=U+PV$$ 其中U是内能，P是压力，V是体积 Enthalpy in Solid 固体中的焓 #\r在固态物理过程中，物质的体积变化通常非常小，因此PV工作相对于内能U的变化可以忽略不计 所以在在固态物理领域，人们可能会将“Enthalpy”或“Enthalpy Change”与“Energy”或“Energy Change”这些术语互换使用 The Gibbs Energy 吉布斯自由能 #\r$$G=H−TS$$\nG：吉布斯能量 H：焓（系统的总能量，包括内能和体积功） T：温度（开尔文，K） S：熵（系统的无序程度） 这个定义说明吉布斯自由能考虑了系统的能量状态（Enthalpy）和无序度（Entropy） Spontaneity for a system 系统的自发性 #\r如果\\(\\Delta G \u0026lt; 0\\)：过程是自发的（有利于发生）。 如果\\(\\Delta G = 0\\)：系统达到平衡。 如果\\(\\Delta G \u0026gt; 0\\)：过程是非自发的（需要外界能量输入） 自发过程的基本条件是整个宇宙的熵（包括系统和环境的熵）总和需要增加，这是在前面的The Second Law of Thermodynamics中定义的 $$\\Delta S_{\\text{system}} + \\Delta S_{\\text{surroundings}} \u0026gt; 0$$ 在恒温下，周围环境的熵变是进入周围环境的热量除以温度 $$\\Delta S_{\\text{Surroundings}} = \\frac{q_{\\text{Surroundings}}}{T}$$ 离开系统的任何热量都与周围环境吸收的热量相同，或者相反，因此有 $$\\Delta S_{\\text{Surrounding}} = \\frac{-q_{\\text{System}}}{T}$$ 于是就可以推出Spontaneous的同时由Entropy和Enthalpy定义的公式变为 $$\\Delta H_{\\text{system}}-T \\Delta S_{\\text{system}} \u0026lt; 0$$ Phase Transformations #\r图中为对不同阶段的水加热后的变化 固态（冰）升温：在冰的温度低于 0°C 时，输入热量会使冰的温度上升。 熔化（0°C 平台）：温度停止上升，因为输入的热量用于冰的相变（熔化），这个热量叫做熔化焓（enthalpy of fusion）。 液态（水）升温：冰完全融化后，输入热量使液态水的温度上升，此时温度上升速率（曲线的斜率）与冰时不同。 汽化（100°C 平台）：在 100°C 时，热量再次用于相变（汽化），这一阶段输入的热量叫做汽化焓（enthalpy of vaporization）。 气态（蒸汽）升温：水完全汽化后，输入热量让蒸汽升温 在图中的斜率定义为 $$\\text{Slope} = \\frac{\\Delta T}{q} , \\left[ = \\frac{K}{\\frac{J}{\\text{mol}}} \\right] $$ $$q = \\frac{1}{\\text{Slope}} \\Delta T$$ \\(\\frac{1}{\\text{Slope}}\\)其还有一个名字叫做Molar Heat Capacity \\(C_P\\)，也就是Specific Heat Capacity比热容 Molar Hear Capacity 摩尔热容 #\r$$q = \\frac{1}{\\text{Slope}} \\Delta T = n C_P \\Delta T$$\nq：热量 n：物质的摩尔数 \\(C_P\\)​：摩尔热容 \\(\\Delta T\\)：温度变化 Specific Heat Capacity 比热容 #\r$$q=mcΔT$$\nm：物质质量 c：比热容 ","date":"Nov 25 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms8.thermodynamics/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/25/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eThe finger of time \r\n    \u003cdiv id=\"the-finger-of-time\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#the-finger-of-time\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e在上一章中讨论了Solide Crystalline Nacl的形成，我们知道它在室温和压力下是稳定的\u003c/li\u003e\n\u003cli\u003e同理当我们看到手机从人手上掉下来并不会感到惊讶\u003c/li\u003e\n\u003cli\u003e通过The Second Law Of Thermodynamics，我们可以意识到时间的流逝\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eThe Second Law of Thermodynamics 热力学第二定律 \r\n    \u003cdiv id=\"the-second-law-of-thermodynamics-%E7%83%AD%E5%8A%9B%E5%AD%A6%E7%AC%AC%E4%BA%8C%E5%AE%9A%E5%BE%8B\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#the-second-law-of-thermodynamics-%E7%83%AD%E5%8A%9B%E5%AD%A6%E7%AC%AC%E4%BA%8C%E5%AE%9A%E5%BE%8B\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eThe entropy of the Universe increases during any spontaneous process\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eSpontaneous 自发的 \r\n    \u003cdiv id=\"spontaneous-%E8%87%AA%E5%8F%91%E7%9A%84\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#spontaneous-%E8%87%AA%E5%8F%91%E7%9A%84\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eSpontaneous\u003c/em\u003e, here, means that the process proceeds on its own, without the need for an input of energy 代表了该过程自行进行，不需要能量输入\u003c/li\u003e\n\u003cli\u003e例如在空气中点燃一张纸，他将会燃烧，我面对如此现象并不会感到惊讶\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eEntropy 熵 \r\n    \u003cdiv id=\"entropy-%E7%86%B5\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#entropy-%E7%86%B5\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e前面反复的介绍的Second Law的重要性，而Entropy的存在将定义如何使用它\u003c/li\u003e\n\u003cli\u003e熵是一个衡量系统混乱程度的物理量，系统的熵越高，系统的无序程度就越高\n$$\\Delta S = \\frac{q_{REV}}{T}$$\u003c/li\u003e\n\u003cli\u003eS is the entropy 熵\u003c/li\u003e\n\u003cli\u003e\\(q_{rev}\\)​ is the heat transferred 传递的热量\u003c/li\u003e\n\u003cli\u003eT is the \u003cem\u003ethermodynamic\u003c/em\u003e temperature 热力学温度，单位为Kelvin\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eThe Thermodynamic Alphabet 热力学字母表 \r\n    \u003cdiv id=\"the-thermodynamic-alphabet-%E7%83%AD%E5%8A%9B%E5%AD%A6%E5%AD%97%E6%AF%8D%E8%A1%A8\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#the-thermodynamic-alphabet-%E7%83%AD%E5%8A%9B%E5%AD%A6%E5%AD%97%E6%AF%8D%E8%A1%A8\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS8.Thermodynamics/ECMS8.Thermodynamics.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 8. Thermodynamics","type":"docs"},{"content":"","date":"Nov 25 2024","externalUrl":null,"permalink":"/tags/la/","section":"Tags","summary":"","title":"LA","type":"tags"},{"content":" Last Edit: 11/25/24\nEigenvectors and Eigenvalues #\r考虑Linear Transformation为一种Function，输入x而输出\\(Ax\\) Eigenvector即对于指定的Vector x，其Ax平行于x，有 $$Ax=\\lambda x$$ 其中x为A的Eigenvector，\\(\\lambda\\)为A的Eigenvalue 特征向量的定义要求\\(x \\neq 0\\) Zero Eigenvalue #\r如果0为Matrix的Eigenvalue，则有 $$Ax=0x=0$$ Eigenvalue 0所对应的Vector Span出了Matrix的Null Space 如果矩阵A为不可逆矩阵，则0是其特征值之一 ex. Projection Matrix #\r对于Projection Matrix P，其Column Space中的任意Vector都会是一个Eigenvector ![[LA8.DiagonalizationandEigenvalues.png]]\n因为当其投影到Subspace的时候并没有改变 因此x为Eigenvector，并且Eigenvalue为1 同时对于Orthogonal于Subspace的Vector，有\\(Px=0\\)，则这个x也是Eigenvector，其Eigenvalue为0 ex. Permutation Matrix #\r$$A = \\begin{bmatrix} 0 \u0026 1 \\\\ 1 \u0026 0 \\end{bmatrix}$$\r对于置换矩阵存在Eigenvector \\(x=[1,1]^T\\)，Eigenvalue为1 另一个Eigenvalue为\\(x=[-1,1]^T\\)，对应Eigenvalue为-1，\\(Ax=-x\\) Trace 迹 #\r\\(n\\times n\\) 的Matrix存在n个Eigenvalue 并且它们的和，即为Trace，等于矩阵对角线上的元素之和 对于二阶矩阵，在已知一个特征值的条件下， 可以据此得到另一个特征值 Solve Ax = lambdax #\r对于\\(Ax=\\lambda x\\)存在两个未知数，下面讨论求解的办法 Rewrite等式为\\((A-\\lambda I)x=0\\) 如果系数矩阵\\(A - \\lambda I\\)是非奇异矩阵（行列式不为零），那么方程组只有Trivial Solution \\(x=0\\) 而如果系数矩阵\\(A - \\lambda I\\)是奇异矩阵（行列式为零），那么方程组可能有非零解\\(x \\neq 0\\) 于是可以推出\\(det(A-\\lambda I)=0\\) 在这个没有x的“特征方程”中，可以解得n个特征值，但是有可能方程有Repeated Root，则会得到重复的Eigenvalue 得到特征值之后，用消元法解\\(A-\\lambda I\\)，这一矩阵零空间中的向量为矩阵 A的特征向量 ex. #\r对于Matirx $$A= \\begin{bmatrix}3 \u0026 1 \\\\1 \u0026 3\\end{bmatrix}$$\r$$\\det (A-\\lambda I) = \\begin{vmatrix} 3-\\lambda \u0026 1 \\\\ 1 \u0026 3-\\lambda \\end{vmatrix} = (3-\\lambda)^2 - 1 = \\lambda^2 - 6\\lambda + 8$$\r在一元二次方程中，6为Trace，8为Determinant 于是可以总结对于二阶矩阵的Eigenvalue为该方程的解 $$\\lambda^2 - \\text{trace}(A) \\lambda + \\det A = 0$$\r对于上面的Matrix则有Eigenvalue = 4 \u0026amp; 2 $$A-4I = \\begin{bmatrix} -1 \u0026 1 \\\\ 1 \u0026 -1 \\end{bmatrix}, \\quad (A-4I)x_1 = 0, \\quad x_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$\r$$A-2I = \\begin{bmatrix} 1 \u0026 1 \\\\ 1 \u0026 1 \\end{bmatrix}, \\quad (A-2I)x_2 = 0, \\quad x_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}$$\r与前面的例子\\(A=\\begin{bmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{bmatrix}\\)的特征值和特征向量相对比，可知两者为一组平移矩阵 在对角元素上分别加3，改变了特征值但是没有改变特征向量 $$Ax = \\lambda x, \\quad \\text{则有} (A+3I)x = \\lambda x + 3x = (\\lambda + 3)x$$\r所以当两个Matrix有相同的Eigenvectors的时候，他们是可以“相加”的 当然其中一个为Identity Matrix的情况除外 Trace Equal to Eigenvalue Summation 矩阵的迹等于特征值之和 #\r将\\(det(A-\\lambda I)=0\\)展开会得到\\(\\lambda\\)的n 阶多项式，多项式的解就是矩阵 A 的特征值 根据多项式根与系数的关系，解之和即特征值之和等于\\(\\lambda^{n-1}\\)的系数 而行列式展开式中只有对角线的积这一项包含的\\(\\lambda^{n-1}\\)（其它项最高是n-2次方），而其系数为矩阵A对角线元素之和即矩阵A的Trace，因此特征值之和与矩阵的迹相等 Symmetry Matrix\u0026rsquo;s Eigenvector Orthogonal 对称矩阵的特征向量正交 #\r\\(\\lambda_1,\\lambda_2\\)是对称矩阵的两个不同的特征值，对应的特征向量分别为x1和x2。 $$\\text{则有 } A\\mathbf{x}_1 = \\lambda_1 \\mathbf{x}_1, \\text{ 左乘 } \\mathbf{x}_2^\\top \\text{ 得 } \\mathbf{x}_2^\\top A\\mathbf{x}_1 = \\lambda_1 \\mathbf{x}_2^\\top \\mathbf{x}_1$$\r$$\\mathbf{x}_2^\\top A\\mathbf{x}_1 = (\\mathbf{A}^\\top \\mathbf{x}_2)^\\top \\mathbf{x}_1 = \\lambda_2 \\mathbf{x}_2^\\top \\mathbf{x}_1。\\\\\r\\text{因此有 } (\\lambda_1 - \\lambda_2) \\mathbf{x}_2^\\top \\mathbf{x}_1 = 0$$\r而两特征值不等，所以两特征向量正交 Complex eigenvalues 复数特征值 #\r$$Q = \\begin{bmatrix} 0 \u0026 -1 \\\\ 1 \u0026 0 \\end{bmatrix} = \\begin{bmatrix} \\cos 90^\\circ \u0026 -\\sin 90^\\circ \\\\ \\sin 90^\\circ \u0026 \\cos 90^\\circ \\end{bmatrix}$$\rQ是一个是一个90度Rotation Matrix 从矩阵的Trace和Determinant的值可以得到\\(\\lambda_1+\\lambda_2=0,\\lambda_1\\lambda_2=1\\) 仅观察Matrix可以发现他的Eigenvector只能是Zero Vector，因为其他Vector乘以Rotation Matrix，其方向将会改变而，不可逆平行于原向量，通过原来的计算可得 $$\\det (Q - \\lambda I) = \\begin{vmatrix} -\\lambda \u0026 -1 \\\\ 1 \u0026 -\\lambda \\end{vmatrix} = \\lambda^2 + 1 = 0$$\r可以解得\\(\\lambda_1=i,\\lambda_2=-i\\) 如果一个矩阵具有复数特征值a+bi则，它的共轭复数a-bi也是矩阵的特征值 实数特征值让特征向量伸缩而虚数让其旋转 Antisymmetric matrices 反对称矩阵 #\r即满足\\(A^T=-A\\)的矩阵 对称矩阵永远具有实数的特征值，而，具有纯虚数的特征值 Triangular matrices and repeated eigenvalues 三角阵和重特征值 #\r对于一个Uppertriangular Matrix $$A = \\begin{bmatrix}3 \u0026 1 \\\\0 \u0026 3\\end{bmatrix}$$\r$$\\det (A - \\lambda I) = \\begin{vmatrix} 3-\\lambda \u0026 1 \\\\ 0 \u0026 3-\\lambda \\end{vmatrix} = (3-\\lambda)(3-\\lambda) = 0\r$$\r即\\(\\lambda_1=\\lambda_2=3\\) $$(A-\\lambda I)x = \\begin{bmatrix} 0 \u0026 1 \\\\ 0 \u0026 0 \\end{bmatrix}x = 0, \\quad \\text{得到} \\quad x_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$$\r其并没有没有线性无关的x2，说明A是一个退化矩阵，对应相同的特征值，而特征向量短缺 Diagonalization 对角化 #\r如果矩阵A具有n个线性无关的特征向量，将它们作为列向量可以组成一个可逆方阵S，并且有 $$AS = A \\begin{bmatrix} \\mathbf{x}_1 \u0026 \\mathbf{x}_2 \u0026 \\cdots \u0026 \\mathbf{x}_n \\end{bmatrix} = \\begin{bmatrix} \\lambda_1 \\mathbf{x}_1 \u0026 \\lambda_2 \\mathbf{x}_2 \u0026 \\cdots \u0026 \\lambda_n \\mathbf{x}_n \\end{bmatrix} = S \\begin{bmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0 \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 0 \u0026 0 \u0026 \\cdots \u0026 \\lambda_n \\end{bmatrix} = S D$$\r根据上式可写出\\(AS=SD\\Rightarrow S^{-1}AS=D\\)，这就叫Diagonalization 同理也有\\(A=SDS^{-1}\\) Power of A 矩阵的幂 #\r特征值给矩阵的幂计算提供了方法。 $$\\text{如果 } A\\mathbf{x} = \\lambda \\mathbf{x}, \\text{ 则有 } A^2\\mathbf{x} = \\lambda A\\mathbf{x} = \\lambda^2 \\mathbf{x}。$$\r这说明了Matrix \\(A\\)与\\(A^2\\)拥有相同的Eigenvector $$A^2 = S D S^{-1} S D S^{-1} = S D^2 S^{-1}$$\r同理可以推广到k-th power的情况，有\\(A^k = S D^k S^{-1}\\) 这说明\\(A^k\\)有着和A一样的特征向量，而特征值为\\(\\lambda^k\\) 如果矩阵A具有n个线性无关的特征向量，并且特征值均满足\\(|\\lambda_i|\u0026lt;1\\)，则k→∞时，Ak→0 Repeated eigenvalues 重特征值 #\r如果矩阵A没有重特征值，则其一定具有n个线性无关的特征向量 如果矩阵A有重特征值，它有可能具有n个线性无关的特征向量，也可能没有 Identity Matrix #\r比如单位阵的特征值为重特征值1，但是其具有n个线性无关的特征向量 UpperTriangular Matrix #\r参考上面的例子 对于一个Uppertriangular Matrix $$A = \\begin{bmatrix}3 \u0026 1 \\\\0 \u0026 3\\end{bmatrix}$$\r$$\\det (A - \\lambda I) = \\begin{vmatrix} 3-\\lambda \u0026 1 \\\\ 0 \u0026 3-\\lambda \\end{vmatrix} = (3-\\lambda)(3-\\lambda) = 0$$\r即\\(\\lambda_1=\\lambda_2=3\\) $$(A-\\lambda I)x = \\begin{bmatrix} 0 \u0026 1 \\\\ 0 \u0026 0 \\end{bmatrix}x = 0, \\quad \\text{得到} \\quad x_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$$\r其存在Repeated Eigenvalue所以可能没有n个Linearly Independent的Eigenvector Difference equations 差分方程 #\r差分方程描述了离散变量之间的递推关系 简单来说，差分方程是用来研究一系列离散点上的函数值之间的关系 从给定的一个向量\\(u_0\\)出发，我们可以通过对前一项乘以矩阵A得到下一项的方式，得到一个向量序列：\\(u_{k+1}=Au_k\\) 这里的\\(u_k+1=Au_k\\)可以是一个一阶差分方程，而\\(u_k=A^ku_0\\)就是方程的解 其可以写出Eigenvector的Linear Combination $$\\mathbf{u}_0 = c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + \\cdots + c_n \\mathbf{x}_n = S\\mathbf{c}$$\r$$A\\mathbf{u}_0 = c_1 \\lambda_1 \\mathbf{x}_1 + c_2 \\lambda_2 \\mathbf{x}_2 + \\cdots + c_n \\lambda_n \\mathbf{x}_n$$\r$$\\mathbf{u}_k = A^k \\mathbf{u}_0 = c_1 \\lambda_1^k \\mathbf{x}_1 + c_2 \\lambda_2^k \\mathbf{x}_2 + \\cdots + c_n \\lambda_n^k \\mathbf{x}_n = D^k S\\mathbf{c}$$\rFibonacci sequence 斐波那契数列 #\r斐波那契数列为0,1,1,2,3,4,8,13……其通项公式为\\(F_{k+2}=F_{k+1}+F_k\\) 令 $$\\mathbf{u}_k = \\begin{bmatrix} F_{k+2} \\\\ F_{k+1} \\end{bmatrix}$$\r$$F_{k+2} = F_{k+1} + F_k, \\quad F_{k+1} = F_{k+1} \\\\ \\text{写成矩阵形式为 } \\mathbf{u}_{k+1} = \\begin{bmatrix} 1 \u0026 1 \\\\ 1 \u0026 0 \\end{bmatrix} \\mathbf{u}_k$$\r所以现在A就是\\(\\begin{bmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{bmatrix}\\)，求解其Eigenvalues有 $$\\det(A - \\lambda I) = \\begin{vmatrix} 1-\\lambda \u0026 1 \\\\ 1 \u0026 -\\lambda \\end{vmatrix} = \\lambda^2 - \\lambda - 1 = 0$$\r\\(\\text{解得 } \\lambda_1 = \\frac{1 + \\sqrt{5}}{2}, \\quad \\lambda_2 = \\frac{1 - \\sqrt{5}}{2}\\)，且\\(\\mathbf{u}_k = A^k \\mathbf{u}_0 = c_1 \\lambda_1^k \\mathbf{x}_1 + c_2 \\lambda_2^k \\mathbf{x}_2\\) 由于\\(\\lambda_1\\)大于零，\\(\\lambda_2\\)小于零，则在k趋于无线的时候，\\(\\lambda_2^k\\)趋于零 从特征值可以求得对应的特征向量\\(\\mathbf{x}_1 = \\begin{bmatrix} \\lambda_1 \\ 1 \\end{bmatrix} \\text{ 的和 } \\mathbf{x}_2 = \\begin{bmatrix} \\lambda_2 \\ 1 \\end{bmatrix}\\) 在因为是二阶方程，而且矩阵\\(A - \\lambda I\\)是奇异矩阵，所以只要符合其中一个方程即可，立刻可以看出\\(\\begin{bmatrix} \\lambda_1 \\ 1 \\end{bmatrix}\\)是解\n$$\\text{从 } \\mathbf{u}_0 = \\begin{bmatrix} F_1 \\\\ F_0 \\end{bmatrix} = c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2, \\text{ 可以求得 } c_1 = -c_2 = \\frac{1}{\\sqrt{5}}$$$$\r\\begin{bmatrix} F_{100} \\\\ F_{99} \\end{bmatrix} = A^{99} \\begin{bmatrix} F_1 \\\\ F_0 \\end{bmatrix} = \\begin{bmatrix} \\lambda_1 \u0026 \\lambda_2 \\\\ 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} \\lambda_1^{99} \u0026 0 \\\\ 0 \u0026 \\lambda_2^{99} \\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} =\r\\begin{bmatrix} c_1 \\lambda_1^{100} + c_2 \\lambda_2^{100} \\\\ c_1 \\lambda_1^{99} + c_2 \\lambda_2^{99} \\end{bmatrix}. \\\\\r\\text{可知 } F_{100} \\approx c_1 \\lambda_1^{100}.\r$$\r","date":"Nov 25 2024","externalUrl":null,"permalink":"/docs/linearalgebra/la8.diagonalizationandeigenvalues/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/25/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eEigenvectors and Eigenvalues \r\n    \u003cdiv id=\"eigenvectors-and-eigenvalues\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#eigenvectors-and-eigenvalues\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e考虑Linear Transformation为一种Function，输入x而输出\\(Ax\\)\u003c/li\u003e\n\u003cli\u003eEigenvector即对于指定的Vector x，其Ax平行于x，有\n$$Ax=\\lambda x$$\u003c/li\u003e\n\u003cli\u003e其中x为A的Eigenvector，\\(\\lambda\\)为A的Eigenvalue\u003c/li\u003e\n\u003cli\u003e特征向量的定义要求\\(x \\neq 0\\)\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eZero Eigenvalue \r\n    \u003cdiv id=\"zero-eigenvalue\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#zero-eigenvalue\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e如果0为Matrix的Eigenvalue，则有\n$$Ax=0x=0$$\u003c/li\u003e\n\u003cli\u003eEigenvalue 0所对应的Vector Span出了Matrix的Null Space\u003c/li\u003e\n\u003cli\u003e如果矩阵A为不可逆矩阵，则0是其特征值之一\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eex. Projection Matrix \r\n    \u003cdiv id=\"ex-projection-matrix\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#ex-projection-matrix\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e对于Projection Matrix P，其Column Space中的任意Vector都会是一个Eigenvector\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e![[LA8.DiagonalizationandEigenvalues.png]]\u003c/p\u003e","title":"LA 8. Diagonalization and Eigenvalues","type":"docs"},{"content":"","date":"Nov 25 2024","externalUrl":null,"permalink":"/docs/linearalgebra/","section":"Docs","summary":"","title":"Linear Algebra","type":"docs"},{"content":" Last Edit 11/21/2024\n正交向量Orthogonal vectors #\r正交就是垂直（perpendicular）的另一种说法 两向量正交的判据之一是其点 积\\(x^Ty=y^Tx\\)=0 当两个向量的夹角为90度时，按照勾股定理（毕达哥拉斯定理 Pythagorean theorem）x，y满足 $$\\|\\mathbf{x}\\|^2 + \\|\\mathbf{y}\\|^2 = \\|\\mathbf{x} + \\mathbf{y}\\|^2 ,||\\mathbf{x}\\|^2 = \\mathbf{x}^T \\mathbf{x}$$\r零向量与所有向量都正交 Orthogonal Subspaces 正交子空间 #\r图中绘制空间成90度角，这是表示这两个空间正交 子空间S与子空间T正交，则S中的任意一个向量都和T中的任意向量正交 Nullspace is perpendicular to row space 零空间与行空间正交 #\r矩阵A的行空间和它的零空间正交。若x在零空间内，则有Ax=0 $$\\begin{bmatrix}\r\\text{row}_1 \\\\\r\\text{row}_2 \\\\\r\\vdots \\\\\r\\text{row}_m\r\\end{bmatrix} \\times \\mathbf{x} = \\begin{bmatrix}\r\\text{row}_1 \\cdot \\mathbf{x} \\\\\r\\text{row}_2 \\cdot \\mathbf{x} \\\\\r\\vdots \\\\\r\\text{row}_m \\cdot \\mathbf{x}\r\\end{bmatrix} = \\begin{bmatrix}\r0 \\\\\r0 \\\\\r\\vdots \\\\\r0\r\\end{bmatrix}\r$$\rx与矩阵A的行向量点积都等于0，则它和矩阵A行向量的线性组合进行点积也为0，所以x与A的行空间正交 同理可以证明列空间与左零空间正交 Orthogonal complements 正交补 #\r行空间和零空间不仅仅是正交，并且其维数之和等于n，我们称行空间和零空间为\\(R^n\\)空间内的正交补 Orthogonal complements Orthonormal #\r如果矩阵的列向量是互相垂直的单位向量，则它们一定是线性无关的 我们将这种向量称之为标准正交（orthonormal） $$例如\\begin{bmatrix}\r1 \\\\\r0 \\\\\r0 \\end{bmatrix}\r,\r\\begin{bmatrix}\r0 \\\\\r1\\\\\r0 \\end{bmatrix},\r\\begin{bmatrix}\r0 \\\\\r0 \\\\\r1 \\end{bmatrix}\r还有\r\\begin{bmatrix}\r\\cos \\theta \\\\\r\\sin \\theta\r\\end{bmatrix}\r,\r\\begin{bmatrix}\r\\cos \\theta \\\\\r\\sin \\theta\r\\end{bmatrix}$$\r## Orthonormal Vectors 标准正交向量\r$$q_i^T q_j = \\begin{cases} 0 \u0026 \\text{若 } i \\neq j \\\\\r1 \u0026 \\text{若 } i = j \\end{cases}$$\r指的是单位长度为1的（Unit Vector） ATA #\r下面讨论如何求解一个无解方程组Ax=b的解 它是一个\\(n\\times n\\)方阵，并且是对称阵\\((A^TA)^T=(A^TA)\\) 本章的核心内容就是当Ax=b无解的时候，求解\\(A^TAx\\)=\\(A^Tb\\)得到最优解 $$例：A = \\begin{bmatrix} 1 \u0026 1 \\\\ 1 \u0026 2 \\\\ 1 \u0026 5 \\end{bmatrix}, \\quad \\text{则} \\ A^T A = \\begin{bmatrix} 1 \u0026 1 \u0026 1 \\\\ 1 \u0026 2 \u0026 5 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 1 \\\\ 1 \u0026 2 \\\\ 1 \u0026 5 \\end{bmatrix} = \\begin{bmatrix} 3 \u0026 8 \\\\ 8 \u0026 30 \\end{bmatrix} \\text{是可逆的矩阵。}\r$$\r但是矩阵\\(A^TA\\)并不总是可逆 $$例：A = \\begin{bmatrix} 1 \u0026 3 \\\\ 1 \u0026 3 \\\\ 1 \u0026 3 \\end{bmatrix}, \\quad \\text{则} \\ A^T A = \\begin{bmatrix} 1 \u0026 1 \u0026 1 \\\\ 3 \u0026 3 \u0026 3 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 3 \\\\ 1 \u0026 3 \\\\ 1 \u0026 3 \\end{bmatrix} = \\begin{bmatrix} 3 \u0026 9 \\\\ 9 \u0026 27 \\end{bmatrix} \\text{是不可逆矩阵。}\r$$\rProjections in 2D 2D中的投影 #\r投影的几何解释便是：在向量a的方向上寻找与向量b距离最近的一 点 这个距离最近的点p就位于穿过b点并与向量a正交的直线 与向量a所在直线的交点上 则p就是b在a上的投影 如果我们将向量p视为b 的一种近似，则长度e=b-p就是这一近似的误差 于是便有方程\\(a^T(b-xa)=0\\) 因为向量a和b是列向量，在计算它们的点积（即内积）时，通常需要将其中一个向量转置成行向量，这样才能进行矩阵乘法并得到标量\n解得 $$\\begin{equation}\rx = \\frac{\\mathbf{a}^T \\mathbf{b}}{\\mathbf{a}^T \\mathbf{a}}, \\quad p = a x = \\mathbf{a} \\frac{\\mathbf{a}^T \\mathbf{b}}{\\mathbf{a}^T \\mathbf{a}}.\r\\end{equation}\r$$\r如果方程的自变量发生改变，p的改变量 如果b变为原来的2倍，则p也变为原来的2倍 而如果a变为原来的2倍， p不发生变化 （从几何角度考虑也很合理） Projection Matrix in 2D #\r$$proj_p=Pb$$\r其中P为投影矩阵 $$\\begin{equation}\rp = a x = a \\frac{\\mathbf{a}^T \\mathbf{b}}{\\mathbf{a}^T \\mathbf{a}}. \\quad \\text{则有} \\quad P = \\frac{\\mathbf{a} \\mathbf{a}^T}{\\mathbf{a}^T \\mathbf{a}}.\r\\end{equation}\r$$\r其分子\\(aa^T\\)是一个矩阵，而分母\\(a^Ta\\)是一个数 观察这个矩阵可知，矩阵P的列空间就是向量a所在的直线 矩阵的秩是1 (直线) Property of projection 投影的性质 #\rSymmetry 对称性 #\r对P做一次转置，其还是P 则P是对称矩阵 Apply Twice #\r如果做两次投影则有P2b=Pb，这是因为 第二次投影还在原来的位置。 因此矩阵P有如下性质：\\(P^T=P,P^2=P\\) Closest vector 最短向量 #\r方程Ax=b有可能无解 当出现比Unknown更多的Equations的时候，只能求解最优解 Ax一定在矩阵A的列空间之内，但是b不一定， p是b在Colunm Space上的Projection，所以其是最优解 将问题转化为求解\\(A\\hat x=p\\) Closest Vector Theorem #\rSuppose V is a subspace of Rn and \\(\\vec x ∈ R^n\\). The closest vector in V to \\(\\vec x\\) is given by \\(ProjV (\\vec x )\\) In other words, \\(|\\text{Proj}_V(\\vec{x}) - \\vec{x}| \\leq |\\vec{v} - \\vec{x}|~\\text{for any } \\vec{v} \\in V\\) 投影的方向到向量上最短的点就是其在改方向上的投影到向量的距离\nProof #\r$$\\|\\vec{v} - \\vec{x}\\|^2 = \\|\\vec{v} + \\text{Proj}_V(\\vec{x}) - \\text{Proj}_V(\\vec{x}) - \\vec{x}\\|^2 \\\\\r= \\|\\vec{v} - \\text{Proj}_V(\\vec{x}) + \\text{Proj}_V(\\vec{x}) - \\vec{x}\\|^2$$\r$$\\|\\vec{v} - \\vec{x}\\|^2 = \\|\\vec{v} - \\vec{x}^\\parallel + \\vec{x}^\\parallel - \\vec{x}\\|^2 \\\\\r= \\|\\vec{v} - \\vec{x}^\\parallel - \\vec{x}^\\perp\\|^2$$\r$$\\|\\vec{v} - \\vec{x}\\|^2 = \\|\\vec{v} - \\vec{x}^\\parallel\\|^2 + \\|-\\vec{x}^\\perp\\|^2 \\\\\r= \\|\\vec{v} - \\vec{x}^\\parallel\\|^2 + \\|\\vec{x}^\\perp\\|^2$$\r可以发现\\(|\\vec{v} - \\vec{x}|^2\\)最小的值出现在\\(\\vec{v} = \\vec{x}^\\parallel\\)的时候 Orthogonal projection 正交投影 #\r注意Orthogonal Projection和Orthogonal Linear Transformation是完全不同的东西 之所以叫Orthogonal Projection指的是这个Projection就是最一般的情况，就是一般所理解的正交于一个Subspace的投影\n在上面的\\(P = A (A^T A)^{-1} A^T\\)中，之所以不拆成\\(P=AA^{-1}(A^T)^{-1}A^T=I\\)，是因为A并不是Square Matrix，即不存在Inverse 当A为Square Matrix的时候，即m=n，Input dim = Output dim，这个Projection也就变成了Identity Matrix，即将自身Project Into自己的空间 但是即使是Projecct到自己的Space，其中仍会包含Linear Transformation Orthogonal Projection Formula 正交投影公式 #\r正交投影公式是通过公式计算一个正交的投影向量在目标子空间的投影，和Orthogonal Linear Transformation无关\n对于一组Orthogonal的Basis，将Vector投影到改Space的公式为 $$\\text{Proj}_V(\\vec{x}) = \\vec{u}_1 (\\vec{u}_1 \\cdot \\vec{x}) + \\vec{u}_2 (\\vec{u}_2 \\cdot \\vec{x})$$\r举例来说，对于一组Orthonormal Vector $$u = {\\begin{bmatrix}\\frac{1}{\\sqrt{2}} \\0 \\\\frac{1}{\\sqrt{2}}\\end{bmatrix},\\begin{bmatrix}-\\frac{1}{\\sqrt{2}} \\0 \\\\frac{1}{\\sqrt{2}}\\end{bmatrix}$$ \\([2,2,2]^T\\)的Projection为 $$\\begin{align*}\r\u0026= \\begin{bmatrix}\r\\frac{1}{\\sqrt{2}} \\\\\r0 \\\\\r\\frac{1}{\\sqrt{2}}\r\\end{bmatrix}\r\\left( \\begin{bmatrix}\r\\frac{1}{\\sqrt{2}} \\\\\r0 \\\\\r\\frac{1}{\\sqrt{2}}\r\\end{bmatrix} \\cdot\r\\begin{bmatrix}\r\\sqrt{2} \\\\\r0 \\\\\r\\sqrt{2}\r\\end{bmatrix} \\right)\r+\r\\begin{bmatrix}\r-\\frac{1}{\\sqrt{2}} \\\\\r0 \\\\\r\\frac{1}{\\sqrt{2}}\r\\end{bmatrix}\r\\left( \\begin{bmatrix}\r-\\frac{1}{\\sqrt{2}} \\\\\r0 \\\\\r\\frac{1}{\\sqrt{2}}\r\\end{bmatrix} \\cdot\r\\begin{bmatrix}\r\\sqrt{2} \\\\\r0 \\\\\r\\sqrt{2}\r\\end{bmatrix} \\right) \\\\\r\u0026= 2 \\begin{bmatrix}\r\\frac{1}{\\sqrt{2}} \\\\\r0 \\\\\r\\frac{1}{\\sqrt{2}}\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r\\sqrt{2} \\\\\r0 \\\\\r\\sqrt{2}\r\\end{bmatrix}\r\\end{align*}$$\rProjection Matrix 正交投影矩阵 #\r正交投影矩阵，将向量正交投影到Subspace上的一个矩阵，A可以是任意矩阵，不是非得是Orthogonal Matrix，其和正交投影公式干的是一样的事，不过用了不同的表达方式\n在\\(R^3\\)空间内，如何将向量b投影到它距离平面最近的一点p？ 如果a1和a2构成了平面的一组基，则平面就是矩阵\\(A=[a1,a2]\\)的列空间 \\(e=b-p\\)是垂直于平面的 已知p在平面内，于是有\\(p=\\hat x_1a_1+\\hat x_2a_2=A\\hat x\\) 而\\(e=b-p=b- A\\hat x\\)正交于平面，因此e与\\(a_1\\),\\(a_2\\)均正交 因此可以得到：\\(a_1^T(b-A\\hat x )=0\\)并且\\(a_2^T(b-A\\hat x )=0\\) 因为a1和a2分别为矩阵A的列向量，即\\(a1^T\\)和\\(a2^T\\)为矩阵\\(A^T\\)的行向量 \\(A^T(b-A\\hat x)=0\\) 由于\\(b-A\\hat x\\)在于矩阵AT的零空间\\(N(A^T)\\)里，从上一讲讨论子空间的正交性可知，向量e与矩阵A的列空间正交，这也正是方程的意义 $$\\begin{align}\r\\hat{x} \u0026= (A^T A)^{-1} A^T b \\\\\rp \u0026= A \\hat{x} = A (A^T A)^{-1} A^T b \\\\\rP \u0026= A (A^T A)^{-1} A^T=\\frac{AA^T}{A^TA}\r\\end{align}$$\r注意区别大小写P\n对于上面的等式在dim = 1中则是\\(\\frac{\\mathbf{a} \\mathbf{a}^T}{\\mathbf{a}^T \\mathbf{a}}\\) 投影矩阵\\(P=A(A^TA)^{-1}A^T\\)，当它作用于向量b，相当于把b投影到矩阵A的列空间 Case when b is in column Space A #\r当b已经在A的列空间之中，有\\(Ax=b\\) $$\\begin{align*}\r\\mathbf{Pb} \u0026= \\mathbf{A}(\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b} \\\\\r\u0026= \\mathbf{A}(\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{Ax} \\\\\r\u0026= \\mathbf{A}((\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{A}) \\mathbf{x} \\\\\r\u0026= \\mathbf{Ax} = \\mathbf{b}\r\\end{align*}$$\rCase when b orthorgal to column Space A #\r如果向量b与A的列空间正交，即向量b在矩阵A的左零空间N(A)中 在Left Null Space的意义在于\\(A^Tb=0\\)，所以\\(Pb=0\\) $$\\mathbf{Pb} = \\mathbf{A}(\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b} = \\mathbf{A}(\\mathbf{A}^\\top \\mathbf{A})^{-1} (\\mathbf{A}^\\top \\mathbf{b}) = \\mathbf{A}(\\mathbf{A}^\\top \\mathbf{A})^{-1} 0 = 0$$\rI−P 的效果: I - P 则是从向量x中移除其在A的列空间上的分量，留下的部分即为x在A的列空间的正交补上的分量 这表明I - P将向量x投影到A的列空间的正交补空间上 Orthogonal Projection Matrix in Orthogonal Basis 矩阵为正交矩阵的正交投影矩阵 #\r正交矩阵的正交投影矩阵的正确读法是，当正交投影矩阵的矩阵为正交矩阵的情况下的正交投影，即改投影矩阵的A为Q的情况下，改投影将不体现“投影”的作用，而是在原空间中做Orthogonal Linear Transformation\nOrthogonal Projection Matrix必须是Square Matrix $$\\mathbf{P} = \\mathbf{Q} (\\mathbf{Q}^\\top \\mathbf{Q})^{-1} \\mathbf{Q}^\\top$$\r- 因为\\\\(Q^TQ=I\\Rightarrow P=QQ^T\\\\)\r如果Q为Square Matrix，则是一个投影到自身空间的Matrix，即\\(P=I\\)，因为Q的列向量张成了整个空间，投影过程不会对向量有任何改变 就上面的例子来说，其Projection在用了Matrix后可以得到 Orthogonal Matrix 正交矩阵 #\r注意这里定义的不再是投影了，而是前面提到的矩阵为正交矩阵的正交投影矩阵，是一个东西\nOrthogonal Matrix 正交矩阵 #\rConsider an n × n matrix A The matrix A is orthogonal if and only if \\(A^TA = I\\) or, equivalently, if \\(A^{−1} = A^T\\) Orthogonal Matrix的Column Vector需要Norm = 1 $$\\mathbf{Q} = \\begin{bmatrix} 0 \u0026 0 \u0026 1 \\\\ 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \\end{bmatrix}, \\quad\r\\text{则有 } \\mathbf{Q}^\\top = \\begin{bmatrix} 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\\\ 1 \u0026 0 \u0026 0 \\end{bmatrix}\r$$\r再比如\\(\\begin{bmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; -1 \\end{bmatrix} \\text{ 并不是正交矩阵}.\\) 因为其Norm为2，\\(\\mathbf{Q} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; -1 \\end{bmatrix}\\)，调整后便可以了 一个例子便是Rotation Matrix $$T(\\vec{x}) = \\begin{bmatrix}\r\\cos(\\theta) \u0026 -\\sin(\\theta) \\\\\r\\sin(\\theta) \u0026 \\cos(\\theta)\r\\end{bmatrix} \\vec{x}$$\r对于Orthogonal Matrix来说，\\(Q^TQ=I\\) $$\\mathbf{Q} = \\begin{bmatrix} \\mathbf{q}_1 \u0026 \\cdots \u0026 \\mathbf{q}_n \\end{bmatrix}, \\quad \\mathbf{Q}^\\top \\mathbf{Q} = \\begin{bmatrix} \\mathbf{q}_1^\\top \\\\ \\vdots \\\\ \\mathbf{q}_n^\\top \\end{bmatrix} \\begin{bmatrix} \\mathbf{q}_1 \u0026 \\cdots \u0026 \\mathbf{q}_n \\end{bmatrix} = \\mathbf{I}$$\rOrthogonal transformations preserve orthogonality 角度不变性 #\rOrthogonal linear transformations不仅保持向量的长度，也保持向量间的角度和orthogonality 正交性 变换前后角度不变的变换是Orthogonal transformation\nProof #\r根据Pythoarorian Theorm $$||\\vec x^2+\\vec y^2||=||\\vec x^2||+||\\vec y^2||$$\r需要证明\\(|T(\\vec{v}) + T(\\vec{w})|^2 = |T(\\vec{v})|^2 + |T(\\vec{w})|^2\\) 对于两个Orthogonal Vector \\(\\vec v ~\u0026amp; ~\\vec w\\)，T是Linear Transformation，有 $$\\|T(\\vec{v}) + T(\\vec{w})\\|^2 = \\|T(\\vec{v} + \\vec{w})\\|^2$$\r由于Orthogonal Linear Transformation preserves the norm of vector $$\\|T(\\vec{v} + \\vec{w})\\|^2=||\\vec v+\\vec w||^2$$\r根据Pythoarorian Theorm $$||\\vec v+\\vec w||^2=||\\vec v^2||+||\\vec w||^2$$\r同理 $$||\\vec v^2||+||\\vec w||^2= \\|T(\\vec{v})\\|^2 + \\|T(\\vec{w})\\|^2$$\rOrthogonal linear transformations preserves dot product 点积不变性 #\rT : \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\) is an orthogonal transformation if and only if $$ T(\\vec{v}) \\cdot T(\\vec{w}) = \\vec{v} \\cdot \\vec{w} \\text{ for all } \\vec{v}, \\vec{w} \\in \\mathbb{R}^n$$\rif \\(\\vec{u}\\) and \\(\\vec{v}\\) are orthogonal, then \\(T(\\vec{u})\\) and \\(T(\\vec{v})\\) are also orthogonal 变换前后dot product不变的变换就是Orthogonal Transformation\nProof #\rT : \\(R^n → R^n\\) is an orthogonal linear transformation $$T(\\vec{u}) \\cdot T(\\vec{v}) = (u_1 T(\\vec{e}_1) + \\dots + u_n T(\\vec{e}_n)) \\cdot (v_1 T(\\vec{e}_1) + \\dots + v_n T(\\vec{e}_n))$$\r根据\\(q_i^T q_j = \\begin{cases} 0 \u0026amp; \\text{若 } i \\neq j \\1 \u0026amp; \\text{若 } i = j \\end{cases}\\) $$T(\\vec{u}) \\cdot T(\\vec{v}) = u_1 v_1 T(\\vec{e}_1) \\cdot T(\\vec{e}_1) + \\ldots + u_n v_n T(\\vec{e}_n) \\cdot T(\\vec{e}_n)$$\r$$= u_1 v_1 \\|T(\\vec{e}_1)\\|^2 + \\ldots + u_n v_n \\|T(\\vec{e}_n)\\|^2 \\\\\r= u_1 v_1 + \\ldots + u_n v_n = \\vec{u} \\cdot \\vec{v}$$\rConverse statement of orthogonal linear transformation 逆命题的成立 #\rif a linear map preserves orthonormality, it should preserve length and hence is an orthogonal map Orthogonal transformations and orthonormal bases 正交基底保证正交变换 #\rA linear transformation T : \\(R^n → R^n\\) is an orthogonal transformation if and only if the vectors \\(T (\\vec e_1), T(\\vec e_2), . . . , T (\\vec e_n)\\) form an orthonormal basis for \\(R^n\\) 当Column Space为Orthogonal Vector的时候，Matrix为Orthogonal Transformation\nProof #\r$$\\|T(\\vec{x})\\|^2 = \\|T(x_1 \\vec{e}_1 + x_2 \\vec{e}_2 + x_3 \\vec{e}_3)\\|^2$$\r$$=\\|T(x_1 \\vec{e}_1) + T(x_2 \\vec{e}_2) + T(x_3 \\vec{e}_3)\\|^2 \\quad (\\text{by linearity of } T)$$\r$$= \\|T(x_1 \\vec{e}_1)\\|^2 + \\|T(x_2 \\vec{e}_2)\\|^2 + \\|T(x_3 \\vec{e}_3)\\|^2 \\quad (\\text{by Pythagoras})$$\r$$= x_1^2 \\|T(\\vec{e}_1)\\|^2 + x_2^2 \\|T(\\vec{e}_2)\\|^2 + x_3^2 \\|T(\\vec{e}_3)\\|^2 \\quad (\\text{by linearity of } T)$$\r$$= x_1^2 + x_2^2 + x_3^2 =||\\vec x||^2~(\\text{since columns are length } 1)$$\r$$\\mathbf{Q} = \\frac{1}{3} \\begin{bmatrix} 1 \u0026 -2\\\\ 2 \u0026 -1 \\\\ 2 \u0026 2\r\\end{bmatrix}, \\text{ 我们可以拓展其成为正交矩阵 } \\frac{1}{3} \\begin{bmatrix} 1 \u0026 -2 \u0026 2 \\\\ 2 \u0026 -1 \u0026 -2 \\\\ 2 \u0026 2 \u0026 1 \\end{bmatrix}$$\rHadamard Matrix #\r$$\\mathbf{Q} = \\frac{1}{2} \\begin{bmatrix} 1 \u0026 1 \u0026 1 \u0026 1 \\\\ 1 \u0026 -1 \u0026 1 \u0026 -1 \\\\ 1 \u0026 1 \u0026 -1 \u0026 -1 \\\\ 1 \u0026 -1 \u0026 -1 \u0026 1 \\end{bmatrix}$$\r仅包含-1和1的Orthogonal Matrix $$\\text{Proj}_V(\\vec{x}) = Q Q^T \\vec{x} =\r\\begin{bmatrix}\r\\frac{1}{\\sqrt{2}} \u0026 -\\frac{1}{\\sqrt{2}} \\\\\r0 \u0026 0 \\\\\r\\frac{1}{\\sqrt{2}} \u0026 \\frac{1}{\\sqrt{2}}\r\\end{bmatrix}\r\\begin{bmatrix}\r\\frac{1}{\\sqrt{2}} \u0026 0 \u0026 \\frac{1}{\\sqrt{2}} \\\\\r-\\frac{1}{\\sqrt{2}} \u0026 0 \u0026 \\frac{1}{\\sqrt{2}}\r\\end{bmatrix}\r\\begin{bmatrix}\r\\sqrt{2} \\\\\r\\sqrt{2} \\\\\r\\sqrt{2}\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r1 \u0026 0 \u0026 0 \\\\\r0 \u0026 0 \u0026 0 \\\\\r1 \u0026 0 \u0026 0\r\\end{bmatrix}\r\\begin{bmatrix}\r\\sqrt{2} \\\\\r\\sqrt{2} \\\\\r\\sqrt{2}\r\\end{bmatrix}\r=\r\\begin{bmatrix}\r\\sqrt{2} \\\\\r0 \\\\\r\\sqrt{2}\r\\end{bmatrix}\r$$\rGram-Schmidt 施密特正交化 #\r一般来说，要获得Orthogonal Matrix，先要做一步化简到该形式，这一步的名字就 Gram-Schmidt 从两个线性无关的向量a和b开始，它们张成了一个空间 我们的目标是找到两个标准正交的向量q1，q2能张成同样的空间 Schmidt给出的结论是如果 我们有一组正交基A和B，那么我们令它们除以自己的长度就得到标准正交基 $$\\mathbf{q}_1 = \\frac{\\mathbf{A}}{\\|\\mathbf{A}\\|}, \\quad \\mathbf{q}_1 = \\frac{\\mathbf{A}}{\\|\\mathbf{A}\\|}\r$$\r当确认了一个方向后，要求出orthogonal于改方向的Vector则就是将b投影到a的方向，取B=b-p（e） $$\\mathbf{B} = \\mathbf{b} - \\frac{\\mathbf{A}^\\top \\mathbf{b}}{\\mathbf{A}^\\top \\mathbf{A}} \\mathbf{A}$$\r通过两边乘上\\(A^T\\)证明其Orthogonal性质 $$A^T\\mathbf{B} = A^T(\\mathbf{b} - \\frac{\\mathbf{A}^\\top \\mathbf{b}}{\\mathbf{A}^\\top \\mathbf{A}} \\mathbf{A})=0$$\rThird Vector #\r同理，由ABC三个Vector为 $$\\mathbf{q}_1 = \\frac{\\mathbf{A}}{\\|\\mathbf{A}\\|}, \\quad \\mathbf{q}_1 = \\frac{\\mathbf{A}}{\\|\\mathbf{A}\\|}\\quad \\mathbf{q}_3 = \\frac{\\mathbf{C}}{\\|\\mathbf{C}\\|}$$\r$$\\mathbf{C} = \\mathbf{c} - \\frac{\\mathbf{A}^\\top \\mathbf{c}}{\\mathbf{A}^\\top \\mathbf{A}} \\mathbf{A} - \\frac{\\mathbf{B}^\\top \\mathbf{c}}{\\mathbf{B}^\\top \\mathbf{B}} \\mathbf{B}\r$$\rex. Two Vectors #\r$$\\mathbf{a} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\text{则有 } \\mathbf{A} = \\mathbf{a}, \\quad \\mathbf{B} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix} - \\frac{3}{3} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$$\r- 则有Orthonormal Matrix Q\r$$\\mathbf{Q} = \\begin{bmatrix} \\mathbf{q}_1 \u0026 \\mathbf{q}_2 \\end{bmatrix} = \\begin{bmatrix} 1 / \\sqrt{3} \u0026 0 \\\\ 1 / \\sqrt{3} \u0026 -1 / \\sqrt{2} \\\\ 1 / \\sqrt{3} \u0026 1 / \\sqrt{2} \\end{bmatrix}$$\rLeast Squares 最小二乘 #\rFitting a line，拟合曲线 假设有三个数据点{(1,1), (2,2), (3,2)} 假设直线方程 \\(b=Dt+C\\) 将三个点带入方程就有\\(C+D=1,C+2D=2,C+3D=2\\) $$\\left[\r\\begin{array}{cc}\r1 \u0026 1 \\\\\r1 \u0026 2 \\\\\r1 \u0026 3 \\\\\r\\end{array}\r\\right]\r\\left[\r\\begin{array}{c}\rC \\\\\rD \\\\\r\\end{array}\r\\right]\r=\r\\left[\r\\begin{array}{c}\r1 \\\\\r2 \\\\\r2 \\\\\r\\end{array}\r\\right]\r$$\r可以发现这个Equation是无解的，但目的在于找到最优解 即方程\\(A^TA\\hat x =A^Tb\\)的解 在这之前需要定义一个Error来判断那条直线为最优解，定义为\\(||e^2||=||Ax-b||^2={e_1}^2+{e_2}^2+{e_3}^2\\) 在不存在Outlier 离群值的时候是一种非常好的Regression way \\(C+Dt分别为p1，p2和p3\\)，它们是满足方程并最接近于b的结果 现在需要求解\\(\\hat x= \\left[\\begin{array}{c}C \\D \\\\end{array}\\right]\\)和p \\(A^TA\\hat x=A^Tb\\) 因为\\(A^T(b-A\\hat x)=0\\)\n$$A^TA=\\left[\r\\begin{array}{ccc}\r1 \u0026 1 \u0026 1 \\\\\r1 \u0026 2 \u0026 3 \\\\\r\\end{array}\r\\right]\r\\left[\r\\begin{array}{ccc}\r1 \u0026 1\\\\\r1 \u0026 2 \\\\\r1 \u0026 3\\\\\r\\end{array}\r\\right]\r=\r\\left[\r\\begin{array}{ccc}\r3 \u0026 6\\\\\r6 \u0026 14\\\\\r\\end{array}\r\\right], A^Tb=\\left[\r\\begin{array}{ccc}\r1 \u0026 1 \u0026 1 \\\\\r1 \u0026 2 \u0026 3 \\\\\r\\end{array}\r\\right]\r\\left[\r\\begin{array}{ccc}\r1\\\\\r2\\\\\r2\\\\\r\\end{array}\r\\right]\r=\\left[\r\\begin{array}{cc}\r5 \\\\\r11 \\\\\r\\end{array}\r\\right]$$\r$$\\quad \\text{则有}\r\\left[\r\\begin{array}{cc}\r3 \u0026 6 \\\\\r6 \u0026 14 \\\\\r\\end{array}\r\\right]\r\\left[\r\\begin{array}{c}\r\\hat{C} \\\\\r\\hat{D} \\\\\r\\end{array}\r\\right]\r=\r\\left[\r\\begin{array}{c}\r5 \\\\\r11 \\\\\r\\end{array}\r\\right]$$\r解得\\(\\hat C=2/3,\\hat D=1/2\\) 亦可以通过求Partical Derivative的方法 $$e1^2 + e2^2 + e3^2 = (C + D - 1)^2 + (C + 2D - 2)^2 + (C + 3D - 2)^2$$ $$展开结果为2 e =3C2+14D2+9-10C-22D+12CD$$ $$求偏导为12C-20+24D=0； 28D-22+12C=0。与A^TA\\hat x=A^Tb相同$$ 于是得到结果 可以验证p与e与A的Column Space Orthogonal 矩阵ATA #\r证明：若A的列向量线性无关时，矩阵\\(A^TA\\)为可逆矩阵 要证明此，假设存在x使得\\(A^TAx=0\\)，后证明x只能是Zero Vector 第一步将灯饰两边同时乘以\\(x^T\\)，有\\(x^TA^TAx=0\\) 可以重写成\\((Ax)^T(Ax)=0\\Rightarrow Ax=0\\) 由于A的Column Vector是Linearly Independent的，所以只有\\(x=0时有A^TAx=0\\) 即\\(A^TAx\\) is invertible ","date":"Nov 21 2024","externalUrl":null,"permalink":"/docs/linearalgebra/la7.orthogonalprojection/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 11/21/2024\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e正交向量Orthogonal vectors \r\n    \u003cdiv id=\"%E6%AD%A3%E4%BA%A4%E5%90%91%E9%87%8Forthogonal-vectors\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E6%AD%A3%E4%BA%A4%E5%90%91%E9%87%8Forthogonal-vectors\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/LinearAlgebra_Static/LA7.Orthogonal\u0026amp;Projection/LA7.Orthogonal\u0026amp;Projection-3.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"LA 7. Orthogonal and Projection","type":"docs"},{"content":" Last Edit: 11/19/24\nSolution 解 #\r对于一个Vector \\(a=[1,2]^T\\)和一个直线\\(y=0\\) 要研究\\(c[1,2]^T=0\\)的问题的时候，很明显不存在Non-Trivial Solution 由于a在\\(R^2\\)中，而\\([1,0]^T\\)仅Span出了\\(R^2\\)中的一个Subspace，其Dim=1 Least Error Solution (Optimization) 最优解 #\r但是对于a到直线的距离仍存在Optimized Solution 最优解出现在\\([1,2]^T\\)的终点在\\([1,0]\\)方向上最短的情况，即一个Error最小的情况 可以从a出发找到无数个到达向量\\([1,0]^T\\)方向的向量 而其中最短的则是\\(\\vec {e}\\) 也可以说\\(\\vec e\\)是Equation Error最小的Solution R^3 Case #\r对于向量\\([1,1,3]^T\\)来说，要计算其到达平面\\(x+y-2z=0\\)的最短距离 \\(\\vec e\\) 则代表了这一个距离 则e的起点在Plane\\(x+y-2z=0\\)上的位置就是这一个最优解 Projection 投影 #\r可以发现，要找到最优解，一个合理的办法是从Projection开始 在上图中p就是a在\\([1,0]^T\\)方向上的投影 则有\\(e=b-p\\) 而最小化这个e就是目标，这个目标通过Orthogonal 正交实现 具体来说从A出发的orthogonal to p的vector e就是这个Optimized Solution \\(R^3\\)中同理，只不过是将投影的改为了Plane 于是便有\\(e=(b-A\\hat x)\\) 要让e垂直于Plane \\(A=[a1,a2]\\) 有\\(a_1^T(b-A\\hat x )=0\\)并且\\(a_2^T(b-A\\hat x )=0\\) 于是可以得到公式 $$A^T(b-A\\hat x)=0\\Rightarrow A^TA\\hat x=A^Tb$$ Least Squares 最小二乘 #\r直接进入例子 对于三个点{(1,1), (2,2), (3,2)} 构建方程\\(y=wx\\) 带入点后得到\\(1=w,2=2w,2=3w\\) 通过\\(A^TA\\hat x=A^Tb\\) $$A^T A = \\begin{bmatrix}\r1 \u0026 2 \u0026 3\r\\end{bmatrix}\r\\begin{bmatrix}\r1 \\\\\r2 \\\\\r3\r\\end{bmatrix}\r= 1^2 + 2^2 + 3^2\r= 1 + 4 + 9\r= 14$$\r$$A^T b = \\begin{bmatrix}\r1 \u0026 2 \u0026 3\r\\end{bmatrix}\r\\begin{bmatrix}\r1 \\\\\r2 \\\\\r2\r\\end{bmatrix}\r= 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 2\r= 1 + 4 + 6\r= 11\r$$\r便有\\(14w=11\\Rightarrow w=\\frac{11}{14}\\) 几何角度 #\r那么上面的公式在几何空间中干的事就是 找到了这个红色的Vector，也就是最小的e 同理运用到最经典的\\(y=wx+b\\)也是一样 \\(1=w+b,2=2w+b,2=3w+b\\) $$A^T A = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 1 \u0026 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 1 \\\\ 2 \u0026 1 \\\\ 3 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1^2 + 2^2 + 3^2 \u0026 1 + 2 + 3 \\\\ 1 + 2 + 3 \u0026 3 \\end{bmatrix} = \\begin{bmatrix} 14 \u0026 6 \\\\ 6 \u0026 3 \\end{bmatrix}$$\r$$A^T \\mathbf{b} = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 1 \u0026 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 2 \\\\ 1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 5 \\end{bmatrix}$$\r$$\\begin{bmatrix} 14 \u0026 6 \\\\ 6 \u0026 3 \\end{bmatrix} \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 5 \\end{bmatrix} $$\r于是有\\(w=\\frac{1}{2},b=\\frac{2}{3}\\) 同理几何上找到了向量在Plane上的投影之间的最小Error 所以这就是\\(A^TA\\hat x=A^Tb\\) 在Linear Regression的作用 需要知道的是这个方法（Normal Equation）求得的是解析解，在一般在feature \u0026lt; 10000的时候采用，但是过程可能不可逆 ","date":"Nov 19 2024","externalUrl":null,"permalink":"/docs/linearalgebra/leastsquare/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/19/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eSolution 解 \r\n    \u003cdiv id=\"solution-%E8%A7%A3\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#solution-%E8%A7%A3\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e对于一个Vector \\(a=[1,2]^T\\)和一个直线\\(y=0\\)\u003c/li\u003e\n\u003cli\u003e要研究\\(c[1,2]^T=0\\)的问题的时候，很明显不存在Non-Trivial Solution\u003c/li\u003e\n\u003cli\u003e由于a在\\(R^2\\)中，而\\([1,0]^T\\)仅Span出了\\(R^2\\)中的一个Subspace，其Dim=1\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/LinearAlgebra_Static/LeastSquare/LeastSquare-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"LA Least Square","type":"docs"},{"content":"","date":"Nov 18 2024","externalUrl":null,"permalink":"/docs/displays/","section":"Docs","summary":"","title":"Displays","type":"docs"},{"content":" Last Edit: 18/11/24\nLight #\r当我们讨论为什么像聚甲基丙烯酸甲酯（PMMA）这样的材料能够透明，而像玻璃态金属或单晶金属（如硅和镍基合金）则是不透明的时候，理解光的本质及其与材料的交互作用是至关重要的。 材料是否透明，很大程度上取决于其电子结构，这决定了它如何吸收光 PMMA #\rPMMA we were able to have a transparent polymer because PMMA is 100 % amorphous 在非晶态的材料中，没有晶体和非晶体区域之间的界限，这种界限在晶体材料中可能会散射光线。 PMMA中不存在这样的晶界，光线可以自由穿过，从而保持材料的透明性 Electromagnetic Spectrum 电磁光谱 #\r可见光只是在包含了radio waves, microwaves, infrared radiation, ultraviolet radiation, x-rays, and gamma rays在内的大量辐射光谱中的极小部分 在# Electromagnetic Spectrum中，光子是Electromagnetic Radiation的基本粒子单位，承载能量和信息，跨越不同wave length的电磁波，包括无线电波、微波、红外线、可见光、紫外线、X射线和伽马射线 Electromagnetic Radiation 电磁辐射 #\rPhotons 光子 #\r光子可以在空间中传播，不需要介质，是能量从一个地点传到另一个地点的方式 光的能量不是连续的，而是Quantized 量子化的 $$E = \\frac{hC}{\\lambda}$$\n\\(h~(Plank~ ~Constant) = 6.626×10^{−34}J⋅s\\) \\(c~(Light~speed)=3\\times 10^8 m\\cdot s^{-1}\\) \\(λ~(Wavelength)~m\\cdot s^{-1}\\) 其更加常见的形式为 $$E=hv$$ \\(ν\\) : frequency of the light in Hertz \\((1 Hz = \\frac1s)​\\) Photoelectric effect 光电效应 #\r由Albert Einstein于1905年提出 当光照射到金属表面时，光子将其能量传递给金属内的电子，如果这些光子的能量足够高，超过了金属的逸出功，电子就会被释放出金属表面 由于每个光子的能量由其频率决定，其能量和频率的关系则由公式\\(E=hv\\)给出 逸出功（Work Function）：逸出功是指电子从固体表面逃逸到真空中所需的最小能量。不同材料有不同的逸出功。 光电子（Photoelectron）：如果光子的能量高于逸出功，电子就会被释放，成为光电子 电子的动能（Kinetic Energy）：释放的光电子将具有一个最大动能，这可以通过 \\(Kmax=hf−ϕK_{\\text{max}}\\)来计算，其中 \\(\\phi\\) 是逸出功 Wave-Particle Duality 波粒二象性 #\r指物质（如光和电子）在某些情况下表现为波动性，而在其他情况下表现为粒子性。 Photoelectric effect展示了光的粒子性，而光的波动性则通过其他实验如双缝实验（Double-slit Experiment）得到证明 Electron volt 电子伏特（eV） #\r计算光子能量：使用的公式是 \\(E = \\frac{hc}{\\lambda}\\)， 650纳米的红激光，将其转换成米\\(650 \\times 10^{-9} \\text{ m}\\)，计算得到的能量是 \\(3.06 \\times 10^{-19}\\)焦耳 虽然结果是正确的，但由于在量子力学和粒子物理中常常处理非常小的能量数值，使用焦耳（Joules）单位可能会显得笨拙和难以理解 于是引入了Electron Volt的概念 电子伏特是基于电子通过1伏特电势差加速后获得的能量。一个电子伏特定义为 \\(1.602 \\times 10^{-19}\\) 焦耳 \\(E_{\\text{Red Photon}} = \\frac{3.06 \\times 10^{-19} , \\text{J}}{1.602 \\times 10^{-19} , \\text{J/eV}} = 1.91 , \\text{eV}\\) Atom 原子 #\r一个基本的Atom Structure可以概括如下 Most of the mass of an atom comes from the nucleus 原子核 Isotopes 同位素 #\rNumber of proton in nucleus决定了原子是什么元素 所以# of Protones也称为Atomic Number 原子序数 如Carbon-14, 写作\\(C_{14}\\) 对于6个Proton来说是Carbon，5个是Boron 硼，7个是Nitrogen 氮气 The Bohr Model of the Atom 原子的玻尔模型 #\r历史上出现了许多个不同的Atom Structure 第一个取得了重大进展的则是Bohr Model 玻尔模型的基本特征是中心核和轨道电子，电子与原子核以固定的距离运行 能量跃迁：电子可以通过吸收或释放一个量子（quanta）的能量，从一个轨道跳跃到另一个轨道。这个能量的量正好等于两个轨道之间的能量差 Limitation 局限性 #\r适用范围：波尔模型只能准确地计算具有一个电子的原子，如氢原子（H）、氦离子（He⁺）和双电离锂离子（Li²⁺）的行为 The Quantum-Mechanical Model of the Atom 原子的量子力学模型 #\r在现代量子力学模型中，我们需要使用Four quantum numbers来完全描述一个电子的状态 而在尼尔斯·波尔（Niels Bohr）的早期模型中，只使用了一个量子数 Principal Quantum Number 主量子数 (n) #\rDescribes the size of the electron orbit 描述了电子轨道的大小 可以是any integer value beginning at 1 不同的壳层代表电子与原子核的平均距离不同，能量也不同。 K shell 是当 n=1 时的电子层，这是最接近原子核的电子层，也是能量最低的壳层 L shell 是当 n=2时的电子层 M shell 是当 n=3时的电子层 Angular Momentum Quantum Number 角动量量子数 ℓ #\r角动量量子数（有时也称为方位量子数）描述了电子**轨道的形状 角动量量子数可以具有比主量子数小 0 到 1 的任何值, that is \\(ℓ=0,1,2,\u0026hellip;n−1\\) 当ℓ=0 时，轨道是球形的（s-轨道） s亚层可容纳 2 个电子 当l=1时，亚层是 p 轨道，形状是偶极形（类似哑铃）p亚层可容纳 6 个电子 当l=2时，亚层是 d 轨道，形状更加复杂，通常有四叶或更多叶的形状 d亚层可容纳 10 个电子 当l=3时，亚层是 f 轨道，形状更为复杂 f亚层可容纳 14 个电子 当 ℓ=1 时，轨道呈哑铃形（p-轨道） 更高的ℓ值对应更复杂的轨道形状 The Magnetic Quantum Number 磁量子数 ml #\r磁量子数主要用于指定原子轨道在三维空间中的取向，并与外部磁场中的能级分裂有关 Describes how many different ways each subshell can be orientated 磁量子数 mℓ​ 可以取从−ℓ到+ℓ的整数值，包括零，其中 ℓ是角动量量子数。这表示对于给定的角动量量子数，存在2ℓ+1个可能的磁量子数。 例如，如果电子处于p轨道ℓ=1，那么mℓ可以是 -1, 0, 或 +1，对应轨道在空间中的三个不同取向。 Spin Quantum Number 自旋量子数 (s) #\r自旋量子数ms并不直观地描述电子的物理自旋，因为电子实际上并不是在空间中围绕某个轴物理旋转。 将电子描述为“旋转”的说法可能会引起误解 量子数值：自旋量子数的可能值为\\(\\frac{1}{2}\\)​ 或\\(-\\frac{1}{2}\\)​，通常表示为电子的自旋向上（up-spin）和自旋向下（down-spin） Eletron‘s Spin 电子的自旋 #\r电子的“自旋”这一术语实际上并不指电子在空间中像地球绕其轴旋转那样的物理旋转 电子自旋是一种量子力学性质，表现为一种内在的角动量 尽管这个性质被称为“自旋”，但它并不涉及电子在任何可见的或传统意义上的物理旋转 Summarize #\r大概总结一下，对于一个电子，有四个变量能决定他的属性，也就是四个Quantum Numbers 这四个变量的由来是由Pauil Exclusion Principle规定的 即同一层级，同一轨道类型，同一轨道编号下最多存在两个电子 当四个Quantum Nubers 全部相同的时候，则两个电子一致 1. Principle Quantum Number 主量子数 #\r以电子所作在的电子层级区分 n = 1 or 2 or 3 or 4 or 5 2. Angular Momentum Qunatum 角量子数 #\r通过同一层级下的不同轨道（以形状）区分 n = 2 \u0026amp; l = 1 or 2 3. Magnetic Quantum Number 磁量子数 #\r同一层级下，同一类型（形状）轨道，不同轨道编号区分 n = 2 \u0026amp; l = 2 \u0026amp; m = -1 4. Spin Quantum Number 自旋量子数 #\r同一层级，同一轨道类型，同一轨道编号的不同电子 根据Pauil Exclusion Principle，同一层级，同一轨道类型，同一轨道编号下最多存在两个电子，其中一个上旋为1/2，下旋为-1/2 n = 2 \u0026amp; l = 2 \u0026amp; m = -1 \u0026amp; s = 1/2 Electron Configuration #\r但是想要描述一个元素的Electron Configuration，只需要描述到亚层就够了，每一个亚层都有它能存在的固定电子数 The Electron Configuration of Carbon #\r元素周期表中可以确认，我们的老朋友碳的原子序数为 6， 这意味着一个碳原子的原子核中有六个质子 因此，要构建一个中性碳原子，我们需要将六个电子放入原子核周围的外壳和子壳中 要注意1s中的1是Pricipal Quantum Number，s是Angular Momentum Quantum Number 而且一个亚层是可以包含2个电子的，所以一个n = 2中的 l =1 的亚层p可以有6个电子，写作\\(2p^6\\) 那么对于Carbon来说就有 $$1s^2 , 2s^2 , 2p^2$$ 有的时候为了省略，可以从该元素的前一个Nobel Gas的Electron Configuration开始写起 对于Carbon来说则是Helium $$[\\text{He}] 2s^2 , 2p^2 $$ When 4s is Closer Than 3d Writting Format #\r在上面的Electron Configuration里可以看到4s层的能量实际上是低于3d的，这主要是因为电子层级不是主要只依靠其Primary Quantum Number决定 但是在书写时统一按照了国际惯例，即按照Primary Quantum Number顺序书写 Physical Reason Under #\r尽管在填充电子时，4s能量低于3d（因此4s轨道先填满），但在元素离子化或化学反应中，4s电子往往更容易被移除。 Titanium #\r所以Ti的完整规定写法是 $$1s^2 2s^2 2p^6 3s^2 3p^6 3d^2 4s^2 ~ and ~ [\\text{Ar}] 3d^2 4s^2$$ 可以发现前一个的3d应该是10个，但只写了2个就到s了 Few Exceptions 特例 #\r在3d亚层处于半满或全满状态时，即4，9时 系统可以通过重新分配电子来达到更稳定的状态 Vanadium #\r$$1s^2 2s^2 2p^6 3s^2 3p^6 3d^3 4s^2$$\n可以发现3d亚层只有3个，不处于即将Half-Filled or Completely Filled的水平 Chromium #\r$$1s^2 2s^2 2p^6 3s^2 3p^6 3d^5 4s^1$$\n4s（小于3d的层级）的电子被3d拿去了，以达到了Half-Filled的水平 Copper #\r$$1s^2 2s^2 2p^6 3s^2 3p^6 3d^{10} 4s^1$$\n4s（小于3d的层级）的电子被3d拿去了，以达到了Full-Filled的水平 Octet Stability #\r\\(He=1s^2\\) \\(Ne=1s^2 2s^2 2p^6\\) \\(Ar=1s^2 2s^2 2p^6 3s^2 3p^6\\) \\(Kr=1s^2 2s^2 2p^6 3s^2 3p^6 4s^2 4p^6\\) 可以发现，由于Nobel Gas的电子构型使其具有八电子（octet）结构，遵循八隅规则 所以他们的Electron Configuration通常可以表示为\\(ns^2 np^6\\)的形式 Ionic Bond #\r离子键的形成过程 #\rCl的Atomic Number是17，其电子构型为\\(1s^2 2s^2 2p^6 3s^2 3p^5\\) 它缺一个电子就可以达到Octet的稳定结构 Na的Atomic Number是11，电子构型为\\(1s^2 2s^2 2p^6 3s^1\\) 或简写为 \\([Ne]3s^1\\) 它如果失去一个电子，也可以达到类似稀有气体的稳定构型 电子转移与离子形成 #\r钠会失去一个电子，形成带正电的钠离子（Na⁺）。 氯会接受一个电子，形成带负电的氯离子（Cl⁻）。 这种电子的转移使得钠和氯都达到了稳定的电子构型，形成了Ionic Bond 离子键的特性 #\r这种静电吸引力是non-directional，即在所有相邻的正负离子之间普遍存在，使得离子晶体结构非常稳定 在晶体中，所有电子都被紧密束缚在各自的离子中，不自由移动，因此固态的NaCldo not conduct electricity 图中显示了NaCl晶体的结构，红色小球表示Na⁺，蓝色大球表示Cl⁻。 黄色箭头表示离子间的静电吸引力。 由于正负离子的规则排列，晶体内每个离子都被周围的异性离子包围，形成稳定的晶格结构 Colvaent Bond #\rColvaent Bond涉及Electron Sharing，即原子通过共享价电子来达到稳定的Octet结构 甲烷（CH₄）是一个简单的例子：碳和氢通过共价键结合，形成一个稳定的分子。 共价键只在Specific Atoms形成，例如在CH4中，Carbon仅与四个Hydrogen Bond形成共价键，而不会与其他原子相连。这种键称为Directional Bond 共价键的本质区别在于电子共享，而离子键则是电子转移 Metallic Bonding #\r金属键通常用两种模型描述：sea-of-electrons model, and the band theory of solids Sea of Electrons Model #\r电子海模型中，Valence electrons不固定在特定的原子核上，而是自由移动，形成一个电子的“海洋”（sea of electrons） 要注意Inner electrons (non-valence electrons)是不在Electron Sea中的，因为他们并不参与反应 Ion Core周围的蓝色区域全是电子 Valence electrons 价电子 #\r原子最外层的电子，直接参与化学反应和形成化学键。它们决定了元素的化学性质，例如其反应性、与其他元素形成的键类型等 这些自由移动的电子使得金属具有Conductivity和延展性，因为电子可以在整个晶体中自由流动 Ion core 离子核 #\r在金属或其他离子化合物结构中，不参与化学键的原子核和内层电子的组合 Conductivity of Sea of Electrons Model 导电性\n模型中，electrons are free to move past the ion cores (or so-called delocalized) 离域化 自由移动的电子可以在金属内传导电流 相较于Covalent Bond来说，其被局限在特定原子之间，因此像聚合物这样的材料通常是电的绝缘体 Ductility #\r金属晶体受到足够大的应力时，一个原子平面可以滑过另一个原子平面 在金属中，原子是按照晶体结构排列的，周围有自由移动的电子（电子海） 当对金属施加较大的力（如拉伸力或剪切力）时，金属中的一个原子平面会滑动到另一个原子平面之上 即使发生滑动，由于电子海的存在，这些自由电子能够迅速重新分布并填补原子之间的空隙，从而保持金属的整体结构稳定，不会断裂 Ceramic #\r而对于Ionic \u0026amp; Covalent Bonding来说，由于其结构性，导致一旦发生了滑动，其负电荷会和负电荷处于同一平面导致Repulsion 在陶瓷材料中正负离子交替排列形成晶体结构。 当试图使一个原子层滑过另一个时，同性电荷的离子（例如两个正离子或两个负离子）会短暂靠近。 同性电荷靠近时会产生强烈的静电排斥力。 结果：导致了陶瓷在变形前就会发生脆性断裂。 Polymer #\rDuality #\r聚合物主要通过covalent bonds将分子内部连接，而分子间的连接靠次级键（如范德华力或氢键）。 在塑性变形中，Secondary Bonds被克服，聚合物分子链滑动，而共价键不会断裂。 结果：聚合物表现出较大的可变形性（如韧性），而不会像陶瓷那样容易断裂。 Conductivity #\r聚合物中，all of the valence electrons are tightly bound in the strong covalent bonds due to the lack of any free electrons 聚合物是electrically insulating Form of Crystal of Salt #\rI know that NaCl forms an ordered solid, but why?\n这是因为物质趋向于从things tend to proceed from higher energy to lower energy 当某些事情发生（如盐形成晶体）是因为这样的状态对能量来说是更“有利”的，也就是“lower energy” $$Na_{(s)} + \\frac{1}{2}Cl_{2(g)} \\rightarrow NaCl_{(s)}$$ 对于上面的反应，我们将其拆分成子反应 Sublimation of Sodium 钠的升华 #\r$$Na(s)​→Na(g)​$$\n固态钠\\(Na_{(s)}\\)直接转化为气态钠原子\\(Na_{(g)}\\)，称为升华（sublimation） 这种物质从固体转变为气体的过程需要能量，称为升华焓\\(\\Delta H_{\\text{sublimation}}\\)，对于钠为\\(ΔHsublimation​=109kJ/mol\\) 这里正值代表了：系统需要吸收能量便反应从左向右进行 也就是说，需要能量来熔化然后煮沸钠 Ionization of Sodium Atom 钠原子电离 #\r气态钠原子\\(Na_{(g)}\\)进一步被电离为钠离子\\(Na^+_{(g)}\\)和一个电子\\(e^-\\) $$Na(g)​→Na(g)^+​+e^−$$ Ionization energy is \\(IENa​=497kJ/mol\\) 可以发现这目前还是一个Posititve，则代表还需要吸收能量 Bond Dissociation of Chlorine Molecule 氯的解离 #\r$$\\frac{1}{​2}Cl_2(g)​→Cl(g)​$$\n\\(BDE_{Cl_2} = 121 , \\frac{kJ}{mol}\\) 这仍然是吸热过程（需要能量输入） Formation of a chlorine anion #\r$$Cl(g)​+e^−→Cl(g)^−​$$\n这一过程中的Electron Affinity为\\(EACl​=−364\\frac{mol}{kJ}​\\) 可以发现能量第一次变为了负的，这是第一个释放能量的步骤 Forming the ionic crystal #\r$$Na(g)+​Cl(g)^−​→NaCl(s)$$\n这一个过程包含了Crystallization energy\\(Ecrystallization​=−777\\frac{mol}{kJ}​\\) 可以发现这一步消耗了很多能量 如果我们将这些能量项中的每一个相加，形成 NaCl 的总能量变化为−414 虽然第一步是吸热的，但整个反应通过后续的强烈放热步骤补偿了这一点。整体反应的自由能变化 (ΔG\\Delta GΔG) 是负的，因而是自发的。这解释了为什么钠和氯最终可以自然形成盐晶体\nThe Band Theory 能带效应 #\r电子能级由于相互的排斥作用发生分裂\n在孤立的原子中，电子能量被量子化，存在于离散的能级中（如s,p,d,f轨道） 这些能级之间的能量差是固定的，不会受到其他原子的影响。 对于每个原子原本的一个能级，靠近后会产生多个稍微不同的能级。 例如：对于两个原子，一个能级会分裂成两个能级；对于N个原子，会分裂成N个能级 在固体中，原子之间的距离非常近，并且一个晶格中会有\\(10^{23}\\)个原子 原子的数量极其庞大时，原本分裂的离散能级数量非常多，且间距变得极其微小，最终看起来像是连续的能量区域——这就是能带（Energy Band） 原子越多，能带越“密集” 当对于一个原子，其存在多个能级，但当多个原子组合在一起的时候，电子的能量状态不再是离散的，而是形成了一个几乎连续的能量区域 孤立原子：电子有固定的、离散的能量（如轨道能级 s,p,d） 固体中：原子靠得很近，电子能级由于相互的排斥作用发生分裂。 分裂后的能量状态数量非常多，间隔非常小，看起来像是连续的，这就形成了能带 Bonding in Metals Like Copper #\r用Copper的Electron Configuration举例 $$Cu =1s^2 , 2s^2 , 2p^6 , 3s^2 , 3p^6 , 3d^{10} , 4s^1$$ 在4s中的两个Sublayer中只存在一个电子 Conductivity #\r导电的本质是低能量跃迁的累积：导电依赖于大量电子在价带和导带之间进行低能量的跃迁。如果3s电子要跃迁到4s或4p，势必要消耗更多的能量，而这在常温下不容易实现。因此，这些高能跃迁对导电贡献很小，甚至可以忽略不计。 所以说当一个轨道中存在Empty States的时候，Valence Electron才能在其中跃迁 Bonding in Metals Like Magnesium #\r对于Mg来说\\(Mg=1s^2 , 2s^2 , 2p^6 , 3s^2\\)，从表面看，3s轨道已经完全填满，因此看起来它不应该有可用的电子来参与导电 但是可以发现3p轨道是空的，但它并不是不可用的 这就像在剧院里，空座位虽然没有人坐，但仍然在那里，可以被占用 可以被占用。3s轨道和3p轨道的能级相互重叠，因此电子可以从3s轨道很容易地被激发到3p轨道 Bonding in Ceramics and Polymers #\r回想一下，Ceramic往往通过Ionic Bond结合在一起，而Ionic Bond涉及Electron在Atom间的转移 还要记住，这种电子转移的发生是为了让每个原子都能获得填充的Valence Shell 由于Valence Shell是填充的，因此没有紧邻填充态的电子能态。此外，这些电子与原子核紧密结合，因此没有自由电子 Polymer也是如此，只是他是Covalent Bond Valence Band 价带 #\rValence Band是指电子填满的最高能量带 拿Si举例，其Electron Configuration为\\(1s^22s^22p^63s^23p^2\\) 其Valence Band即为\\(3s^2\\)，3p虽然是最高能量带，但他并没有填充满 导电性：满带的电子不能自由流动，因为能量状态已经填满，没有空位供电子移动 Full Band满带 #\rFull Band是指电子完全填满的能量带。 在硅（Si）的例子中，\\(1s^2 2s^2 2p^6\\) 层被完全填满，这些内层电子构成了满带 这些满带主要是低能级的核心电子带，电子在这些带中被完全填满，无法参与到导电过程中 导电性：满带的电子由于能态完全填满，没有额外的空间或能级供电子跃迁，因此不参与导电。这些带在正常条件下对材料的导电性几乎没有贡献 Conduction Band 导带 #\r导带是指紧邻满带之上的未填满能量带 拿Si举例，其Electron Configuration为\\(1s^22s^22p^63s^23p^2\\) 其Valence Band即为\\(3p^2\\) 当电子被激发到导带后，它们可以在材料中自由移动，从而参与导电 Conduction Band通常是空的，或者仅有少量电子占据（在导体中可能存在部分填充） 导带中的电子是Valence electrons AKA Free Electrons，可以在材料中移动并产生电流 Simiconductor 半导体 #\r一些材料，即半导体，具有的Bond Gap没有绝缘体那么大 这很重要，因为这意味着我们可以控制这些材料中的电子流动。这是太阳能光伏、LED 照明和我们所有现代电子产品的基础 但大约 4 eV 通常是一个不错的数字。如果材料的带隙大于 4 eV，我们可以将其视为绝缘体，如果带隙小于 4 eV（但不为零） Conductors, Insulators, and Semiconductors #\r重新根据导电效率定义这三种的区别 从左到右依次为 conductors semiconductors and insulators Back To Light #\r可见光由光子能量在 2-3 eV 之间的光子组成 如果材料的带隙大于 3 eV（例如 SiO₂，二氧化硅），那么可见光光子的能量不足以激发电子从价带跃迁到导带。 结果：光子通过材料时不会被吸收，因此材料对可见光是透明的。 举例：玻璃主要由 SiO₂ 构成，因此玻璃是透明的。 Energy efficiency of the building #\r光穿过窗户进入室内会导致热量积聚，从而增加空调的能耗 解决方案：在窗户上镀金属薄层 金属薄层可以反射部分阳光（尤其是紫外线光子）。 如果金属层够薄，它仍然允许大部分可见光通过，同时减少紫外线和热量的传递。 优点：提高建筑的能源效率，降低室内过热问题。 Light \u0026amp; Metal #\r金属的特点：没有明显的带隙（导带和价带重叠） 结果1：光子容易激发电子：\n可见光光子的能量足够将金属中的自由电子激发到更高能级\n因此，金属吸收光，并且是不透明的\n结果2：金属的光泽（反射性）：\n被光子激发的电子会迅速返回到较低能量状态，在这个过程中重新发射光子\n这种现象导致金属表面反射光，从而看起来有光泽（“金属光泽”）\nSilicon 硅 #\r实验表明，每个硅原子会形成 four identical bonds 但是，根据电子配置，3s 和 3p 轨道的能量不同，这意味着它们的性质不同 可以推出结论“Our model is limited. It can\u0026rsquo;t explain bonding in silicon” sp3 Hybridization 轨道杂化理论 #\r这里提出的解决方法是这样的：让我们只拿一个s轨道和3p轨道并将它们混合在一起 Diamond Cubic #\r现在我们有四个等效的轨道，这些轨道的分布是对称的，彼此之间具有等价性 it wouldn\u0026rsquo;t make sense for, say, three of them to be clumped close to one another and one bond to be all alone 也就是说，它们在空间中的位置分布是均匀的 Tetrahedral Configuration #\rSemiconductors #\r能够控制半导体的导电性对我们来说很重要 可以通过将杂质引入Semiconductors中以改变其导电性 Intrinsic Semiconductors 本征半导体 #\r拿Silicon举例，其在3p轨道上，存在了4个Valence Electron 不同的Silicon则和其他的通过Covalent Bond组合成如下 Intrinsic Semiconductors一种纯净的半导体，没有杂质掺杂 在Absolute Zero的时候可以形成如上图的结构 Hole 空穴 #\r当温度上升了之后，Electrons会被Promoted进入Conduction Band导带 电子从Valence Band跃迁到Conduction Band后，会在原来的位置留下一个hole，即一个缺少电子的位置 其中，Electron和Hole空穴（电子缺失造成的正电荷）的数量是平衡的 没多一个Electron被promote后，都会留下对应的Hole，亦可说他们是成对出现的 种类型的半导体称为Intrinsic Semiconductors，因为所有可用于导电的东西都来自半导体本身，而不是我们添加到其中的任何东西 Intrinsic Semiconductors在实践中并不是特别有用，因为我们几乎总是添加杂质来控制Conductivity 电导率 Conductivity 电导率 #\r计算一个Simiconductor的 Conductivity的公式为 $$\\sigma = nq\\mu_n + pq\\mu_p $$ 而对于Intrinsic Semiconductors的特殊情况来说，由于存在Concentration of holes = Concentration of electons，于是就有 $$\\sigma = nq(\\mu_n + \\mu_p)$$ Concentration of electrons 电子浓度 n #\rSince electrons carry a negative charge 所以可以通过\\(\\frac{noofelectorns}{m^3}\\) Concentration of holes 空穴浓度 #\r在本征半导体中，电子浓度（n）和空穴浓度（p）是相同的 这是因为Intrinsic Semiconductors中的电子和空穴都是由相同数量的价带电子激发到导带产生的 因此，在热平衡状态下，电子的生成和复合是平衡的，所以电子浓度和空穴浓度相等 Mobility 迁移率 #\r对于Mobility来说存在Electron Mobility和Hole Mobility，分别通过\\(\\mu_n和\\mu_p\\)来表示 其单位为\\(\\frac{m^2}{V_s}\\) Charge 电荷 #\rCharge指的是Magnitude of the fundamental charge \\(1.602\\times 10^{-19}C\\) Extrinsic Semiconductors 外延半导体 #\r本征半导体并不是特别实用，因为我们通常会向半导体中添加Impurities（称为dopants）以仔细控制其Conductivity 我们向半导体中添加Small amount of dopant的Dopants时，掺杂剂引入的电导率会压倒任何本征半导体，因此我们称之为Extrinsic Semiconductors 基本上存在两种加入Dopants的方式，使用额外的Electron或者Hole的方法 由于电子是Negataive Charged的，将添加Electron的叫做n- type Semiconductor 而Hole是Positive的（虽然其是Neutral的，但由于Hole存在于Electron的中间，所以看上去是“Positive”的），所以添加Hole的叫做p-type Semiconductor Extrinsic n-Type Semiconductors 外本征n型半导体 #\r想要在Intrinsic Semiconductor中添加Extra Electrons，可以通过Zero Dimension Inpurity的Point Defects来添加Inpurities Atoms，而添加的这一个Atom会带来额外的电子 已知对于Silicon来说其存在4个Valence Electrons：\\(3s^23p^2\\)，一种合适的做法便是将位于元素周期表右侧的Atom加入，即一个存在5个Valence Electron的Atom，这便是P，磷 已知磷的Atomic Number为15，其Electron Configuration为\\(1s^2 2s^2 2p^6 3s^2 3p^3\\) 因此添加到硅晶格中时，当与相邻的硅原子形成四个共价键时，将有一个额外的电子踢来踢去 如上图所示，元素中出现了一个多余的电子，由于没有足够的周围硅原子来形成稳定的共价键，因此这个电子不像其他共价键中的电子那样稳定地束缚 Donor Level供体能级 #\r在Gap Band内，接近导带底部的蓝色线表示磷原子提供的额外电子的能级 这个能级非常接近导带，因此只需很少的能量就可以将电子激发到导带中。 由于Impurity P为Simiconductor贡献的 Charge carriers，亦或者说是价带比通过本征促进产生的电荷载流子多得多 因此我们可以忽略\\(\\mu_p\\)，只用电子浓度和迁移率来计算 n 型半导体的电导率 $$σ_{n−type​}=nqμ_n​$$ Extrinsic p-Type Semiconductors #\r同理对于Silicon来说，选他左边的元素，B 在Extrinsic p-Type Semiconductors中，电子不需要被激发到Conduction Band才能导电 相反，价带中的电子可以轻易被激发到B提供的一个Hole上，从而填补那里的空穴，也就是Bnad Diagram上的Acceptor Level 同理可以忽略\\(\\mu_n\\)的大小 $$σ_{p−type}​=pqμ_p​$$ Solid Ionic Conductivity #\r当我们思考和谈论固体材料中的导电性时，我们会考虑电子的运动，就半导体而言，还会考虑“空穴”的运动 然而，导电性也可以通过Solid Ionic内的运动来实现 ","date":"Nov 18 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms7.lightquantum/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 18/11/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eLight \r\n    \u003cdiv id=\"light\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#light\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e当我们讨论为什么像聚甲基丙烯酸甲酯（PMMA）这样的材料能够透明，而像玻璃态金属或单晶金属（如硅和镍基合金）则是不透明的时候，理解光的本质及其与材料的交互作用是至关重要的。\u003c/li\u003e\n\u003cli\u003e材料是否透明，很大程度上取决于其电子结构，这决定了它如何吸收光\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003ePMMA \r\n    \u003cdiv id=\"pmma\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#pmma\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003ePMMA we were able to have a transparent polymer because PMMA is 100 % amorphous\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS7.Light\u0026amp;Quantum/ECMS7.LIGHT.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 7. Light and Quantum","type":"docs"},{"content":"\rYour browser does not support the video tag.\rfrom manim import *\rimport numpy as np\rimport torch\rimport random\rclass LinearRegression(Scene):\rdef construct(self):\rdef data_generator(w, b, num):\rX = torch.normal(0, 1, (num, len(w)))\ry = torch.matmul(X, w) + b\ry += torch.normal(0, 0.01, y.shape)\rreturn X, y.reshape((-1, 1))\rtrue_w = torch.tensor([2, -3.4])\rtrue_b = 4.2\rfeatures, labels = data_generator(true_w, true_b, 1000)\rhead = Text(\u0026#34;Linear Regression Display - Buezwqwg\u0026#34;)\rhead.set_color(BLUE)\rself.play(Create(head))\rself.wait(1)\rself.play(Uncreate(head))\rfeature_one = features[:, [0]].tolist()\rfeature_two = features[:, [1]].tolist()\rlabels_list = labels.tolist()\raxes_1 = Axes(\rx_range=[min(feature_one)[0]-1, max(feature_one)[0]+1, 1],\ry_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\rx_length=5,\ry_length=5,\raxis_config={\u0026#34;color\u0026#34;: BLUE},\r)\raxes_2 = Axes(\rx_range=[min(feature_two)[0]-1, max(feature_two)[0]+1, 1],\ry_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\rx_length=5,\ry_length=5,\raxis_config={\u0026#34;color\u0026#34;: BLUE},\r)\raxes = VGroup(axes_1, axes_2)\raxes.arrange(RIGHT, buff=1)\rself.play(Create(axes))\rpoints_1 = []\rfor i in range(len(labels_list)):\rdot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\rpoints_1.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\rpoints_2 = []\rfor i in range(len(labels_list)):\rdot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\rpoints_2.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\rpoints_group_1 = VGroup(*points_1)\rpoints_group_2 = VGroup(*points_2)\rself.play(Create(points_group_1), Create(points_group_2))\r# Draw the true model lines\rtrue_line_1 = axes_1.plot(lambda x: true_w[0].item() * x + true_b, color=GREEN)\rtrue_line_2 = axes_2.plot(lambda x: true_w[1].item() * x + true_b, color=GREEN)\rself.play(Create(true_line_1), Create(true_line_2))\r# Initialize parameters\rw = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\rb = torch.zeros(1, requires_grad=True)\rw_1 = w.detach().numpy()[0][0]\rw_2 = w.detach().numpy()[1][0]\rb_0 = b.detach().numpy()[0]\rline_1 = axes_1.plot(lambda x: w_1 * x + b_0, color=BLUE)\rline_2 = axes_2.plot(lambda x: w_2 * x + b_0, color=BLUE)\rself.play(Create(line_1), Create(line_2))\r# Add text to display loss, w, b\rloss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {0:.4f}\u0026#34;)\rloss_text.to_edge(UP)\rw_text = MathTex(f\u0026#34;w_1 = {w_1:.4f},\\\\ w_2 = {w_2:.4f}\u0026#34;)\rw_text.next_to(loss_text, DOWN)\rb_text = MathTex(f\u0026#34;b = {b_0:.4f}\u0026#34;)\rb_text.next_to(w_text, DOWN)\rparam_text = VGroup(loss_text, w_text, b_text)\rself.play(Write(param_text))\rdef data_iter(batch_size, features, labels):\rnum = len(features)\rindex = list(range(num))\rrandom.shuffle(index)\rfor i in range(0, num, batch_size):\rbatch_index = torch.tensor(index[i:min(i+batch_size, num)])\ryield features[batch_index], labels[batch_index]\rbatch_size = 10\rdef linreg(X, w, b):\rreturn torch.matmul(X, w) + b\rdef squared_loss(y_hat, y):\rreturn (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\rdef sgd(params, lr, batch_size):\rwith torch.no_grad():\rfor param in params:\rparam -= lr * param.grad / batch_size\rparam.grad.zero_()\rlr = 0.03\rnum_epochs = 3 # Changed to 3 epochs\rnet = linreg\rloss = squared_loss\rupdate_interval = 1 # Update after every batch\rbatch_counter = 0\r# Start training and update after every backpropagation\rfor epoch in range(num_epochs):\rfor X, y in data_iter(batch_size, features, labels):\rl = loss(net(X, w, b), y)\rl.sum().backward()\rsgd([w, b], lr, batch_size)\rbatch_counter += 1\r# Get the latest parameters\rnew_w_1 = w.detach().numpy()[0][0]\rnew_w_2 = w.detach().numpy()[1][0]\rnew_b_0 = b.detach().numpy()[0]\r# Update lines\rnew_line_1 = axes_1.plot(lambda x: new_w_1 * x + new_b_0, color=BLUE)\rnew_line_2 = axes_2.plot(lambda x: new_w_2 * x + new_b_0, color=BLUE)\rself.play(\rTransform(line_1, new_line_1),\rTransform(line_2, new_line_2),\rrun_time=0.1 # Adjusted animation time for smoother update\r)\r# Update loss and parameter displays\rwith torch.no_grad():\rtrain_l = loss(net(features, w, b), labels)\rcurrent_loss = float(train_l.mean())\rnew_loss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {current_loss:.4f}\u0026#34;)\rnew_loss_text.to_edge(UP)\rnew_w_text = MathTex(f\u0026#34;w_1 = {new_w_1:.4f},\\\\ w_2 = {new_w_2:.4f}\u0026#34;)\rnew_w_text.next_to(new_loss_text, DOWN)\rnew_b_text = MathTex(f\u0026#34;b = {new_b_0:.4f}\u0026#34;)\rnew_b_text.next_to(w_text, DOWN)\rself.play(\rTransform(loss_text, new_loss_text),\rTransform(w_text, new_w_text),\rTransform(b_text, new_b_text),\rrun_time=0.1\r)\r# Output current epoch\u0026#39;s loss\rprint(f\u0026#39;epoch {epoch + 1}, loss {current_loss:f}\u0026#39;)\rself.wait(2) ","date":"Nov 18 2024","externalUrl":null,"permalink":"/docs/displays/linearregression_display/","section":"Docs","summary":"\u003cvideo width=\"640\" height=\"360\" controls\u003e\r\n  \u003csource src=\"LG_Display.mp4\" type=\"video/mp4\"\u003e\r\n  Your browser does not support the video tag.\r\n\u003c/video\u003e\r\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003efrom manim import *\r\nimport numpy as np\r\nimport torch\r\nimport random\r\n\r\nclass LinearRegression(Scene):\r\n    def construct(self):\r\n        def data_generator(w, b, num):\r\n            X = torch.normal(0, 1, (num, len(w)))\r\n            y = torch.matmul(X, w) + b\r\n            y += torch.normal(0, 0.01, y.shape)\r\n            return X, y.reshape((-1, 1))\r\n\r\n        true_w = torch.tensor([2, -3.4])\r\n        true_b = 4.2\r\n        features, labels = data_generator(true_w, true_b, 1000)\r\n\r\n        head = Text(\u0026#34;Linear Regression Display - Buezwqwg\u0026#34;)\r\n        head.set_color(BLUE)\r\n        self.play(Create(head))\r\n        self.wait(1)\r\n        self.play(Uncreate(head))\r\n\r\n        feature_one = features[:, [0]].tolist()\r\n        feature_two = features[:, [1]].tolist()\r\n        labels_list = labels.tolist()\r\n        axes_1 = Axes(\r\n            x_range=[min(feature_one)[0]-1, max(feature_one)[0]+1, 1],\r\n            y_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\r\n            x_length=5,\r\n            y_length=5,\r\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\r\n        )\r\n        axes_2 = Axes(\r\n            x_range=[min(feature_two)[0]-1, max(feature_two)[0]+1, 1],\r\n            y_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\r\n            x_length=5,\r\n            y_length=5,\r\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\r\n        )\r\n        axes = VGroup(axes_1, axes_2)\r\n        axes.arrange(RIGHT, buff=1)\r\n        self.play(Create(axes))\r\n\r\n        points_1 = []\r\n        for i in range(len(labels_list)):\r\n            dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\r\n            points_1.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\r\n        points_2 = []\r\n        for i in range(len(labels_list)):\r\n            dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\r\n            points_2.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\r\n        points_group_1 = VGroup(*points_1)\r\n        points_group_2 = VGroup(*points_2)\r\n        self.play(Create(points_group_1), Create(points_group_2))\r\n\r\n        # Draw the true model lines\r\n        true_line_1 = axes_1.plot(lambda x: true_w[0].item() * x + true_b, color=GREEN)\r\n        true_line_2 = axes_2.plot(lambda x: true_w[1].item() * x + true_b, color=GREEN)\r\n        self.play(Create(true_line_1), Create(true_line_2))\r\n\r\n        # Initialize parameters\r\n        w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\r\n        b = torch.zeros(1, requires_grad=True)\r\n        w_1 = w.detach().numpy()[0][0]\r\n        w_2 = w.detach().numpy()[1][0]\r\n        b_0 = b.detach().numpy()[0]\r\n        line_1 = axes_1.plot(lambda x: w_1 * x + b_0, color=BLUE)\r\n        line_2 = axes_2.plot(lambda x: w_2 * x + b_0, color=BLUE)\r\n        self.play(Create(line_1), Create(line_2))\r\n\r\n        # Add text to display loss, w, b\r\n        loss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {0:.4f}\u0026#34;)\r\n        loss_text.to_edge(UP)\r\n        w_text = MathTex(f\u0026#34;w_1 = {w_1:.4f},\\\\ w_2 = {w_2:.4f}\u0026#34;)\r\n        w_text.next_to(loss_text, DOWN)\r\n        b_text = MathTex(f\u0026#34;b = {b_0:.4f}\u0026#34;)\r\n        b_text.next_to(w_text, DOWN)\r\n        param_text = VGroup(loss_text, w_text, b_text)\r\n        self.play(Write(param_text))\r\n\r\n        def data_iter(batch_size, features, labels):\r\n            num = len(features)\r\n            index = list(range(num))\r\n            random.shuffle(index)\r\n            for i in range(0, num, batch_size):\r\n                batch_index = torch.tensor(index[i:min(i+batch_size, num)])\r\n                yield features[batch_index], labels[batch_index]\r\n\r\n        batch_size = 10\r\n\r\n        def linreg(X, w, b):\r\n            return torch.matmul(X, w) + b\r\n\r\n        def squared_loss(y_hat, y):\r\n            return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\r\n\r\n        def sgd(params, lr, batch_size):\r\n            with torch.no_grad():\r\n                for param in params:\r\n                    param -= lr * param.grad / batch_size\r\n                    param.grad.zero_()\r\n\r\n        lr = 0.03\r\n        num_epochs = 3  # Changed to 3 epochs\r\n        net = linreg\r\n        loss = squared_loss\r\n\r\n        update_interval = 1  # Update after every batch\r\n        batch_counter = 0\r\n\r\n        # Start training and update after every backpropagation\r\n        for epoch in range(num_epochs):\r\n            for X, y in data_iter(batch_size, features, labels):\r\n                l = loss(net(X, w, b), y)\r\n                l.sum().backward()\r\n                sgd([w, b], lr, batch_size)\r\n\r\n                batch_counter += 1\r\n                # Get the latest parameters\r\n                new_w_1 = w.detach().numpy()[0][0]\r\n                new_w_2 = w.detach().numpy()[1][0]\r\n                new_b_0 = b.detach().numpy()[0]\r\n\r\n                # Update lines\r\n                new_line_1 = axes_1.plot(lambda x: new_w_1 * x + new_b_0, color=BLUE)\r\n                new_line_2 = axes_2.plot(lambda x: new_w_2 * x + new_b_0, color=BLUE)\r\n                self.play(\r\n                    Transform(line_1, new_line_1),\r\n                    Transform(line_2, new_line_2),\r\n                    run_time=0.1  # Adjusted animation time for smoother update\r\n                )\r\n\r\n                # Update loss and parameter displays\r\n                with torch.no_grad():\r\n                    train_l = loss(net(features, w, b), labels)\r\n                    current_loss = float(train_l.mean())\r\n                new_loss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {current_loss:.4f}\u0026#34;)\r\n                new_loss_text.to_edge(UP)\r\n                new_w_text = MathTex(f\u0026#34;w_1 = {new_w_1:.4f},\\\\ w_2 = {new_w_2:.4f}\u0026#34;)\r\n                new_w_text.next_to(new_loss_text, DOWN)\r\n                new_b_text = MathTex(f\u0026#34;b = {new_b_0:.4f}\u0026#34;)\r\n                new_b_text.next_to(w_text, DOWN)\r\n                self.play(\r\n                    Transform(loss_text, new_loss_text),\r\n                    Transform(w_text, new_w_text),\r\n                    Transform(b_text, new_b_text),\r\n                    run_time=0.1\r\n                )\r\n\r\n            # Output current epoch\u0026#39;s loss\r\n            print(f\u0026#39;epoch {epoch + 1}, loss {current_loss:f}\u0026#39;)\r\n\r\n        self.wait(2)\n\u003c/code\u003e\u003c/pre\u003e","title":"Linear Regression Display","type":"docs"},{"content":"","date":"Nov 13 2024","externalUrl":null,"permalink":"/tags/econ/","section":"Tags","summary":"","title":"Econ","type":"tags"},{"content":"Notes taken from 小Lin说的个人空间-小Lin说个人主页-哔哩哔哩视频. space.bilibili.com/520819684?spm_id_from=333.337.search-card.all.click. Requires further completion\n","date":"Nov 13 2024","externalUrl":null,"permalink":"/docs/economic/","section":"Docs","summary":"\u003cp\u003eNotes taken from 小Lin说的个人空间-小Lin说个人主页-哔哩哔哩视频. space.bilibili.com/520819684?spm_id_from=333.337.search-card.all.click.\nRequires further completion\u003c/p\u003e","title":"Economics","type":"docs"},{"content":" Last Edit: 11/13/2024\n交易量最大的市场\n货币 #\r行业标准，每种货币用三个字母来代替 一维理解 #\r假设什么都没有，借1.6mJPY去换10kUSD 这时候可以获得USD的利息，但同时需要支付JPY的利息 但如果USD/JPY涨了，而JPY没变，则赚了对应的JPY Forex的特殊性 #\r其覆盖面极广，同时涉及专业公司与市场中的每一个人 Decentralized 去中心化 #\r对于Stock Market可能存在Nasdaq这类的交易所 但是Foreign exchange没有一个中央的交易机构 Market Maker 做市商 #\r提供交易流动性的场所 由于Foreign Exchange是去中心化的，但为了保持资金的流通一般散户甚至公司都会去找到到Marker Marker来做交易 导致了Market Maker的交易量极大的前提下，参与人数极少 Hedge 对冲 #\r而这些Market Maker并不靠与客户的对赌赚钱 而是通过强大的风险管控能力去做对冲 Censorship 监管 #\r由于其Decentralized的特点，其Censorship一般都比较松 这也导致了在交易中动手脚的可能性上升 Fixing 定盘价 #\r在每天London 4PM的时候前后一分钟交易量算出来的均价 类似于Stock的收盘价 全球大量的衍生产品都将高度依赖于这一个价格 只要交易员每天在指定时间只做指定的一个货币，改货币价格直接就上升了 High Frequency Trading #\r依靠算法套利的公司 一笔赚的非常少但是量大 Foreign Payment #\r对于企业，Foreign Exchange的维度又引入了时间的概念 假设造手机，在成本投入的半年后才能开始实现收益，但未来的汇率是不稳定的 Foreign Exchange Forward Comtract 外汇远期合约 #\r在现在对冲掉未来的风险 对于实际情况可能复杂的多，对于供应链上的供应商来说存在更多的情况 债 #\r在当地企业景气的时候，可能存在外资的涌入 但是外资的投资将采用其货币，而为了规避汇率风险则需要引入Cross Currency Swap Cross Currency Swap 货币利率互换协议 #\r可以实现虽然借的是USD债 但通过互换协议相当于规避了Currency的风险 Swap #\r互换的主要交易时基于未来的 其主要是因为对于未来Currency的不确定性导致的 Forex Products by Trading Volume #\rCurrency #\r对于一个国家，货币就衡量了当下所有商品的标尺 但犹豫利率的存在，导致一个3%年利率的国家一年后的103等价于当下的100 （纯理论） 而将两个维度结合便可以形成一个Plane 而Foreign Exchange 则是不同坐标系的转换 即Basis Change Central Bank \u0026amp; Government 央行和政府 #\r央行可以通过一系列操作影响整个市场，但是其并不能起到做庄的效果 互换协议，双边政府互相给钱，促进双方货币在国际贸易上的占比 也是去美元化的一种方法 ","date":"Nov 13 2024","externalUrl":null,"permalink":"/docs/economic/foreignexchange/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/13/2024\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e交易量最大的市场\u003c/p\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e货币 \r\n    \u003cdiv id=\"%E8%B4%A7%E5%B8%81\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E8%B4%A7%E5%B8%81\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/Economic_Static/ForeignExchange/ForeignExchange%E5%A4%96%E6%B1%87.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Foreign Exchange","type":"docs"},{"content":" Last Edit: 11/10/24\nPolymer 聚合物 #\r前缀“poly-”意指“很多”，暗示了这些聚合物分子结构中有许多重复的部分。 而“mer”指的是“重复单元”或“基元”，这是一种分子的基本单元 Basic Structure #\r与Crystal Structure不同，他们的基本结构是Cubics。并且其会在三维空间（即沿着三个方向）重复排列，形成一个规则的晶体结构 Polymer的结构则不同，它的基元通常只在One Dimnesion上重复，这种重复形成了一条长链 可以看到Polymer的内部结构为长线，而线其实是由很小的分子所组成的 Polyethylene and Polymer #\rPolymer #\r聚合物是一个广义术语，指的是由许多重复单元（基元）组成的长链分子。 聚合物的基元可以是各种各样的分子结构，不限于某一种类型。 Polyethylene #\r聚乙烯是一种具体的聚合物，由重复的乙烯单元（C₂H₄）构成。乙烯单元经过聚合反应形成长链，从而产生聚乙烯。 它是最常见的合成聚合物之一，但只是聚合物的一种类型。 放大上图可以发现 Polyethylene由String结构组成，而String则由分子组成 分子内部，如上图的Polyethylene则是由Hydrogen和Carbon Atoms组成，而Atom之间的作用力则为Intramolecular Bonds（作用在分子内部的相互作用力） Polymer\u0026rsquo;s Deformation #\r在观察超市购买了重物，并将它们放入一个塑料袋中后，可能会发现袋子的把手开始拉伸，甚至感到不适，因为它开始压入手中。随着重量的增加，把手继续拉伸，但在某个点，它似乎停止了延展。有时你甚至能在被拉伸的地方看到颜色的轻微变化 这一现象也可以通过String Model解释 Initial Stage #\r拉伸初期，由于Polymer内部结构仍是Randomly Oriented的Polymer chains没有按照拉伸的方向对齐 于是就可以像拉开松散的线一般将他们分开 Bonds #\rPrimary Bond #\rPrimary bond是 intramolecular bond，存在于一个分子内部的原子之间，例如Covelent Bond或Ionic Bond 这种键非常强，是构成Polymer Chians的基本骨架。 Secondary Bond #\rSecondary bond 是 intermolecular bond，即分子之间的作用力，比如范德华力或氢键 这种键相对较弱，存在于不同分子链之间，允许它们在一定条件下滑动或重新排列 Plastic Deformation Stage #\r随着拉伸的继续，Polymer chains开始发生滑动 滑动可以类比为面条互相滑动的情形 这就是分子间的“Friction”——在Polymer中我们称之为“Intermolecular Force”，也叫“secondary bond” （次级键） Alignment Stage #\r在Polymer Chain到位了之后，Secondary Bond（Inter Molecular Bond）的作用逐渐减小，变为了Primary Bond（Intra Molecular Bond） 所以Polymer的Plastic Deformation所需要的Stress反而会上升 Yield Strength #\r由于对于Polymer来说在发生了Plastic Deformation后其Polymer Chain将会变为Secondary Bond受力，其Stress反而上升了，所以将Curve在Plastic Deformation时候的峰值作为Yield Strength是合理的 The change of mer Units #\r已知Polymer由Polymer Chain组成，而Polymer Chain则是由很多Molecular (Mer)组成的 所以改变Molecular的元素便直接改变了Polymer Polypropylene 聚丙烯 #\r聚丙烯 (PP) 是一种非常常见且有用的聚合物。星巴克® 的那些漂亮的可重复使用杯子就是用 PP 制成的 在塑料瓶的底下可以看到其Recycling Code 为5 通常来说Polypropylene会比Polythylene更加坚固，其来自于额外的\\(CH_3\\) Polyvinylchloride (PVC) #\r出于某些原因，PVC的名字里就出现了Cl，所以其分子当然也包含Cl 而对于PVC中的V来说，其代表了Vinyl，是这种结构 Periodic Table 元素周期表 #\r对于周期表来说，其具有以下特性 周期（横向）趋势：当从左到右观察周期表时，原子半径逐渐减小，电子更接近原子核，因此核对外层电子的吸引力增强，Electronegativity增大 族（纵向）趋势：从周期表的顶端向底部移动时，原子半径增大，外层电子距离原子核更远，核对电子的吸引力减弱，因此Electronegativity减小 Electronegativity 电负性 #\r反应了Atom吸引电子的强弱程度 Bonding 分子键 #\rCovalent Bond 共价键 #\r十分熟悉的一种Bonding Type 一个Molecule中的两个Atom共享Electron Nonpolar Covalent Bond 非极性共价键 #\r当两个原子的Electronegativity几乎相等的时候，形成Nonpolar Covalent Bond 其特性为Electorn将均匀的分布于Atom之间 Polar Covalent Bond 极性共价键 #\r两个Atom的Electronegativity具有明显差异的时候 其仍然是Covalent Bond，但是Sharing的Atom会明显的偏向于其中一个Electronegative更大的Atom 可以发现Cl带有了部分的负电荷（Electron），所以其是具有更高Electronegativity的那个 并且Electrons在Covalent Bond中将会偏向于Cl Ionic Bond 离子键 #\r两个原子间的Electronegativity差极大时，一个Atom将会抢走另外一个的Electron形成Ionic Bond Polytetrafluoroethylene 聚四氟乙烯 #\r一种非反应性的Polymer 每一个Carbon Atom上都结合了大量的Floride，它们很大可以防止PTFE内部不被波坏 虽然Floride的electronegativity很高，但由于其Molecule中的对称性结构，所以形成了一个NonPolar Covalent Bond Hydrophobic 疏水性 #\rPTFE的特殊点在于其为一种Hydrophobic Polymer，具体来说Liquid无法通过其缝隙进入材料，而Gas却可以自由的通过 这就是户外服装在雨中保持干爽的愿意，一般称其为“Breathing\u0026quot; Polymethylmethacrylate 聚甲基丙烯酸甲酯 PMMA #\r一种透明的Polymer 每个合并单元上的大侧基团（通常称为 \u0026ldquo;笨重 \u0026ldquo;侧基）会阻止分子相互靠近组织。 这就确保了聚合物是完全无组织的，或者说是无定形的。 当聚合物结晶时，其折射率与无定形时不同。 如果聚合物中既有无定形的部分，也有结晶的部分（即所谓的半结晶），那么穿过聚合物的光线就不会沿着直接的路径传播，聚合物就会呈现半透明或不透明的状态。 因此，PMMA 之所以是透明的，部分原因是合并单元确保其保持 100% 透明 Length of Polymer Chain #\r前面提到过Polymer Chain是很长的，但却没有给出一个量化的办法 在合成Polymer的时候，控制其分子长度是很困难的 但可以得到一个其长度的分布图 假定一个理想的Polymer Sample，其Polymer Chain的存在需要通过一种分布来描述 描述这个分布的方式并不是通过长度而是重量 具体来说，假设有如下盒子，其里面包含了绳子 木盒子，里面有一段绳子。 你不能打开盒子，你需要确定盒子里绳子的长度。 给你一个空盒子的质量、一根一米长的绳子和一个天平。 你可以用质量来确定盒子里绳子的长度 发现可以通过绳子单位长度的质量计算盒子里绳子的长度 Number Average Molecular Weight 数均分子量 #\r$$\\overline{M}{\\text{number}} = \\frac{\\sum{n=1}^{i} M_n x_n}{\\sum x_n}$$\n所有分子的分子量加总后除以分子总数得到的平均分子量 \\(M_{number}\\)​：数均分子量 \\(M_n​\\)：第n类分子的分子量 \\(x_n\\)：第n类分子的数量比例（即该类别分子数占总分子数的比例） Analogy Candy Box #\rWeight Average Molecular Weight 重均分子量 #\r$$\\overline{M}{\\text{weight}} = \\sum{n=1}^{i} M_n w_n $$\n\\(M_{weight}\\)​：重均分子量 \\(M_n​\\)：第n类分子的分子量 \\(x_n\\)：第n类分子的质量分数（即该类别分子总质量占总所有分子总质量的比例） Analogy #\r用同样的模型说明 Dispersity 分散性 #\r$$\\mathcal{D} = \\frac{\\overline{M}{\\text{weight}}}{\\overline{M}{\\text{number}}} $$\n\\(\\mathcal{D}\\): 分散性 Dispersity \\(M_{weight}\\)​：重均分子量 \\(M_{number}\\)​：数均分子量 当\\(\\mathcal{D} \u0026gt;1\\)时：分子量分布较宽，即多分散（Polydisperse）。随着\\(\\mathcal{D}\\)值增大，分子量的差异越大，分布越宽 Why Molecular Weight Anyway? #\r当面条较长时，很难将一根面条从其他面条中分离出来。 聚合物也是如此，当然，这也是了解分子量如此重要的原因。 随着聚合物分子量的增加，强度也会增加，而且由于长分子的缠结增加，断裂应变通常也会增加。 Ways of molecules stack up #\r聚合物分子虽然通常是线性的，但并非直线。 也就是说，它们是曲线形的 但这并不意味着它们总是完全无序的。 我们还看到，当聚合物发生塑性变形( chain Orientation) 时，会产生一些有序性 Crystalline Polymer 半结晶聚合物 #\r在某些情况下，它们可以在没有任何外部负载的情况下自行有序化 聚乙烯等简单聚合物中的分子通常会在自身上来回折叠 但是由于分子非常长，聚合物永远不可能 100% 结晶 并且由于Crystal Region比Amorphous通过Secondary Bond更牢固地结合在一起，因此这些区域的强度更高 Change of Crystal Density of Polymer 改变聚合物的结晶度 #\r对于Polymer Chain来说，几乎总是有一些所谓的分支从主分子上延伸出来，其仍然是分子的一部分 事实上，我们经常会专门设计一种聚合物，使其具有分支。 低密度聚乙烯LDPE就是这种情况 低密度聚乙烯LDPE中更多和更长的分支降低了分子相互靠近排列的能力，降低了结晶度，从而降低了密度、强度甚至弹性模量 Change of the Intramolecular Bonds #\r想要通过mer unit 改变Polymer整体强度，则可以use elements that are very electronegative We could also ensure that they are bonded to something that is very easy to make positive 于是就可以想到Hydrogen Hydrogen #\r对于Hydrogen来说其有很低的Electronegativity，其Electron很容易被抢走，剩下其Proton 只需要一个有点电负性的元素，氢就会变成正元素 Hydrongen Bond #\r犹豫Hydrogen具有的特小的Electronegativity，导致了其极易产生一个High Strengh Polar Covalent Bond（强偶极子键），所以一种特殊的Bond则尤其命名：Hydrogen Bond Introducing Hydrogen Bond between Molecules #\r将分子之间引入氢键是一种增强分子间相互作用的方式 Cross Link 交联 #\r交联则是通过强的主键（即分子内的共价键）将聚合物分子永久地连接起来，从而显著增强聚合物的强度和弹性。交联聚合物的一个经典例子是天然乳胶橡胶。 在制作弹性体时，交联程度需要适中。如果交联过多，聚合物会变得硬且脆，失去弹性，不再适合作为弹性体 Temperature #\r聚合物的许多特性都是由分子间的弱键造成的 这些键（有时也称为相互作用）更容易被热能破坏，即使温度相对较低：接近室温 在金属或陶瓷中，大部分特性都是由将它们连接在一起的强键、主键的性质决定的（稍后将详细介绍），这些键的键能远远高于接近室温时的热能。 Melting Temperature 熔点温度（Tm​） #\r熔点温度指的是聚合物从固态转变为液态的温度 超过这个温度后，聚合物会像厚液体一样流动 Glass Transition Temperature 玻璃化转变温度 (Tg) #\r通常适用于非晶态或半晶态聚合物 表示的是聚合物从硬脆的“玻璃态”转变为柔软、易变形的“橡胶态”的温度 低于Tg的温度下，聚合物链段的运动受到限制，材料表现出类似玻璃的刚性 高于Tg的温度下，链段有更多的活动空间，材料变得柔软。 Melting Process #\r在加热过程中，热能将首先在Amorphous Region开始破坏分子间的键能。 随着持续加热，热能最终将足以破坏Crystal Region的分子间键 因此，Amorphous Region被破坏时的较低温度被称为Glass Transition Temperature 当Polymer低于Tg时，其像普通窗玻璃一样又硬又脆 Loading Time 施加负载的时间 #\r快速施加负荷：当它被快速拉伸时会像脆性材料一样断裂，且无永久变形。这是因为其分子链较短，在快速拉伸时分子之间没有足够的时间进行重新排列，导致聚合物直接破裂。 长时间施加负荷，聚合物会发生蠕变，即随着时间的延长，材料会逐渐变形 弹性变形（Elastic Deformation） #\r特点：弹性变形是瞬时的，即加载后立即产生变形，卸载后立即恢复原状。 变形性质：弹性变形是可逆的，即材料可以恢复到原始形状，不会有永久的变形残留。 应用场景：在桌子短暂放置在地毯上的情况下，地毯纤维受到压力后会产生弹性变形，但桌子移开后，地毯几乎立即恢复原状。 分子运动：在弹性变形中，聚合物分子链段的变形非常有限，分子间没有发生显著的滑动或重新排列。 粘性变形（Viscous Deformation） #\r特点：粘性变形是随时间累积的，即需要长时间加载才能显现。 变形性质：粘性变形是不可逆的，即变形在卸载后不完全恢复，会留下永久变形。 应用场景：当桌子长时间放置在地毯上（例如一年），地毯纤维会缓慢移动或滑动，导致永久变形，即使桌子移开后，地毯也无法完全恢复原状。 分子运动：在粘性变形中，聚合物分子链段逐渐滑动，重新排列，表现为类似液体流动的行为，这个过程不可逆。 粘弹性变形（Viscoelastic Deformation） #\r聚合物材料通常表现出粘弹性变形，即兼具弹性和粘性变形的特性。它们在短时间内表现为弹性变形，但在长时间加载下逐渐表现出粘性变形。不同聚合物在粘弹性方面有所差异：\nLimits of the noodle model #\r一个模型几乎总是有缺点的。 如果不是这样，我们就会称之为定律 再次考虑前面提到的Transparent Glass， 我们说过，部分原因是由于这种聚合物是完全无定形的，这是事实 但是，这并不能解释为什么Amorphous Polymer本身就应该是透明的 要真正理解这一点，我们需要进一步了解可见光的本质以及光与材料中电子的相互作用 这是因为材料的透明性主要取决于光在其中的传播方式 当可见光照射到材料上时，光会与材料中的电子发生相互作用，而这种相互作用的方式决定了光是被吸收、反射还是通过材料 在透明的非晶态聚合物（如Plexiglas®）中，分子的电子结构允许可见光穿过，而不会显著散射或吸收光，因此表现出透明性。 相比之下，在非晶态金属中，电子结构密集且自由度较高，能够大量吸收和反射光，从而使材料表现为不透明和反光 ","date":"Nov 10 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms6.plastics/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/10/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003ePolymer 聚合物 \r\n    \u003cdiv id=\"polymer-%E8%81%9A%E5%90%88%E7%89%A9\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#polymer-%E8%81%9A%E5%90%88%E7%89%A9\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e前缀“poly-”意指“很多”，暗示了这些聚合物分子结构中有许多重复的部分。\u003c/li\u003e\n\u003cli\u003e而“mer”指的是“重复单元”或“基元”，这是一种分子的基本单元\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eBasic Structure \r\n    \u003cdiv id=\"basic-structure\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#basic-structure\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e与Crystal Structure不同，他们的基本结构是Cubics。并且其会在三维空间（即沿着三个方向）重复排列，形成一个规则的晶体结构\u003c/li\u003e\n\u003cli\u003ePolymer的结构则不同，它的基元通常只在One Dimnesion上重复，这种重复形成了一条长链\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS6.Plastics/ECMS6.Plastics.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"MCMS 6. Plastics","type":"docs"},{"content":" Last Edit: 11/4/2024\nYour browser does not support the video tag. Full Code is Provided\nimport numpy as np import torch import random class LinearRegression(Scene): def construct(self): def data_generator(w,b,num): X = torch.normal(0, 1, (num, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2,-3.4]) true_b = 4.2 features, labels = data_generator(true_w,true_b,1000) normal_data = features[:,[0]].numpy() #plt.hist(normal_data,bins=100,density=True,color=\u0026#39;lightblue\u0026#39;) head = Text(\u0026#34;Linear Regression - Buezwqwg\u0026#34;) head.set_color(BLUE) self.play(Create(head)) head_0 = Text(\u0026#34;In one process of Linear Regression, there bascially includes 5 steps\u0026#34;,font_size=30) self.play(Uncreate(head),Write(head_0)) self.play(head_0.animate.move_to(UP*3.5)) head_1 = Text(\u0026#34;1. Initial Parameters\u0026#34;,font_size=30) head_2 = Text(\u0026#34;2. Defining Model and Loss Function\u0026#34;,font_size=30) head_3 = Text(\u0026#34;3. Optimization\u0026#34;,font_size=30) head_4 = Text(\u0026#34;4. Loop\u0026#34;,font_size=30) head = VGroup(head_1,head_2,head_3,head_4) head.arrange(DOWN) self.play(Write(head)) # -------------------------------------------------------------------------------------------- head_5 = Text(\u0026#34;In this animate, we start with generating the data\u0026#34;,font_size=30) head_5.move_to(UP*3.5) self.play(Uncreate(head),Uncreate(head_0),Write(head_5)) code_text = \u0026#39;\u0026#39;\u0026#39; def data_generator(w, b, num): X = torch.normal(0, 1, (num, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2,-3.4]) true_b = 4.2 features, labels = data_generator(true_w,true_b,1000) \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) self.play(Write(code),Uncreate(head_5),run_time=3) self.wait(3) self.play(Unwrite(code)) axes = Axes( x_range=[-4, 4, 1], y_range=[0, 0.5, 0.1], axis_config={\u0026#34;color\u0026#34;: BLUE}, ).add_coordinates() # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2) normal_curve = axes.plot( lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2), color=YELLOW ) # 绘制均值为0的竖线 mean_line = DashedLine( start=axes.c2p(0, 0), end=axes.c2p(0, (1 / (2 * PI) ** 0.5)), color=RED ) # 添加图形和标注 self.play(Create(axes)) self.play(Create(normal_curve), Create(mean_line)) # 标注均值和标准差 mean_label = MathTex(r\u0026#34;\\mu=0\u0026#34;).next_to(mean_line, DOWN) std_label = MathTex(r\u0026#34;\\sigma=1\u0026#34;).next_to(normal_curve, UP, buff=0.5) self.play(Write(mean_label), Write(std_label)) # 展示最终效果 self.wait(2) self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line)) # -------------------------------------------------------------------------------------------- head = Text(\u0026#34;Displaying the distribution of features\u0026#34;) feature_one = features[:,[0]].tolist() feature_two = features[:,[1]].tolist() labels = labels.tolist() axes_1 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=5, # x轴的长度 y_length=5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_2 = Axes( x_range=[min(feature_two)[0], max(feature_two)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=5, # x轴的长度 y_length=5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes = VGroup(axes_1,axes_2) axes.arrange(RIGHT,buff=1) self.play(Create(axes)) points_1 = [] for i in range(len(labels)): dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0]) points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_1 = [Create(dot) for dot in points_1] points_2 = [] for i in range(len(labels)): dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0]) points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_2 = [Create(dot) for dot in points_2] self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005)) # 抽取样本-------------------------------------------------------------------------------------------- head = Text(\u0026#39;Shuffle the data and divided into samples(batches)\u0026#39;,font_size=30) self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2))) head.move_to(UP*3.5) code_text = \u0026#39;\u0026#39;\u0026#39; def data_iter(batch_size,features,labels): num = len(features) index = list(range(num)) random.shuffle(index) for i in range(0,num,batch_size): batch_index = torch.tensor(index[i:min(i+batch_size,num)]) yield features[batch_index], labels[batch_index] \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) self.play(Write(code)) self.wait(2) self.play(Uncreate(code),Unwrite(head)) def data_iter(batch_size,features,labels): num = len(features) index = list(range(num)) random.shuffle(index) for i in range(0,num,batch_size): batch_index = torch.tensor(index[i:min(i+batch_size,num)]) return batch_index.tolist() axes_1 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_2 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_3 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_4 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_5 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5) axes.arrange(RIGHT) sample_1 = data_iter(10,features,labels) points_1 = [] for i in sample_1: dot_position = axes_1.coords_to_point(features[i][0],labels[i][0]) points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_1 = [Create(dot) for dot in points_1] sample_2 = data_iter(10,features,labels) points_2 = [] for i in sample_2: dot_position = axes_2.coords_to_point(features[i][0],labels[i][0]) points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_2 = [Create(dot) for dot in points_2] sample_3 = data_iter(10,features,labels) points_3 = [] for i in sample_3: dot_position = axes_3.coords_to_point(features[i][0],labels[i][0]) points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_3 = [Create(dot) for dot in points_3] sample_4 = data_iter(10,features,labels) points_4 = [] for i in sample_4: dot_position = axes_4.coords_to_point(features[i][0],labels[i][0]) points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_4 = [Create(dot) for dot in points_4] sample_5 = data_iter(10,features,labels) points_5 = [] for i in sample_5: dot_position = axes_5.coords_to_point(features[i][0],labels[i][0]) points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_5 = [Create(dot) for dot in points_5] head = Text(\u0026#34;Display five of Sample Batches (Batch Size = 10)\u0026#34;,font_size=30) head.set_color(BLUE) head.move_to(UP*2.5) self.play(Write(head)) self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05)) self.wait(3) self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5))) # 定义模型-------------------------------------------------------------------------------------------- head_1 = Text(\u0026#39;Define the Function\u0026#39;) head_1.set_color(BLUE) code_text_1 = \u0026#39;\u0026#39;\u0026#39; def linreg(X, w, b): return torch.matmul(X, w) + b \u0026#39;\u0026#39;\u0026#39; code_1 = Code(code=code_text_1,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head_2 = Text(\u0026#39;Define the Loss Function\u0026#39;) head_2.set_color(BLUE) code_text_2 = \u0026#39;\u0026#39;\u0026#39; def squared_loss(y_hat, y): return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 \u0026#39;\u0026#39;\u0026#39; code_2 = Code(code=code_text_2,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head = VGroup(head_1,code_1,head_2,code_2) head.arrange(DOWN,buff=1) self.play(Write(head)) self.wait(2) self.play(Unwrite(head)) # 展示MSE-------------------------------------------------------------------------------------------- head = MathTex(r\u0026#34;Lose~Function~MSE~:(y_i - \\hat{y}_i)^2\u0026#34;) head.set_color(BLUE) head.move_to(UP*3) self.play(Write(head)) axes = Axes( x_range=[-10, 10, 2.5], y_range=[0, 100, 20], x_length=10, y_length=5, axis_config={\u0026#34;color\u0026#34;: GREEN}, ) # 定义MSE函数 mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10]) mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10]) # 将元素添加到场景中 self.play(Create(axes),Create(mse_curve)) self.wait(2) self.play(Uncreate(head)) head = Text(\u0026#34;The MSE Derivative indicates that loss will be increasing as it increase\u0026#34;,font_size=30) head.set_color(BLUE) head.move_to(UP*3) self.play(Write(head),Create(mse_der)) self.wait(3) self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes)) # 展示SGD-------------------------------------------------------------------------------------------- head = Text(\u0026#34;Now Conduct the Optimization Method\u0026#34;) head.move_to(UP*3) head.set_color(BLUE) code_text = \u0026#39;\u0026#39;\u0026#39; def sgd(params, lr, batch_size): with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head_2 = Text(\u0026#39;Apply this optimization method for each batch\u0026#39;) head_2.set_color(BLUE) sgd = MathTex(r\u0026#34;(w,b)\\leftarrow (w,b)-\\eta g\u0026#34;) main = VGroup(head,code,head_2,sgd) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(2) self.play(Uncreate(main),run_time=0.1) # 计算梯度-------------------------------------------------------------------------------------------- head = Text(\u0026#34;Now Calculate the Gradient\u0026#34;) head.set_color(BLUE) head.move_to(UP*3) grad = MathTex(r\u0026#34;\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w} \\left( y_i - (w x_i + b) \\right)^2\u0026#34;) grad_1 = MathTex(r\u0026#34;= \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - (wx_i + b)) \\cdot (-x_i)\u0026#34;) grad_2 = MathTex(r\u0026#34;= -\\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#34;) main = VGroup(head,grad,grad_1,grad_2) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(2) self.play(Uncreate(main),run_time=0.01) head = Text(\u0026#34;Then apllies the formula for 1000/10=100 Times\u0026#34;,font_size=45) head.set_color(BLUE) grad = MathTex(r\u0026#39;w := w + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#39;) grad_1 = MathTex(r\u0026#34;b := b + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i)\u0026#34;) main = VGroup(head,grad,grad_1) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(3) self.play(Uncreate(main),run_time=0.01) # 总结-------------------------------------------------------------------------------------------- code_text = \u0026#39;\u0026#39;\u0026#39; lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) l.sum().backward() sgd([w, b], lr, batch_size) with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;) \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head = Text(\u0026#34;Then applies the whole process in epochs and that\u0026#39;s linear regression\u0026#34;,font_size=30) main = VGroup(head,code) main.arrange(DOWN,buff=1) self.play(Write(main)) --- ","date":"Nov 4 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/linearregression/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/4/2024\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cvideo width=\"640\" height=\"360\" controls\u003e\n  \u003csource src=\"LinearRegression.mp4\" type=\"video/mp4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\u003cp\u003eFull Code is Provided\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-from\" data-lang=\"from\"\u003eimport numpy as np\nimport torch\nimport random\n\nclass LinearRegression(Scene):\n    def construct(self):\n        def data_generator(w,b,num):\n            X = torch.normal(0, 1, (num, len(w)))\n            y = torch.matmul(X, w) + b\n            y += torch.normal(0, 0.01, y.shape)\n            return X, y.reshape((-1, 1))\n\n        true_w = torch.tensor([2,-3.4])\n        true_b = 4.2\n        features, labels = data_generator(true_w,true_b,1000)\n        normal_data = features[:,[0]].numpy()\n        #plt.hist(normal_data,bins=100,density=True,color=\u0026#39;lightblue\u0026#39;)  \n         \n\n        head = Text(\u0026#34;Linear Regression - Buezwqwg\u0026#34;)\n        head.set_color(BLUE)\n        self.play(Create(head))\n\n        head_0 = Text(\u0026#34;In one process of Linear Regression, there bascially includes 5 steps\u0026#34;,font_size=30)\n        self.play(Uncreate(head),Write(head_0))\n        self.play(head_0.animate.move_to(UP*3.5))\n        head_1 = Text(\u0026#34;1. Initial Parameters\u0026#34;,font_size=30)\n        head_2 = Text(\u0026#34;2. Defining Model and Loss Function\u0026#34;,font_size=30)\n        head_3 = Text(\u0026#34;3. Optimization\u0026#34;,font_size=30)\n        head_4 = Text(\u0026#34;4. Loop\u0026#34;,font_size=30)\n        head = VGroup(head_1,head_2,head_3,head_4)\n        head.arrange(DOWN)\n        self.play(Write(head))\n\n        # --------------------------------------------------------------------------------------------\n\n        head_5 = Text(\u0026#34;In this animate, we start with generating the data\u0026#34;,font_size=30)\n        head_5.move_to(UP*3.5)\n        self.play(Uncreate(head),Uncreate(head_0),Write(head_5))\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def data_generator(w, b, num):\n            X = torch.normal(0, 1, (num, len(w)))\n            y = torch.matmul(X, w) + b\n            y += torch.normal(0, 0.01, y.shape)\n            return X, y.reshape((-1, 1))\n            \n        true_w = torch.tensor([2,-3.4])\n        true_b = 4.2\n        features, labels = data_generator(true_w,true_b,1000)\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        self.play(Write(code),Uncreate(head_5),run_time=3)\n        self.wait(3)\n        self.play(Unwrite(code))\n        axes = Axes(\n            x_range=[-4, 4, 1],\n            y_range=[0, 0.5, 0.1],\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\n        ).add_coordinates()\n\n        # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2)\n        normal_curve = axes.plot(\n            lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2),\n            color=YELLOW\n        )\n\n        # 绘制均值为0的竖线\n        mean_line = DashedLine(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(0, (1 / (2 * PI) ** 0.5)),\n            color=RED\n        )\n\n        # 添加图形和标注\n        self.play(Create(axes))\n        self.play(Create(normal_curve), Create(mean_line))\n        \n        # 标注均值和标准差\n        mean_label = MathTex(r\u0026#34;\\mu=0\u0026#34;).next_to(mean_line, DOWN)\n        std_label = MathTex(r\u0026#34;\\sigma=1\u0026#34;).next_to(normal_curve, UP, buff=0.5)\n        self.play(Write(mean_label), Write(std_label))\n\n        # 展示最终效果\n        self.wait(2)\n        self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line))\n\n        # --------------------------------------------------------------------------------------------\n\n        head = Text(\u0026#34;Displaying the distribution of features\u0026#34;)\n        feature_one = features[:,[0]].tolist()\n        feature_two = features[:,[1]].tolist()\n        labels = labels.tolist()\n        axes_1 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=5,  # x轴的长度\n            y_length=5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_2 = Axes(\n            x_range=[min(feature_two)[0], max(feature_two)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=5,  # x轴的长度\n            y_length=5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes = VGroup(axes_1,axes_2)\n        axes.arrange(RIGHT,buff=1)\n        self.play(Create(axes))\n\n        points_1 = []\n        for i in range(len(labels)):\n            dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\n            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_1 = [Create(dot) for dot in points_1]\n        points_2 = []\n        for i in range(len(labels)):\n            dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\n            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_2 = [Create(dot) for dot in points_2]\n        self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005))\n\n        # 抽取样本-------------------------------------------------------------------------------------------- \n\n        head = Text(\u0026#39;Shuffle the data and divided into samples(batches)\u0026#39;,font_size=30)\n        self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)))\n        head.move_to(UP*3.5)\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def data_iter(batch_size,features,labels):\n            num = len(features)\n            index = list(range(num))\n            random.shuffle(index)\n            for i in range(0,num,batch_size):\n                batch_index = torch.tensor(index[i:min(i+batch_size,num)])\n                yield features[batch_index], labels[batch_index]\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        self.play(Write(code))\n        self.wait(2)\n        self.play(Uncreate(code),Unwrite(head))\n\n        def data_iter(batch_size,features,labels):\n            num = len(features)\n            index = list(range(num))\n            random.shuffle(index)\n            for i in range(0,num,batch_size):\n                batch_index = torch.tensor(index[i:min(i+batch_size,num)])\n                return batch_index.tolist()\n\n\n\n        axes_1 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        \n\n        axes_2 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        \n        axes_3 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_4 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_5 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5)\n        axes.arrange(RIGHT)\n\n        sample_1 = data_iter(10,features,labels)\n        points_1 = []\n        for i in sample_1:\n            dot_position = axes_1.coords_to_point(features[i][0],labels[i][0])\n            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_1 = [Create(dot) for dot in points_1]\n\n        sample_2 = data_iter(10,features,labels)\n        points_2 = []\n        for i in sample_2:\n            dot_position = axes_2.coords_to_point(features[i][0],labels[i][0])\n            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_2 = [Create(dot) for dot in points_2]\n\n        sample_3 = data_iter(10,features,labels)\n        points_3 = []\n        for i in sample_3:\n            dot_position = axes_3.coords_to_point(features[i][0],labels[i][0])\n            points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_3 = [Create(dot) for dot in points_3]\n\n        sample_4 = data_iter(10,features,labels)\n        points_4 = []\n        for i in sample_4:\n            dot_position = axes_4.coords_to_point(features[i][0],labels[i][0])\n            points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_4 = [Create(dot) for dot in points_4]\n\n        sample_5 = data_iter(10,features,labels)\n        points_5 = []\n        for i in sample_5:\n            dot_position = axes_5.coords_to_point(features[i][0],labels[i][0])\n            points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_5 = [Create(dot) for dot in points_5]   \n        head = Text(\u0026#34;Display five of Sample Batches (Batch Size = 10)\u0026#34;,font_size=30)\n        head.set_color(BLUE)\n        head.move_to(UP*2.5)\n        self.play(Write(head))\n        self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05))\n        self.wait(3)\n        self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5)))\n\n        # 定义模型-------------------------------------------------------------------------------------------- \n\n        head_1 = Text(\u0026#39;Define the Function\u0026#39;)\n        head_1.set_color(BLUE)\n        code_text_1 = \u0026#39;\u0026#39;\u0026#39;\n        def linreg(X, w, b):\n            return torch.matmul(X, w) + b\n        \u0026#39;\u0026#39;\u0026#39;\n        code_1 = Code(code=code_text_1,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head_2 = Text(\u0026#39;Define the Loss Function\u0026#39;)\n        head_2.set_color(BLUE)\n        code_text_2 = \u0026#39;\u0026#39;\u0026#39;\n        def squared_loss(y_hat, y):\n            return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n        \u0026#39;\u0026#39;\u0026#39;\n\n\n        code_2 = Code(code=code_text_2,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head = VGroup(head_1,code_1,head_2,code_2)\n        head.arrange(DOWN,buff=1)\n        self.play(Write(head))\n        self.wait(2)\n        self.play(Unwrite(head))\n\n        # 展示MSE--------------------------------------------------------------------------------------------    \n        head = MathTex(r\u0026#34;Lose~Function~MSE~:(y_i - \\hat{y}_i)^2\u0026#34;)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        self.play(Write(head))\n        axes = Axes(\n            x_range=[-10, 10, 2.5],\n            y_range=[0, 100, 20],\n            x_length=10,\n            y_length=5,\n            axis_config={\u0026#34;color\u0026#34;: GREEN},\n        )\n        \n        # 定义MSE函数\n        mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10])\n        mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10])\n        # 将元素添加到场景中\n        self.play(Create(axes),Create(mse_curve))\n        self.wait(2)\n        self.play(Uncreate(head))\n        head = Text(\u0026#34;The MSE Derivative indicates that loss will be increasing as it increase\u0026#34;,font_size=30)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        self.play(Write(head),Create(mse_der))\n        self.wait(3)\n        self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes))\n\n\n        # 展示SGD--------------------------------------------------------------------------------------------\n        head = Text(\u0026#34;Now Conduct the Optimization Method\u0026#34;)\n        head.move_to(UP*3)\n        head.set_color(BLUE)\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def sgd(params, lr, batch_size):\n        with torch.no_grad():\n            for param in params:\n                param -= lr * param.grad / batch_size\n                param.grad.zero_()\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head_2 = Text(\u0026#39;Apply this optimization method for each batch\u0026#39;)\n        head_2.set_color(BLUE)\n        sgd = MathTex(r\u0026#34;(w,b)\\leftarrow (w,b)-\\eta g\u0026#34;)\n        main = VGroup(head,code,head_2,sgd)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(2)\n        self.play(Uncreate(main),run_time=0.1)\n        \n        # 计算梯度--------------------------------------------------------------------------------------------\n        head = Text(\u0026#34;Now Calculate the Gradient\u0026#34;)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        grad = MathTex(r\u0026#34;\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w} \\left( y_i - (w x_i + b) \\right)^2\u0026#34;)\n        grad_1 = MathTex(r\u0026#34;= \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - (wx_i + b)) \\cdot (-x_i)\u0026#34;)\n        grad_2 = MathTex(r\u0026#34;= -\\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#34;)\n\n        main = VGroup(head,grad,grad_1,grad_2)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(2)\n        self.play(Uncreate(main),run_time=0.01)\n        head = Text(\u0026#34;Then apllies the formula for 1000/10=100 Times\u0026#34;,font_size=45)\n        head.set_color(BLUE)\n        grad = MathTex(r\u0026#39;w := w + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#39;)\n        grad_1 = MathTex(r\u0026#34;b := b + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i)\u0026#34;)\n        main = VGroup(head,grad,grad_1)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(3)\n        self.play(Uncreate(main),run_time=0.01)\n\n        # 总结--------------------------------------------------------------------------------------------\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        lr = 0.03\n        num_epochs = 3\n        net = linreg\n        loss = squared_loss\n\n        for epoch in range(num_epochs):\n            for X, y in data_iter(batch_size, features, labels):\n                l = loss(net(X, w, b), y)\n                l.sum().backward()\n                sgd([w, b], lr, batch_size)\n            with torch.no_grad():\n                train_l = loss(net(features, w, b), labels)\n                print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;)\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head = Text(\u0026#34;Then applies the whole process in epochs and that\u0026#39;s linear regression\u0026#34;,font_size=30)\n        main = VGroup(head,code)\n        main.arrange(DOWN,buff=1)\n        self.play(Write(main))\n---\n\u003c/code\u003e\u003c/pre\u003e","title":"Linear Regression","type":"docs"},{"content":" Last Edit: 10/24/24\nIntroduction to Determinate #\r行列式是一个每个方阵都具有的数值 Determinate measures the factor by which the area of a given region increases or decreases The \u0026ldquo;determinant\u0026rdquo; of a transformation Determinate in R^2 #\rDeterminant计算的是Linear Transformation改变的Basis Vector所围成的面积的大小\n而对于一个Linear Transformation，大部分情况下Basis Vector围成的面积都是一个长方形\n而[[MAT188 Chapter 2 Linear Transformations#2.2 Linear Transformations in Geometry]]中存在一种Sheer Transformation，即对于一个Basis Vector来说，其出现了不属于其方向上的分量\n如上图中的\\(\\vec e_2\\)来说，其为\u0026lt;2,2\u0026gt;，即产生了Sheer\n在这种情况下，所围成的面积便成为了Parallelogram\n于是就有了两种计算\\(\\mathbb R^2\\)行列式的办法 $$det(A)=|A||B|sin\\theta$$ 这个平行四边形同时适用于Cross Product于Determinate\n这一个公式同样也是[[Cross Product 向量叉乘]]的大小（Norm），同时也是一个3x3Determinate的大小（体积）\n如果说Cross Product要找的是一个向量，Determinate要找的则是一个体积\n总的来说，要找\\(R^2\\)中的Determinate的值，其本质在求Linear Transformation后Basis Vector围成的面积\n而这一个面积可以通过\\(|A||B|sin\\theta\\)求，其同时也可以通过\n两个向量的Position Vector上的点的差值求 其最后化简之后便有 $$\\text{det} \\left( \\begin{bmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{bmatrix} \\right) = (a + b)(c + d) - ac - bd - 2bc = ad - bc $$ 这便是Determinate最初的定义 The Determinate of a 3x3 Matrix #\r$$A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \\begin{bmatrix} | \u0026amp; | \u0026amp; | \\ \\vec{u} \u0026amp; \\vec{v} \u0026amp; \\vec{w} \\ | \u0026amp; | \u0026amp; | \\end{bmatrix} $$\n3x3的Matrix的Determinate的几何意义为在\\(\\mathbb R^3\\)中的Parallelepiped的Volume 而对于\\(\\mathbb R^3\\)中的三个向量来说，如果它们线性相关（如共面，即两个向量可以通过Lienar Conbination得到第三个向量的情况下），则他们在3x3Determinate的几何意义也是就是体积便不再存在，其是一个高度为0的长方体 具体来说判断的方法便是\\(\\vec{u} \\cdot (\\vec{v} \\times \\vec{w}) = 0\\)，见下图 Determinate为0的几何意义 #\r具体来说，当\\(det(A)=0\\)的时候，其相当于一个Linear Transformation至少压缩了一个维度 而在维度被压缩之后，此过程并不可逆，见下图 Definition 6.1.1 Determinant of a 3 × 3 matrix, in terms of the Columns #\r在上面提到过了行列式的几何意义为体积，而\\(\\vec u,\\vec v\\)并不会一直出现在xy Plane中，要计算其体积，基本上要用底面积乘以高的形式 而底面积则可以通过\\(\\vec c= \\vec v\\times \\vec w\\)的Cross Product，同时求出其大小与方向 具体来说，其大小即为\\(|u|\\)，而其方向应该是垂直于vw Plane的 而将\\(\\vec u\\cdot \\vec c\\)时，则可以得到Determinate中的第三个Vetor在一个垂直于vw Plane的同时具有方向和大小的向量\\(\\vec c\\)上的Projection长度乘以其向量\\(\\vec c\\)（本身Norm为vw所围成的平行四边形的面积） 则最终得到Determinate中的第三个向量\\(\\vec u\\)在一个垂直于vw Plane的方向上的分量，在几何意义上来说为平行六面体的高 和一个\\(\\vec v\\times \\vec w\\)所得到的两个向量围成的平行四边形的长度，即平行六面体的底面积 两者相乘便可以得到该3x3 Matrix Determinate的值，即这三个Vector所围成的Parallelepiped体积\\ $$\\begin{align}\\text{det} , A = \\vec{u} \\cdot (\\vec{v} \\times \\vec{w}) \\ = \\begin{bmatrix} a_{11} \\ a_{21} \\ a_{31} \\end{bmatrix} \\cdot \\left( \\begin{bmatrix} a_{12} \\ a_{22} \\ a_{32} \\end{bmatrix} \\times \\begin{bmatrix} a_{13} \\ a_{23} \\ a_{33} \\end{bmatrix} \\right) \\ = \\begin{bmatrix} a_{11} \\ a_{21} \\ a_{31} \\end{bmatrix} \\cdot \\begin{bmatrix} a_{22}a_{33} - a_{32}a_{23} \\ a_{32}a_{13} - a_{12}a_{33} \\ a_{12}a_{23} - a_{22}a_{13} \\end{bmatrix} \\ = a_{11}(a_{22}a_{33} - a_{32}a_{23}) + a_{21}(a_{32}a_{13} - a_{12}a_{33}) + a_{31}(a_{12}a_{23} - a_{22}a_{13}) \\ = a_{11}a_{22}a_{33} - a_{11}a_{32}a_{23} + a_{21}a_{32}a_{13} - a_{21}a_{12}a_{33} + a_{31}a_{12}a_{23} - a_{31}a_{22}a_{13}\\end{align} $$\n上述介绍的所有都是有助于理解Determinant的而非考试的重点，意义在于理解，正式的内容将从下面开始\nProperties of Determinant #\rLinearity of Determinant #\r行列式对任何一列或一行都是线性的 也就是说，当我们把一列（或一行）表示为两个向量的和或乘以一个标量时，行列式也可以相应地拆分为两个行列式的和，或乘以标量 当有如下Determinate时 $$L(\\vec{x}) = \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x}+\\vec y \u0026 -\r\\end{bmatrix}\r\\right)$$\r- Matrix23位置的值为一个Variable x，而因为det在任意一行，列中都是线性的，即其也满足Linear的两个定义\r$$L(\\vec{x} + \\vec{y}) = L(\\vec{x}) + L(\\vec{y}) \\quad \\text{and} \\quad L(k\\vec{x}) = kL(\\vec{x})$$\r- 在Determinate中有\r$$\\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x} + \\vec{y} \u0026-\r\\end{bmatrix}\r\\right)\r= \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x} \u0026 -\r\\end{bmatrix}\r\\right)\r+ \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{y} \u0026 -\r\\end{bmatrix}\r\\right)\r$$\r$$\\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 k\\vec{x} \u0026 -\r\\end{bmatrix}\r\\right)\r= k \\, \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x} \u0026 -\r\\end{bmatrix}\r\\right)\r$$\r- 如果在矩阵的一行乘上 t而剩下的n-1行保持不变，则行列式的值就要乘上 t\r$$\\left| \\begin{array}{cc} ta \u0026 tb \\\\ c \u0026 d \\end{array} \\right| = t \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r$$\r- 同理对于Linear Transformation的另外一个性质也通用 $$\\left| \\begin{array}{cc} a + a' \u0026 b + b' \\\\ c \u0026 d \\end{array} \\right| = \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right| + \\left| \\begin{array}{cc} a' \u0026 b' \\\\ c \u0026 d \\end{array} \\right|\r$$\r- 需要知道的是，这并不是在说$$det(A+B)=det(A)+det(B)$$\r- 而是对于Square Matrxi的每一行来说是Linear的\rChange of matrix\u0026rsquo;s effect on Determinate #\r当交换矩阵的两行，Determinant的值将会变号 如果你交换一个3×3矩阵的两行，行列式的值也会反号 同理也能知道 $$\\left| \\begin{array}{cc} 0 \u0026 1 \\\\ 1 \u0026 0 \\end{array} \\right| = -1\r$$\rNon-Squre Matrix Can\u0026rsquo;t Have Determinate #\rNon-Squre的Matrix会出现在当有两行是完全相同的时候 其证明可以是，当交换了两个相同的Matrix的Row的时候，其根据[[#Change of matrix\u0026rsquo;s effect on Determinate]]会发生变号，而可以观察发现新的Matrix和原来的没有区别，有Det=-Det，所以det=0 Row Operation\u0026rsquo;s influence on Determinate #\r从矩阵的某行 k 减去另一行 i 的倍数，并不改变行列式的数值（消元的过程不改变行列式） $$\\left| \\begin{array}{cc} a \u0026 b \\\\ c - ta \u0026 d - tb \\end{array} \\right| = \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right| - \\left| \\begin{array}{cc} a \u0026 b \\\\ ta \u0026 tb \\end{array} \\right|\r$$\r$$= \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r- t \\left| \\begin{array}{cc} a \u0026 b \\\\ a \u0026 b \\end{array} \\right|\r$$\r- 根据[[#Change of matrix's effect on Determinate]]，后一项的Determinant为0，即整体Det不变\rZero Rows Determinant #\r矩阵 A 的某一行都是 0，则其行列式为 0 根据[[#Linearity of Determinate]]可以知道，当t=0的时候， $$\\left| \\begin{array}{cc} ta \u0026 tb \\\\ c \u0026 d \\end{array} \\right| = t \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|$$\r- 有$$\\left| \\begin{array}{cc} 0\\cdot a \u0026 0\\cdot b \\\\ c \u0026 d \\end{array} \\right|= 0 \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|$$\r即Determinant为0 Trangular Matrix\u0026rsquo;s Determinant #\r$$\\left| \\begin{array}{cccc} d_1 \u0026 * \u0026 \\cdots \u0026 * \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 * \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right| = \\left| \\begin{array}{cccc} d_1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right|\r= d_1 d_2 \\cdots d_n\r\\left| \\begin{array}{cccc} 1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 1 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 1 \\end{array} \\right| = d_1 d_2 \\cdots d_n\r$$\r根据[[#Row Operation\u0026rsquo;s influence on Determinate]]，当是通过Row Operation得到Triangular Matrix的时候，正负号可能发生改变 对于非Diagonal上的元素，根据[[#Row Operation\u0026rsquo;s influence on Determinate]]可以做Row Operation在不改变Determinant的前提下将他们全部消掉，有 $$\\left| \\begin{array}{cccc} d_1 \u0026 * \u0026 \\cdots \u0026 * \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 * \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right|= \\left| \\begin{array}{cccc} d_1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right|$$\r- 再通过[[#Linearity of Determinate]]提取出每个Row Pivot上的d\r$$= d_1 d_2 \\cdots d_n\r\\left| \\begin{array}{cccc} 1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 1 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 1 \\end{array} \\right| = d_1 d_2 \\cdots d_n$$\rSingular Matrix\u0026rsquo;s Determinant #\r对于Rank小于Row的Square Matrix，其Determinant为0 Numerial Approach of Determinant #\r当有一个Matrix的时候，想要计算其Determinant，即需要将其化为Triangular Matrix，最简单的方式即为化为Upper Triangular Matrix 而要消成Upper Triangular Matrix的方式即为将C化为0（拿2x2Matrix举例） $$\\left[ \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc} a \u0026 b \\\\ 0 \u0026 d - \\frac{c}{a}b \\end{array} \\right]\r$$\r$$\\left| \\begin{array}{cc} a \u0026amp; b \\ c \u0026amp; d \\end{array} \\right| = a \\left( d - \\frac{c}{a}b \\right) = ad - bc $$\nDeterminant of Product #\r\\(det(AB)=det(A)\\cdot det(B)\\) \\(det(A+B)\\neq det(A)+det(B)\\) Determinant of Inverse #\r\\(det(A^{-1})\\) 已知\\(A^{-1}A=I\\)，即\\(det(A^{-1})det(A)=det(I)=1\\) 则有\\(det(A^{-1})=\\frac{1}{det(A)}\\) Determinant of Square #\r\\(det(A^2)=det(A)^2=det(A)\\cdot det(A)\\) Determinant of Coefficient before Matrix #\r\\(det(2A)=2^ndet(A)\\) 对于nxn Matrix来说，犹豫每一个Row都乘上的Coefficient 2，即存在\\(2^n\\)的总系数 Determinant of Transpose #\r\\(det(A^T)=det(A)\\) $$\\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r= \\left| \\begin{array}{cc} a \u0026 c \\\\ b \u0026 d \\end{array} \\right|\r= ad - bc\r$$\rFormular for Determinant #\r2x2 #\r$$\\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r=\r\\left| \\begin{array}{cc} a \u0026 0 \\\\ c \u0026 d \\end{array} \\right|\r+ \\left| \\begin{array}{cc} 0 \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r=\r\\left| \\begin{array}{cc} a \u0026 0 \\\\ c \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{cc} 0 \u0026 d \\\\ 0 \u0026 d \\end{array} \\right|\r+ \\left| \\begin{array}{cc} 0 \u0026 b \\\\ 0 \u0026 d \\end{array} \\right|\r=\r0 + ad - cb + 0\r=\rad - bc\r$$\r3x3 #\r将每一行拆成3部分，每一部分都对应了不同的行上的不同元素 总共会得到\\(3^3\\)个Matrix即27个，而其中大部分Matrix由于行或列上全为0有Det=0 而其中的非零情况出现在每一列都有的情况下（因为我们是从行出发开始分解的，所以每一行都保证了有值） $$\\left| \\begin{array}{ccc} a_{11} \u0026 a_{12} \u0026 a_{13} \\\\ a_{21} \u0026 a_{22} \u0026 a_{23} \\\\ a_{31} \u0026 a_{32} \u0026 a_{33} \\end{array} \\right|\r=\r\\left| \\begin{array}{ccc} a_{11} \u0026 0 \u0026 0 \\\\ 0 \u0026 a_{22} \u0026 0 \\\\ 0 \u0026 0 \u0026 a_{33} \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} a_{11} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 a_{23} \\\\ 0 \u0026 a_{32} \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 a_{12} \u0026 0 \\\\ a_{21} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 a_{33} \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 a_{12} \u0026 0 \\\\ 0 \u0026 a_{22} \u0026 0 \\\\ a_{31} \u0026 0 \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 0 \u0026 a_{13} \\\\ a_{21} \u0026 0 \u0026 0 \\\\ 0 \u0026 a_{32} \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 0 \u0026 a_{13} \\\\ 0 \u0026 a_{22} \u0026 0 \\\\ a_{31} \u0026 0 \u0026 0 \\end{array} \\right|\r$$\r$$=\ra_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}$$\r- 其中所有的负值，都是用了[[#Change of matrix's effect on Determinate]]将Matrix做Row Exchange成Uppertriangular Matrix而导致的Determinant的变号\r但这个做法再4x4中并不通用，所以需要从2x2，3x3中推导出nxn的公式 Big Formula A #\r$$det(A)=\\sum_{n!}\\pm a_{1\\alpha}a_{2\\beta}a_{3\\gamma}\\dots a_{n\\omega}$$\r\\(n!\\)：由于我们是用Row做的，即在Row1中有n个Column可以选，而到了Row2中，只有n-1个Column可以选，以此类推可能性即为\\(n!\\) \\(\\alpha,\\beta,\\gamma,\\omega\\)：列标号中的任何值 $$\\left| \\begin{array}{cccc} 0 \u0026 0 \u0026 1 \u0026 1 \\\\ 0 \u0026 1 \u0026 1 \u0026 0 \\\\ 1 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 0 \u0026 1 \\end{array} \\right| =\r\\left| \\begin{array}{cccc} 0 \u0026 0 \u0026 0 \u0026 1 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 0 \u0026 0 \\end{array} \\right|\r+\r\\left| \\begin{array}{cccc} 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{array} \\right|\r$$\r分解出的两个Matrix中，第一个需要做两次Row Exchange得到Identity，即为1，而第二个则需要1次Row Exchange就可以得到Identify，即为-1，1-1=0 Cofactor Formula 代数余子式 #\r3x3 #\r代数余子式是用较小的矩阵的行列式来写出 n 阶行列式的公式 $$\\text{det} \\left( \\mathbf{A} \\right) = a_{11} \\left( a_{22}a_{33} - a_{23}a_{32} \\right)\r+ a_{12} \\left( -a_{21}a_{33} + a_{23}a_{31} \\right)\r+ a_{13} \\left( a_{21}a_{32} - a_{22}a_{31} \\right)\r$$\r$$=\\left| \\begin{array}{ccc} a_{11} \u0026 0 \u0026 0 \\\\ 0 \u0026 a_{22} \u0026 a_{23} \\\\ 0 \u0026 a_{32} \u0026 a_{33} \\end{array} \\right|\r+\r\\left| \\begin{array}{ccc} 0 \u0026 a_{12} \u0026 0 \\\\ a_{21} \u0026 0 \u0026 a_{23} \\\\ a_{31} \u0026 0 \u0026 a_{33} \\end{array} \\right|\r+\r\\left| \\begin{array}{ccc} 0 \u0026 0 \u0026 a_{13} \\\\ a_{21} \u0026 a_{22} \u0026 0 \\\\ a_{31} \u0026 a_{32} \u0026 0 \\end{array} \\right|$$\r由于第二个Martri在化为Identity时只需要一次Row Exchange而其他的都需要两次，所以第二个Cofactor为减去 Cofactor Formula #\r将原公式中属于矩阵第一行的\\(a_{ij}\\)提出来，其系数即为代数余子式，是一个低阶行列式的值。这个低阶行列式是由原矩阵去掉\\(a_{ij}\\)所在的行和列组成的。 对矩阵中任意元素\\(a_{ij}\\)而言，其代数余子式\\(C_{ij}\\)j就是矩阵的行列式的公式中\\(a_{ij}\\)的系数 \\(C_{ij}\\)等于原矩阵移除第i行和第j列后剩余元素组成的n-1阶矩阵的行列式数值乘以\\((-1)^{i+j}\\) \\(C_{ij}\\)在 i+j 为偶数时为正，奇数时为负数 则可以总结对于n阶Square Matrix来说，有 $$\\text{det} \\left( \\mathbf{A} \\right) = a_{11} C_{11} + a_{12} C_{12} + \\cdots + a_{1n} C_{1n}\r$$\rex. in 2x2 #\rCofactor Formula最简单的应用即为在2x2 Matrix中，有 $$\\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r= ad + b(-c)$$\rex. 三对角阵（tridiagonal matrix） #\r只在Tridiagonal Matrix这种特殊结构中可行 $$\\mathbf{A_4} = \\left[ \\begin{array}{cccc} 1 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 1 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 1 \u0026 1 \\\\ 0 \u0026 0 \u0026 1 \u0026 1 \\end{array} \\right]\r$$\r![[LA6.Determinats-9.png]]\rFormula for A Inverse #\r已知2阶Matrix的Inverse为 $$\\left[ \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right]\r= \\frac{1}{ad - bc} \\left[ \\begin{array}{cc} d \u0026 -b \\\\ -c \u0026 a \\end{array} \\right]\r$$\r- 通过观察上2x2的例子可以得出\r$$\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top$$\r通过观察可以发现d是a的C，-b是c的C，-c是b的C，a是d的C Adjoint Matrix 伴随矩阵 #\r此处的Cofactor的Transpose\\(C^T\\)便可以称为Adjoint Martirx，即伴随矩阵 对于Adjoint Matrix来说，其大小总是原Matrix的Dimension-1即为n-1 Proof of A Inverse #\r已知Gauss Jordan Elimination提到\\([A|I]\\)在A被消成I后，I会变成\\(A^{-1}\\) 同时\\(A^{-1}A=I\\)，现在将\\(\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top\\)带入 $$A\\cdot \\mathbf{A}^{-1} = A\\cdot\\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top=I$$\r$$A\\cdot C^T=det(A)\\cdot I$$\r如果上式成立，则\\(\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top\\)为真命题 $$\\mathbf{AC}^T = \\begin{bmatrix} a_{11} \u0026 \\cdots \u0026 a_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ a_{n1} \u0026 \\cdots \u0026 a_{nn} \\end{bmatrix}\r\\begin{bmatrix} C_{11} \u0026 \\cdots \u0026 C_{n1} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ C_{1n} \u0026 \\cdots \u0026 C_{nn} \\end{bmatrix}\r$$\r- 对于所有结果矩阵对角线上的的元素来说都有\r$$\\sum_{j=1}^{n} a_{1j} C_{1j} = \\det(\\mathbf{A})$$\r即他们本身就是det(A)的展开式 而现在要研究结果矩阵非对角线上的内容 可以发现每一个非对角线上的元素都将为0 $$AC^T = \\begin{bmatrix}\r\\det A \u0026 0 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 \\det A \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 0 \u0026 \\ddots \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 0 \u0026 \\cdots \u0026 \\det A\r\\end{bmatrix} = \\det(A)I\r$$\rCramer’s Rule 克莱姆法则 #\r对于问题Ax=b来说，其解法很简单的就等于\\(x=A^{-1}B\\) 而在知道了\\(A^{-1}\\)的值之后有 $$\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^T \\mathbf{b}$$\r选择乘上\\(C^T\\) 的不再是A而是b了，但一个Matrix乘以一个Cofactor Matrix的做法又令人想到了Determinant，可以发现 $$x_j = \\frac{\\det(\\mathbf{B}_j)}{\\det(\\mathbf{A})}\r$$\r其中每一个\\(B_i\\)都是一个第i列被\\(B_i\\)所替换的Matrix A，具体来说有 $$\\mathbf{B}_1 = \\begin{bmatrix}\rb_1 \u0026 a_{12} \u0026 \\cdots \u0026 a_{1n} \\\\\rb_2 \u0026 a_{22} \u0026 \\cdots \u0026 a_{2n} \\\\\rb_3 \u0026 a_{32} \u0026 \\ddots \u0026 \\vdots \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 a_{n-1\\,n} \\\\\rb_n \u0026 a_{n2} \u0026 \\cdots \u0026 a_{nn}\r\\end{bmatrix}\r, \\quad\r\\mathbf{B}_n = \\begin{bmatrix}\ra_{11} \u0026 \\cdots \u0026 a_{1\\,n-1} \u0026 b_1 \\\\\ra_{21} \u0026 \\cdots \u0026 a_{2\\,n-1} \u0026 b_2 \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \u0026 \\vdots \\\\\ra_{n-1\\,1} \u0026 \\cdots \u0026 a_{n-1\\,n-1} \u0026 b_{n-1} \\\\\ra_{n1} \u0026 \\cdots \u0026 a_{n2} \u0026 b_n\r\\end{bmatrix}\r$$\r可以发现等式中的\\(C^T_i \\cdot B_i\\)正好等于\\(B_i\\)的Determinant 其实相比于消元法，采用克莱姆法则计算方程的解效率较低。。。\n","date":"Oct 24 2024","externalUrl":null,"permalink":"/docs/linearalgebra/la6.determiants/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 10/24/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eIntroduction to Determinate \r\n    \u003cdiv id=\"introduction-to-determinate\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#introduction-to-determinate\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e行列式是一个每个方阵都具有的数值\u003c/li\u003e\n\u003cli\u003eDeterminate measures the factor by which the area of a given region increases or decreases\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/LinearAlgebra_Static/LA6.Determinants/LA6.Determinats.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"LA 6. Determinants","type":"docs"},{"content":"","date":"Oct 22 2024","externalUrl":null,"permalink":"/docs/mathematicalanalysis/","section":"Docs","summary":"","title":"Mathematical Analysis","type":"docs"},{"content":"\rYour browser does not support the video tag.\r","date":"Oct 22 2024","externalUrl":null,"permalink":"/docs/displays/projectilemotion/","section":"Docs","summary":"\u003cvideo width=\"640\" height=\"360\" controls\u003e\r\n  \u003csource src=\"Projectile.mp4\" type=\"video/mp4\"\u003e\r\n  Your browser does not support the video tag.\r\n\u003c/video\u003e","title":"Projectile Motion when air resisitance is propftional to velocity","type":"docs"},{"content":" “这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想” - 三体\nYour browser does not support the video tag.\r","date":"Oct 17 2024","externalUrl":null,"permalink":"/docs/displays/montecarlomethod/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003e“这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想” - 三体\u003c/p\u003e","title":"Monte Carlo Approach to Calculate π","type":"docs"},{"content":" Last Edit: 10/16/24\nStress-Strain Curve #\r当回头重新看Stress-Strain Curve的时候可以发现一些特殊的点 如Proportion Limit，Ultimate Tensile Strength两个点 Proportional Limit #\r在曲线的最初阶段，存在一个接近于Linear的区域，其代表了Linear Elastic的Region 而也必会存在一个点象征着Linear Elastic Region的结束 但犹豫一些测量的误差或者精度上的问题，将导致最终的这一个Linear Elastic结束的点无法被确定，所以需要一个约定俗成的方法 0.2% Offset Yield Strength #\r在\\(\\epsilon=0.002\\)的位置画一个平行于Linear Elastic Region的直线，其交于SS Curve的位置即为Yield Strength ex. #\rA hypothetical metal has a 0.2% offset yield strength of 358 MPa, an ultimate tensile strength of 522 MPa, and a fracture strength of 460 MPa. A sample of this metal, originally 1 m in length with a cross section of 2 mm × 2 mm is loaded along its long axis. Just before fracture, while the load is still applied, the length is 1.3 m and when the load is released, the length is 1.18 m. Calculate the modulus of elasticity in GPa.（这我做集贸啊）\n之前有公式\\(\\sigma=E\\epsilon\\) 现在需要将其推广到\\(E=\\frac{\\Delta \\sigma}{\\epsilon}\\)上 \\(\\Delta \\sigma\\)：有460MPa-0MPa=460MPa \\(\\epsilon\\)：有0.3-0.18=0.12 Uniform deformation #\r在经历了Linear Elastic Region后，出现的便是Uniform Platic区间 这个阶段内，材料的塑性变形均匀地分布在整个试样或构件中 其具体的Unifrom体现在了变形过程中结构的完整性上 在Non-Unifrom Region中，材料已经发生了局部的Fracture，即产生了Neck 这也解释了为什么在过了Ultimate Tensile strength后Stress开始下降，即当金属样品承受的应力值逐渐增大时，最终会开始失效。 在拉伸过程中，当金属原子间的一些键断裂时，就会出现这种情况 The Dislocation #\rDisloaction广泛存在于各种晶体材料中，不仅限于金属。 它们在不同类型材料中的运动机制和对材料性能的影响各不相同。 例如，在金属中位错运动相对容易，而在陶瓷和半导体中则较为困难，且其作用更为复杂 具体来说存在有4种不同的Imperfections，来自不同的维度 Dislocation Density #\rDislocation Density: 材料中单位体积内存在的位错数量 Dislocation Density越高，材料的强度和硬度可能会有所增加，但延展性会降低 Metal #\rDislocations are always present in metals. 可以通过加热的方式更改Metal的dislocation density Zero-Dimensional Imperfections or Point Defects #\r点缺陷（Point Defect）是指材料的晶体结构中，由于原子或离子位置上的异常，导致的局部晶体结构缺陷 Interstitial Impurities #\rInterstitial Impurities指的是一些较小的Atom（比如碳、氮等）进入了Crystal中本来空着的间隙位置 Substitutional Impurities #\r当一个外来原子取代了晶体中正常位置上的原子时，形成Substitutional Impurities。 这种缺陷在合金中常见，例如铜和锌形成的黄铜 Vacancies #\r在晶体中，一个本应有原子的位置上缺少了一个原子 它会导致周围的原子重新调整位置，影响材料的物理性质 但Vacancies的产生并不会导致Material的Strength发生改变s Zero-Dimension\u0026rsquo;s Influence on Higher Dimension #\r当Zero Dimension Impurity发生的时候，其会对周围的Crystal Structure产生一个Strain Fields 这个Strain Field将会Repel其他的Atom进入Dislocation 其通常会造成一种One-Dimensional Imperfection Ratio of the Number of Vacancies #\r$$\\frac{N_v}{N} = e^{\\frac{-Q_v}{kT}} \\tag{1}$$\nNv​：这是Crystal中Number of Vancancies，即晶体结构中缺失原子的位置数。\nN：Number of Atoms\nQv​：表示生成一个Vacancies所需的Energy，通常以电子伏特（eV）为单位。\nk：这是Boltzmann Constant，数值约为 \\(1.38\\times 10^{-23}J/K\\)，用于将温度与能量联系起来。\nT：这是Thermodynamic temperature，以开尔文（K）为单位，是热力学温度是根据热力学原理来衡量系统绝对温度的一个度量，其零点对应理论上的绝对零度，即系统的分子运动几乎完全停止、能量达到最低的状态。\n从本质上讲，原子在它们的晶格位点上拼命振动，试图跳出它们的位点。 它们真的很努力。 我的意思是说，每秒大约有 1013 次！ 在固态中，由于结合能强于热能，大多数情况下它们都不会成功（实际上，原子形成有序固体有很大的节能作用，但我们稍后会详细介绍）。 但偶尔也会有原子从其晶格位置成功跃迁，并移动到晶格的其他位置。 这就会留下一个缺失的原子或空位。 因此，我们可以将空位的形成看作是将原子固定在晶格位点上的结合能与将原子从晶格位点上挤出的热能之间的持续斗争\nThermodynamics 热力学 #\rThermodynamics说明了在一个物体中，能量是分布在Atoms上而不是对于所有Atom都具有相同能量的 因此，单个原子可能有足够的能量跳出其晶格位置，而其余大多数原子则没有，这是有道理的 如图 10 所示，随着温度的升高，我们发现有更多的原子进入了高能态。 同时，随着温度的降低，高能态原子的数量也在减少。 Boltzmann Distribution #\r在绝对零度（0 K）：所有原子都会处于最低能量状态，因为此时系统没有足够的能量让原子占据更高的能量状态。 在无限温度（∞ K）：系统的温度极高，粒子的能量足以占据任何能量状态。因此所有能量状态的粒子数都会趋于相等，也就是所有能量状态均等分布 在Boltzmann Distribution中，不会出现所有粒子只占据最高能量状态的情况，即使是在极高温度下 One-Dimensional Imperfections or Dislocations #\rCold Work #\r当我们对金属进行Plastic Deformation时，会产生新的Dislocation。 这增加了Dislocation Density，这将在未来中增加Dislocation的难度 当我们观察金属的Stress-Strain Curve，发现应力在Yield Strength之后继续增加时，我们首次观察到金属通过Plastic Deformation而得到强化。 事实上，如果我们Unload一个sample并重新load，其在stress水平达到我们在前一个循环中留下的Stress之前不会开始Plastic Deformation 这是一种通过塑性变形进行的强化，不过在工业上，我们通常是通过轧制或拉动金属零件，或将金属零件压入模具来实现塑性变形，而不是通过简单的拉伸来拉动零件 Hot Work #\r材料被加热到再结晶温度以上，这意味着减少了Dislocation Density Two-Dimensional Imperfections #\rFree Surfaces #\r自由表面（Free Surfaces） 是指材料的外部表面, 这些表面与外界环境直接接触 原子排列不规则：在材料的自由表面，原子周围的配位数（与其他原子结合的数量）比材料内部的原子要少 Grain Boundaries #\r是指多晶材料中不同Crystal Structure交界的地方 当Dislocation在Crystal中移动，其必定要穿过Grain Boundary，然而这对位错来说是一个挑战 如果我们减小金属的晶粒尺寸，就会产生更多的晶界和更多的位错运动障碍，从而有望提高金属的强度 This strengthening mechanism has a pretty self-explanatory name: grain size reduction. Three-Dimensional Imperfections or Second Phase Particles #\rThree-dimensional imperfections occur any time we have a second phase within a solid 从本质上讲，如果固体中存在晶体结构不同的区域，就会出现第二相或三维缺陷 ","date":"Oct 16 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms5.furtheronstressstrain/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 10/16/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eStress-Strain Curve \r\n    \u003cdiv id=\"stress-strain-curve\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#stress-strain-curve\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS5.FurtherOnStressStrain/MCMS5.FurtherOnStressStrain.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e当回头重新看Stress-Strain Curve的时候可以发现一些特殊的点\u003c/li\u003e\n\u003cli\u003e如Proportion Limit，Ultimate Tensile Strength两个点\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eProportional Limit \r\n    \u003cdiv id=\"proportional-limit\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#proportional-limit\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e在曲线的最初阶段，存在一个接近于Linear的区域，其代表了Linear Elastic的Region\u003c/li\u003e\n\u003cli\u003e而也必会存在一个点象征着Linear Elastic Region的结束\u003c/li\u003e\n\u003cli\u003e但犹豫一些测量的误差或者精度上的问题，将导致最终的这一个Linear Elastic结束的点无法被确定，所以需要一个约定俗成的方法\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003e0.2% Offset Yield Strength \r\n    \u003cdiv id=\"02-offset-yield-strength\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#02-offset-yield-strength\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS5.FurtherOnStressStrain/MCMS5.FurtherOnStressStrain-1.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"MCMS 5. Further On Stress Strain","type":"docs"},{"content":"\r元数据 #\r[!abstract] 三体（全集）\n书名： 三体（全集） 作者： 刘慈欣 简介： 每个人的书架上都该有套《三体》！关于宇宙最狂野的想象！就是它！征服世界的中国科幻神作！包揽九项世界顶级科幻大奖！出版16个语种，横扫30国读者！奥巴马、雷军、马化腾、周鸿祎、潘石屹、扎克伯格……强推！刘慈欣获得2018年度克拉克想象力贡献社会奖！刘慈欣是中国科幻小说的最主要代表作家，亚洲首位世界科幻大奖“雨果奖”得主，被誉为中国科幻的领军人物。 出版时间： 2018-12-01 00:00:00 ISBN： 9785214589626 分类： 精品小说-科幻小说 出版社： 海南省电子音像出版社 PC地址：https://weread.qq.com/web/reader/ce032b305a9bc1ce0b0dd2a 高亮划线 #\r第十三章 红岸之二 #\r📌 雷政委说完站起来，迈着军人的稳健步伐离去。叶文洁的双眼盈满了泪水，透过眼泪，屏幕上的代码变成了一团团跳动的火焰。自父亲死后，这是她第一次流泪。 叶文洁开始熟悉监听部的工作，她很快发现，自己在这里远不如在发射部顺利，她已有的计算机知识早已落后，大部分软件技术都得从头学起。虽然有雷政委的信任，但对她的限制还是很严的，她可以看程序源代码，但不许接触数据库。 在日常工作中，叶文洁更多是接受杨卫宁的领导，他对她更加粗暴了，动不动就发火。雷政委多次劝他也没用，好像一见到叶文洁，他就充满了一种无名的焦虑。 ⏱ 2024-08-30 09:52:10 ^695233-17-5851\n第十六章 三体、哥白尼、宇宙橄榄球、三日凌空 #\r📌 很好，哥白尼，很好，你这种现实的、符合实验科学思想的想法是大多数学者不具备的，就凭这一点，你的理论也值得听一听。” 教皇对汪淼点点头，“说说看吧。” 汪淼走到长桌的另一端，让自己镇定了一下，说：“其实很简单：太阳的运行之所以没有规律，是因为我们的世界中有三颗太阳，它们在相互引力的作用下，做着无法预测的三体运动。当我们的行星围绕着其中的一颗太阳做稳定运行时，就是恒纪元；当另外一颗或两颗太阳运行到一定距离内，其引力会将行星从它围绕的太阳边夺走，使其在三颗太阳的引力范围内游移不定时，就是乱纪元；一段不确定的时间后，我们的行星再次被某一颗太阳捕获，暂时建立稳定的轨道，恒纪元就又开始了。这是一场宇宙橄榄球赛，运动员是三颗太阳，我们的世界就是球！” ⏱ 2024-08-30 10:05:15 ^695233-20-2574\n第十七章 三体问题 #\r📌 这时，我就像一个半生寻花问柳的放荡者突然感受到了爱情。 “你不知道庞加莱[插图]吗？”汪淼打断魏成问。 当时不知道，学数学的不知道庞加莱是不对，但我不敬仰大师，自己也不想成大师，所以不知道。但就算当时知道庞加莱，我也会继续对三体问题的研究。全世界都认为这人证明了三体问题不可解，可我觉得可能是个误解，他只是证明了初始条件的敏感性，证明了三体系统是一个不可积分的系统，但敏感性不等于彻底的不确定，只是这种确定性包含着数量更加巨大的不同形态。现在要做的是找到一种新的算法。当时我立刻想到了一样东西：你听说过“蒙特卡洛法”吗？哦，那是一种计算不规则图形面积的计算机程序算法，具体做法是在软件中用大量的小球随机击打那块不规则图形，被击中的地方不再重复打击，这样，达到一定的数量后，图形的所有部分就会都被击中一次，这时统计图形区域内小球的数量，就得到了图形的面积，当然，球越小结果越精确。 ⏱ 2024-08-30 10:16:26 ^695233-21-4188\n📌 这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想。这就是我解决三体问题的策略。我研究三体运动的任何一个时间断面，在这个断面上，各个球的运动矢量有无限的组合，我将每一种组合看做一种类似于生物的东西，关键是要确定一个规则：哪种组合的运行趋势是“健康的”和“有利的”，哪种是“不利的”和“有害的”，让前者获得生存的优势，后者则产生生存困难，在计算中就这样优胜劣汰，最后生存下来的就是对三体下一断面运动状态的正确预测。 ⏱ 2024-08-30 10:16:27 ^695233-21-4775\n第三十二章 古筝行动 #\r📌 叶文洁：如果他们能够跨越星际来到我们的世界，说明他们的科学已经发展到相当的高度，一个科学如此昌明的社会，必然拥有更高的文明和道德水准。 审问者：你认为这个结论，本身科学吗？ 叶文洁：…… ⏱ 2024-09-02 13:19:12 ^695233-36-12335-12543\n危机纪年第20年，三体舰队距太阳系4.15光年 #\r📌 人类大脑的进化需要两万至二十万年才能实现明显的改变，而人类文明只有五千年历史，所以我们目前拥有的仍然是原始人的大脑 ⏱ 2024-10-12 11:38:59 ^695233-47-9290-9347\n📌 政治思想工作是通过科学的理性思维来建立信念。 ⏱ 2024-10-12 11:39:43 ^695233-47-10801-10823\n下部 黑暗森林 #\r📌 普通人的目光，是他们所在地区和时代的文明程度的最好反映。他曾经看到过一组由欧洲摄影师拍摄的清朝末年的照片，最深的印象就是照片上的人呆滞的目光，在那些照片上，不论是官员还是百姓，眼睛中所透出的只有麻木和愚钝，看不到一点生气。 ⏱ 2024-10-12 12:13:19 ^695233-48-2540-2651\n📌 给岁月以文明，而不是给文明以岁月。 ⏱ 2024-10-12 13:19:45 ^695233-48-32277-32294\n📌 章北海停下手中的笔，抬头看着舱外的东方延绪，他的目光平静如水，“同为军人，知道我们之间最大的区别在哪里吗？你们按照可能的结果来决定自己的行动；而我们，不管结果如何，必须尽责任，这是唯一的机会，所以我就做了。 ⏱ 2024-10-12 13:59:12 ^695233-48-84364-84467\n📌 两个多世纪前，阿瑟·克拉克在他的科幻小说《2001：太空奥德赛》中描述了一个外星超级文明留在月球上的黑色方碑，考察者用普通尺子量方碑的三道边，其长度比例是1∶3∶9，以后，不管用何种更精确的方式测量，穷尽了地球上测量技术的最高精度，方碑三边的比例仍是精确的1∶3∶9，没有任何误差。克拉克写道：那个文明以这种方式，狂妄地显示了自己的力量。 ⏱ 2024-10-12 14:10:45 ^695233-48-100308-100477\n危机纪年第208年，三体舰队距太阳系2.07光年 #\r📌 因为在昨天晚上的演讲中，你说人类迟迟未能看清宇宙的黑暗森林状态，并不是由于文明进化不成熟而缺少宇宙意识，而是因为人类有爱。 “这不对吗？” 对，虽然“爱”这个词用在科学论述中涵义有些模糊，但你后面的一句话就不对了，你说很可能人类是宇宙中唯一拥有爱的种族，正是这个想法，支撑着你走完了自己面壁者使命中最艰难的一段。 “当然，这只是一种表达方式，一种不严格的……比喻而已。” 至少我知道三体世界也是有爱的，但因其不利于文明的整体生存而被抑制在萌芽状态，但这种萌芽的生命力很顽强，会在某些个体身上成长起来。 “请问您是……” 我们以前不认识，我是两个半世纪前曾向地球发出警告的监听员。 “天啊，您还活着？”庄颜惊叫道。 也活不了多长时间了，我一直处于脱水状态，但这么长的岁月，脱水的机体也会老化。不过我真的看到了自己想看的未来，我感到很幸福。 “请接受我们的敬意。”罗辑说。 我只是想和您讨论一种可能：也许爱的萌芽在宇宙的其他地方也存在，我们应该鼓励她的萌发和成长。 “为此我们可以冒险。” 对，可以冒险。 “我有一个梦，也许有一天，灿烂的阳光能照进黑暗森林。” 这时，这里的太阳却在落下去，现在只在远山露出顶端的一点，像山顶上镶嵌着的一块光灿灿的宝石。孩子已经跑远，同草地一起沐浴在金色的晚霞之中。 太阳快落下去了，你们的孩子居然不害怕？ “当然不害怕，她知道明天太阳还会升起来的。” ⏱ 2024-10-12 15:08:48 ^695233-49-13253-14291\n危机纪元4年，云天明 #\r📌 程心似乎听到了他心中的话，她慢慢抬起头来，他们的目光第一次这么近地相遇，比他梦中的还近，她那双因泪水而格外晶莹的美丽眼睛让他心碎。 但接着，程心说出一句完全意外的话：“天明，知道吗？安乐死法是为你通过的。” ⏱ 2024-10-14 12:31:36 ^695233-55-21409-21540\n危机纪元1-4年，程心 #\r📌 “你会把你妈卖给妓院吗？”维德问。 ⏱ 2024-10-14 12:31:06 ^695233-56-2751-2768\n威慑纪元61年，执剑人 #\r📌 因为杀的人太少了。杀一个人是要被判死刑的，杀几个几十个更是如此，如果杀了几千几万人，那就罪该万死；但如果再多些，杀了几十万人呢？当然也该判死刑，但对于有些历史知识的人，这个回答就不是太确定了；再进一步，如果杀了几百万人呢？那可以肯定这人不会被判死刑，甚至不会受到法律的惩处，不信看看历史就知道了，那些杀人超过百万的人，好像都被称为伟人和英雄；更进一步，如果这人毁灭了一个世界，杀死了其中的所有生命，那他就成了救世主！ ⏱ 2024-10-15 01:16:23 ^695233-60-4506-4714\n📌 “很复杂，直接原因是：那个恒星系，就是他向宇宙广播了坐标导致其被摧毁的那个，不知道其中有没有生命，但肯定存在有的可能，所以他被指控有世界灭绝罪的嫌疑。这是现代法律中最重的罪了。” ⏱ 2024-10-16 14:17:58 ^695233-60-4831-4920\n📌 如果说面壁计划是人类历史上首次出现的怪物，那黑暗森林威慑和执剑人在历史上却有过先例。公元20世纪华约和北约两大军事集团的冷战就是一个准终极威慑。冷战中的1974年，苏联启动Perimeter计划，建立了一个后来被称为末日系统的预警系统，其目的是在北约核突袭中，当政府决策层和军队高级指挥层均被消灭、国家已失去大脑的情况下，仍具备启动核反击的能力。它利用核爆监测系统监控苏联境内的核爆迹象，所有的数据会汇整到中央计算机，经过逻辑判读决定是否要启动核反击。这个系统的核心是一个绝密的位于地层深处的控制室，当系统做出反击的判断时，将由控制室内的一名值班人员启动核反击。公元2009年，一位曾参加过Perimeter战略值班的军官对记者披露，他当时竟然只是一名刚从伏龙芝军事学院毕业的二十五岁的少尉！当系统做出反击判断时，他是毁灭的最后一道屏障。这时，苏联全境和东欧已在火海之中，他在地面的亲人和朋友都已经死亡，如果他按下启动反击的按钮，北美大陆在半个小时后也将同样成为生命的地狱，随之而来的覆盖全球的辐射尘和核冬天将是整个人类的末日。那一时刻，人类文明的命运就掌握在他手中。后来，人们问他最多的话就是：如果那一时刻真的到来，你会按下按钮吗？ 这位历史上最早的执剑人说：我不知道。 ⏱ 2024-10-16 14:26:20 ^695233-60-9487-10051\n📌 人们发现威慑纪元是一个很奇怪的时代，一方面，人类社会达到空前的文明程度，民主和人权得到前所未有的尊重；另一方面，整个社会却笼罩在一个独裁者的阴影下。有学者认为，科学技术一度是消灭极权的力量之一，但当威胁文明生存的危机出现时，科技却可能成为催生新极权的土壤。在传统的极权中，独裁者只能通过其他人来实现统治，这就面临着低效率和无数的不确定因素，所以，在人类历史上，百分之百的独裁体制从来没有出现过。但技术却为这种超级独裁的实现提供了可能，面壁者和持剑者都是令人忧虑的例子。超级技术和超级危机结合，有可能使人类社会退回黑暗时代。 ⏱ 2024-10-16 14:28:31 ^695233-60-10907-11168\n📌 “看，她是圣母玛丽亚，她真的是！”年轻母亲对人群喊道，然后转向程心，热泪盈眶地双手合十，“美丽善良的圣母，保护这个世界吧，不要让那些野蛮的嗜血的男人毁掉这美好的一切。” ⏱ 2024-10-16 23:16:52 ^695233-60-20469-20553\n威慑纪元62年，奥尔特星云外，“万有引力”号 #\r📌 “三维，在弦理论中，不算时间维，宇宙有十个维度，可只有三个维度释放到宏观，形成我们的世界，其余的都卷曲在微观中。” ⏱ 2024-10-16 23:24:46 ^695233-61-13721-13778\n威慑纪元最后十分钟，62年11月28日16：17：34至16：27：58，威慑控制中心 #\r📌 在程心的潜意识中，她是一个守护者，不是毁灭者；她是一个女人，不是战士。她将用自己的一生守护两个世界的平衡，让来自三体的科技使地球越来越强大，让来自地球的文化使三体越来越文明，直到有一天，一个声音对她说：放下红色开关，到地面上来吧，世界不再需要黑暗森林威慑，不再需要执剑人了。 ⏱ 2024-10-17 01:26:30 ^695233-63-1797-1934\n威慑后一小时，失落的世界 #\r📌 “这都是为什么？”程心喃喃地问，更像是问自己。 “因为宇宙不是童话。” ⏱ 2024-10-17 01:30:38 ^695233-64-4901-4964\n第三部 #\r📌 安逸的美梦彻底破灭，黑暗森林理论得到了最后的证实，三体世界被摧毁了。 ⏱ 2024-10-17 15:58:32 ^695233-70-5401-5435\n广播纪元7年，云天明的童话 #\r📌 AA拿过程心叠好的带篷的小纸船，称赞很漂亮，然后示意程心也进浴室。在盥洗台上，她用小刀片从香皂上切下了小小的一片，然后把小纸船的尾部扎了一个小孔，把那一小片香皂插入小孔中，抬头对程心神秘地一笑，轻轻地把纸船放进已灌满水并且水面已经平静下来的浴缸中。 小船向前移动了，在这片小小的水面上，从此岸航向彼岸。 程心立刻明白了原理：香皂在水中溶解后，降低了小船后方水面的张力，但船前方水面的张力不变，小船就被前方水面的张力拉过去了￼。但这个想法转瞬即逝，程心的思想随即被一道闪电照亮！在她的眼中，浴缸中平静的水面变成了漆黑的太空，白色的小纸船在这无际的虚空中以光速航行… ⏱ 2024-10-17 22:30:43 ^695233-73-48751-49192\n📌 每秒十六点七千米，太阳系的第三宇宙速度，如果达不到这个速度就不可能飞出太阳系。 光也一样。 如果太阳系的真空光速降到每秒十六点七千米以下，光将无法逃脱太阳的引力，太阳系将变成一个黑洞￼。 ⏱ 2024-10-17 22:40:01 ^695233-73-63863-64231\n广播纪元8年，命运的抉择 #\r📌 这让我想起了那天夜里峨眉山的云海，”瓦西里说，“那是中国的一座山，在那山的顶上看月亮是最美的景致。那天夜里，山下全是云海，望不到边，被上空的满月照着，一片银色，很像现在看到的样子。” ⏱ 2024-10-17 22:55:27 ^695233-74-19329-19420\n📌 “其实吧，从科学角度讲，毁灭一词并不准确，没有真正毁掉什么，更没有灭掉什么，物质总量一点不少都还在，角动量也还在，只是物质的组合方式变了变，像一副扑克牌，仅仅重洗而已……可生命是一手同花顺，一洗什么都没了。” ⏱ 2024-10-17 22:55:16 ^695233-74-19474-19578\n第五部 #\r📌 剩下的事就是清理了，歌者再次从仓库中取出那个质量点。他突然想到清理弹星者是不能用质量点的，这个星系的结构与前面已死的那个星系不同，有死角，用质量点可能清理不干净，甚至白费力气，这要用二向箔才行。可是歌者没有从仓库里取二向箔的权限，要向长老申请。 ⏱ 2024-10-18 13:45:16 ^695233-79-5867-5989\n掩体纪元66年，太阳系外围 #\r📌 白Ice笑了起来，“再简单不过的事，你忘记《古兰经》中的故事了？如果大山不会走向穆罕默德，穆罕默德可以走向大山。” ⏱ 2024-10-18 14:26:27 ^695233-81-7378-7435\n📌 丁仪接着说：“在危机初期，当智子首次扰乱加速器时，有几个人自杀。我当时觉得他们不可理喻，对于搞理论的，看到那样的实验数据应该兴奋才对。但现在我明白了，这些人知道的比我多，比如杨冬，她知道的肯定比我多，想得也比我远，她可能知道一些我们现在都不知道的事。难道制造假象的只有智子？难道假象只存在于加速器末端？难道宇宙的其他部分都像处女一样纯真，等着我们去探索？可惜，她把她知道的都带走了。” ⏱ 2024-10-18 14:45:18 ^695233-81-10117-10309\n📌 “我说别傲慢，弱小和无知不是生存的障碍，傲慢才是，想想水滴吧！” ⏱ 2024-10-18 15:50:30 ^695233-81-12293-12325\n📌 “现在逃离，就像在瀑布顶端附近的河面上划船，除非超过一个逃逸速度，否则不论怎样划，迟早都会坠入瀑布，就像在地面向上扔石头，不管扔多高总会落回来。整个太阳系都在跌落区，从中逃离必须达到逃逸速度。” “逃逸速度是多少？” “我反复计算过四遍，应该没错。” “逃逸速度是多少？！” “启示”号和“阿拉斯加”号上的人们屏息凝神，替全人类倾听末日判决，白Ice把这判决平静地说出来： “光速。” ⏱ 2024-10-18 15:55:33 ^695233-81-15419-15751\n掩体纪元67年，二维太阳系 #\r📌 程心现在回想起两次看到《星空》时奇怪的感觉：画面中星空之外的部分，那火焰般的树，暗夜中的村庄和山脉，都呈现出明显的透视和纵深；但上方的星空却丝毫没有立体感，像挂在夜空中的一幅巨画。 因为星空是二维的。 他是怎么画出来的？1889年的凡·高，精神第二次崩溃的凡·高，难道真的用分裂和谵妄的意识，跨越五个多世纪的时空，看到了现在？！或者反过来，他早就看到了未来，这最后审判日的景象才是他精神崩溃和自杀的真正原因？！ ⏱ 2024-10-18 16:17:09 ^695233-83-16582-16843\n📌 隧洞前的罗辑笑了笑，“我要是想走，刚才就跟你们走了，我这样岁数的人，不适合远航了。孩子们，不要为我操心了，我说过的，我什么都没有失去。准备启动空间曲率驱动。” 罗辑的最后一句话是对飞船A.I.说的。 “航线参数？”A.I.问。 “目前航线的延长线吧，我也不知道你们要去哪儿，我想现在你们自己也不知道，要是想起了目的地，在星图上指出来就行了，半径五万光年内的大部分恒星，飞船都可以自动导航到达。” “指令执行中，空间曲率驱动引擎三十秒后启动。”A.I.说。 ⏱ 2024-10-18 16:19:44 ^695233-83-19371-19739\n📌 在宇宙中，曲率驱动航迹既可以成为危险标志，也能成为安全声明。如果航迹在一个世界旁边，是前者；如果把这个世界包裹在其中，则是后者。就像一个手拿绞索的人，他是危险的；但如果他把绞索套到自己的脖子上，他就变成安全的了。 ⏱ 2024-10-18 16:25:06 ^695233-83-22164-22270\n📌 “你们有个约会！”AA说。 “是的，我们有个约会。”程心机械地回答，感情的激荡使她处于呆滞状态。 “那就去你们的星星！” “好的，去我们的星星。”程心激动地对AA说。然后她问飞船A.I.，“能够定位DX3906恒星吗，这是危机纪元初的编号？” “可以，这颗恒星现在的编号是S74390E2，请确认。” ⏱ 2024-10-18 16:27:08 ^695233-83-26179-26441\n第六部 #\r📌 大气成分：氧35%，氮63%，二氧化碳2%，还有微量惰性气体，可以呼吸，但大气压只有0.53个地球标准气压，出舱后不要剧烈活动。”飞船A.I.说。 “站在飞船附近的那个生物是什么？”AA问。 “正常人类。”A.I.简单地回答。 ⏱ 2024-10-18 16:29:13 ^695233-84-4555-4724\n📌 也有人叫它末日飞船。那些光速飞船没有目的地，只是把曲率引擎开到最大功率疯狂加速，无限接近光速，目的就是用相对论效应跨越时间，直达宇宙末日。据他们计算，十年内就可以跨越五百亿年，那他们现在已经到了，哦，当然是以他们的参照系。其实，并不需要有意识地做这事，比如在飞船加速到光速后，曲率引擎出现无法修复的故障，使飞船不能减速，你也可能在有生之年到达宇宙末日。” ⏱ 2024-10-18 16:43:21 ^695233-84-13148-13325\n读书笔记 #\r本书评论 #\r","date":"Oct 16 2024","externalUrl":null,"permalink":"/notes/thethreebodyproblem/","section":"Thoughts","summary":"\u003ch1 class=\"relative group\"\u003e元数据 \r\n    \u003cdiv id=\"%E5%85%83%E6%95%B0%E6%8D%AE\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%85%83%E6%95%B0%E6%8D%AE\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cblockquote\u003e\n\u003cp\u003e[!abstract] 三体（全集）\u003c/p\u003e","title":"The Three Body Problem","type":"notes"},{"content":"Record some thinking\n","date":"Oct 16 2024","externalUrl":null,"permalink":"/notes/","section":"Thoughts","summary":"\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"Thoughts","type":"notes"},{"content":" Last Edit: 11/26/24\nSet Notation 集合 #\rSet is a collection of objects, called elements of the set $$A={t|t\\in R}$$ 这就是一个矩阵，其中A为t，而t可以取任意Real Number 这就是集合的表示方法，用这种方法便可以表示向量，直线和平面 Union of Sets 并集 #\r对于两个集合，其Union是另一个Set，其元素包含了all elements of A and B $$A \\cup B = { x \\in X \\mid x \\in A \\text{ or } x \\in B }$$ Intersection of Sets 交集 #\r对于Intersection来说，这个Set包含了任意同时出现在A与B中的元素 $$A \\cap B = { x \\in X \\mid x \\in A \\text{ and } x \\in B }.$$ Vector 向量 #\r思考一个问题，对于一个坐标轴上的点\\((2,1)\\)和一个向量\\(\\vec v =[2,1]^T\\)，他们的区别 Point：点是空间中的一个位置，用坐标表示，如P = (x, y, z)表示三维空间中的一个具体位置。点没有方向和大小。 Vector：向量是一个有大小和方向的数学对象，通常表示两个点之间的位移。例如，从点 A到点B的向量可以表示为\\(\\mathbf{v} = \\overrightarrow{AB}\\) 亦可以说Point是绝对位置，而向量是一个点到另外一个的有方向的距离 Norm 模 #\r对于\\(\\vec{v} = \\begin{bmatrix} v_1 \\ v_2 \\ \\vdots \\ v_n \\end{bmatrix} \\text{ be in } \\mathbb{R}^n\\) 它的Norm，也就是向量长度即为 $$\\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}$$ Dot Product 点积 #\rDot Product，用来衡量两个Vector方向相似程度的一个Scalar 其值从-1到1（仅当两个Vector为Unit Vector的情况下），从方向相反，到同方向，在0的时候代表两个Vector Orthogonal 垂直 Definition #\r$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n = \\sum_{i=1}^n a_i b_i$$\n公式中的每一项\\(a_i b_i\\)表示两个向量在某个维度上的大小相乘 点积实际上是在将a和b的方向分量进行逐一对比，累加得到两个向量在整个空间上的“相似性”，每一个维度的相似性都将累加到最终的空间相似性上 想要表达这个相似性，还存在另一种方式，即通过Projection $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos \\theta$$ 其代表了b在a方向上Projection的长度再乘以a Angle Between Vectors #\r同理可以用Dot Product公式推导两个Vector之间的夹角 $$\\theta = \\cos^{-1} \\left( \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{AB} \\right) \\quad 0^\\circ \\leq \\theta \\leq 180^\\circ$$ Application #\rDot Product的一种用法便是Force Vector Directed Along a Line 对于一个在\\(\\vec u\\)方向上的Force F，有 $$\\vec F_u=\\vec F\\cdot \\vec u=(F\\times 1)\\cdot \\cos\\theta=F\\cdot \\cos\\theta$$ 而这一个\\(\\cos\\theta\\)则是 $$\\cos\\theta=\\frac{\\vec v\\cdot \\vec{w}}{|v|\\times|w|}$$ 当把其中一个Vector替换为Direction Vector时，便可以得到Axis-Force\u0026rsquo;s Direction Cosine $$\\cos\\theta = \\frac{{r_xi+r_yj+r_zk}}{\\sqrt{r_x^2+r_y^2+r_z^2}\\cdot 1}$$ 所以就有\\(F\\cdot \\cos \\theta=F\\cdot \\vec u\\)之后便可以得到Force在\\(\\vec u\\)方向上的力的Magnitude（注意Dot Product的结果是一个Scalar） 要想得到Force在\\(\\vec u\\)方向上的Cartesian Vector，需要再次乘以Direction Vector 最终公式 #\r$$\\vec F=F_u\\cdot \\vec u= (\\vec F\\cdot u\\cdot\\cos\\theta)\\cdot \\vec u=(F\\cdot\\frac{{r_xi+r_yj+r_zk}}{\\sqrt{r_x^2+r_y^2+r_z^2}\\cdot 1})\\cdot\\vec u$$\nLine 直线 #\r考虑一个问题Consider the two points P = (1, 2) and Q = (−1, 4)，find an equation for the line y = mx + b which passes through P and Q 通过Point-Point Formula可以算出 $$y_2-y_1=\\frac{y_2 - y_1}{x_2 - x_1}(x_2-x_1)$$ 解得 \\(y=−x+3\\) 现在考虑相似的问题 Suppose x and y satisfy the vector equation $$\\begin{bmatrix} x \\ y \\end{bmatrix} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix} + k \\begin{bmatrix} 1 \\ -1 \\end{bmatrix}, \\text{ where } k \\in \\mathbb{R}$$ 可以发现两个不同的形式表达了同一个Line，他们之所以等价是因为 \\([1,2​]^T\\)是直线上的一个固定点（在直线上）。 \\(k\\begin{bmatrix} 1 \\ -1 \\end{bmatrix}\\)是一个方向向量，表示直线的方向（斜率），而方程中的斜率为\\(m = \\frac{\\Delta y}{\\Delta x}\\)，用Direction Vector的y分量除以x分量就可以得到一样的结果 而对于Intersection来说，当k=-1的时候，有 $$\\begin{bmatrix} x \\ y \\end{bmatrix} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix} + \\begin{bmatrix} -1 \\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\ 3\\end{bmatrix}$$ 也就是截距 于是就可以定义如下的Line using Set Notation $$L = { \\vec{u} \\in \\mathbb{R}^2 \\mid \\vec{u} = \\vec{v} + t \\vec{d}, \\text{ for some } t \\in \\mathbb{R} }$$ 为什么需要两个向量 —\u0026gt; 因为需要Position Vector区分不同平行的直线\nPlane #\r同理对于一个Plane，可以通过两个Linearly Independent的Vector与一个Plane上的点表示 至于前面Line中所提到的Slope，在这里便是Partical Derivative $$\\mathcal{P} = { \\vec{u} \\in \\mathbb{R}^3 \\mid \\vec{u} = \\vec{p} + s \\vec{d}_1 + t \\vec{d}_2, \\text{ for some } s, t \\in \\mathbb{R} }$$ 可以发现u是一个\\(\\mathbb R^3\\)中的Subset，这代表了该Subset只含有两个Degree of Freedom，即为三维空间中的一个Plane 注意这里的Plane是一个Subset，不是Subspace，具体原因将在后面指出\nNormal Vector #\r对于平面\\(Ax+By+Cz=D\\)来说，其Normal Vecotr，也就是垂直于整个Plane的Vector即为\\(n=[A,B,C]^T\\) Proof #\r\\(P(x_1​,y_1​,z_1​)\\)和\\(Q(x_2, y_2, z_2)\\)都在平面上，那么它们的坐标满足平面方程 $$Ax_1+By_1+Cz_1=D，Ax_2+By_2+Cz_2=D$$ 则可以构建向量\\(\\vec{v} = \\begin{bmatrix} x_2 - x_1 \\ y_2 - y_1 \\ z_2 - z_1 \\end{bmatrix}\\) 结合方程组则有\\(A(x_2-x_1)+B(y_2-y_1)+C(z_2-z_1)=0\\) 也就是\\(\\vec v \\cdot \\vec n =0\\)，其中\\(\\vec{n} = \\begin{bmatrix} A \\ B \\ C \\end{bmatrix}\\) Second definition #\r结合上面的思想便可以得到Space的第二个Set Notation表达式 $$\\mathcal{P} = { \\vec{u} \\in \\mathbb{R}^3 \\mid \\vec{n} \\cdot (\\vec{u} - \\vec{p}) = 0 }$$ \\(u∈R^3\\)：表示平面上的任意点的坐标。 \\(\\vec{n} \\in \\mathbb{R}^3\\)：表示平面的法向量，即垂直于平面表面的向量。 \\(\\vec{p} \\in \\mathbb{R}^3\\)：表示平面上的一个固定点，用于确定平面的具体位置。 点积\\(\\vec{n} \\cdot (\\vec{u} - \\vec{p}) = 0\\)：表示向量\\((\\vec{u} - \\vec{p})\\)与法向量\\(\\vec{n}\\)正交。 \\(\\vec p\\)为Plane上一点，\\(\\vec n\\)为Plane的Normal Vector，剩下的就是点上的任意位置了，由于任意\\(\\vec u\\)都在平面中，\\(\\vec u-\\vec p\\) 将仍处于Plane中，而\\(\\vec n⋅(\\vec u−\\vec p​)=0\\)是限定他的条件 所有的Vector都为Position Vector，即以坐标系Origin为Head的Vector\nex. #\rFind the equation for a plane \\(\\mathcal{P}\\) which contains the three points P = (0, 1, 1), Q = (2, 0, 4), andR = (0, 0, 1)\nExpress \\(\\mathcal{P}\\) in vector form, in normal form, and as an equation ax + by + cz = d\n用三个点确定Plane上的两个Vector\n$$\\vec{v}_1 = \\begin{bmatrix} 2 - 0 \\\\ 0 - 1 \\\\ 4 - 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}$$\r$$\\vec{v}_2 = \\begin{bmatrix} 0 - 0 \\\\ 0 - 1 \\\\ 1 - 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix}$$\r","date":"Oct 15 2024","externalUrl":null,"permalink":"/docs/linearalgebra/la1.vectorlineplane/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/26/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eSet Notation 集合 \r\n    \u003cdiv id=\"set-notation-%E9%9B%86%E5%90%88\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#set-notation-%E9%9B%86%E5%90%88\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003eSet is a collection of objects, called elements of the set\n$$A={t|t\\in R}$$\u003c/li\u003e\n\u003cli\u003e这就是一个矩阵，其中A为t，而t可以取任意Real Number\u003c/li\u003e\n\u003cli\u003e这就是集合的表示方法，用这种方法便可以表示向量，直线和平面\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eUnion of Sets 并集 \r\n    \u003cdiv id=\"union-of-sets-%E5%B9%B6%E9%9B%86\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#union-of-sets-%E5%B9%B6%E9%9B%86\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e对于两个集合，其Union是另一个Set，其元素包含了all elements of A and B\n$$A \\cup B = { x \\in X \\mid x \\in A \\text{ or } x \\in B }$$\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eIntersection of Sets 交集 \r\n    \u003cdiv id=\"intersection-of-sets-%E4%BA%A4%E9%9B%86\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#intersection-of-sets-%E4%BA%A4%E9%9B%86\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e对于Intersection来说，这个Set包含了任意同时出现在A与B中的元素\n$$A \\cap B = { x \\in X \\mid x \\in A \\text{ and } x \\in B }.$$\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eVector 向量 \r\n    \u003cdiv id=\"vector-%E5%90%91%E9%87%8F\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#vector-%E5%90%91%E9%87%8F\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e思考一个问题，对于一个坐标轴上的点\\((2,1)\\)和一个向量\\(\\vec v =[2,1]^T\\)，他们的区别\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint\u003c/strong\u003e：点是空间中的一个位置，用坐标表示，如P = (x, y, z)表示三维空间中的一个具体位置。点没有方向和大小。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVector\u003c/strong\u003e：向量是一个有大小和方向的数学对象，通常表示两个点之间的位移。例如，从点 A到点B的向量可以表示为\\(\\mathbf{v} = \\overrightarrow{AB}\\)\u003c/li\u003e\n\u003cli\u003e亦可以说\u003cstrong\u003ePoint是绝对位置，而向量是一个点到另外一个的有方向的距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eNorm 模 \r\n    \u003cdiv id=\"norm-%E6%A8%A1\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#norm-%E6%A8%A1\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e对于\\(\\vec{v} = \\begin{bmatrix} v_1 \\ v_2 \\ \\vdots \\ v_n \\end{bmatrix} \\text{ be in } \\mathbb{R}^n\\)\u003c/li\u003e\n\u003cli\u003e它的Norm，也就是向量长度即为\n$$\\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}$$\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eDot Product 点积 \r\n    \u003cdiv id=\"dot-product-%E7%82%B9%E7%A7%AF\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#dot-product-%E7%82%B9%E7%A7%AF\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003eDot Product，用来衡量两个Vector方向相似程度的一个Scalar\u003c/li\u003e\n\u003cli\u003e其值从-1到1（仅当两个Vector为Unit Vector的情况下），从方向相反，到同方向，在0的时候代表两个Vector Orthogonal 垂直\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eDefinition \r\n    \u003cdiv id=\"definition\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#definition\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003e$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n = \\sum_{i=1}^n a_i b_i$$\u003c/p\u003e","title":"LA 1. Vector Line \u0026 Plane","type":"docs"},{"content":"Record some thinking\n","date":"Oct 14 2024","externalUrl":null,"permalink":"/blogs/","section":"Blogs","summary":"\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"Blogs","type":"blogs"},{"content":"","date":"Oct 14 2024","externalUrl":null,"permalink":"/tags/ontario/","section":"Tags","summary":"","title":"Ontario","type":"tags"},{"content":"\r","date":"Oct 14 2024","externalUrl":null,"permalink":"/blogs/ontariolake/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/OntarioLake/Ontario%20Lake-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/OntarioLake/Ontario%20Lake-2.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Ontario Lake","type":"blogs"},{"content":"","date":"Oct 14 2024","externalUrl":null,"permalink":"/tags/pic/","section":"Tags","summary":"","title":"Pic","type":"tags"},{"content":" Last Edit: 9/25/24\nYoung’s modulus change with Density #\rOrdered Solids #\r大多数固体材料都是Polycrystaline 多晶体的 尤其是Metal在原子尺度上是按照Crystal Structure排列的 Atomic scale尺度一般在\\(10^{-10}\\)的级别 Unit Cell #\r最小的Convenient Building Block Grains #\r多晶材料的这些晶体被称为晶粒（grains）。每个晶粒内部的原子排列是有序的，但不同晶粒之间的原子排列方向各不相同，这种微观结构对材料的机械性能和物理性质有重要影响 Short Range Order #\r指的是在局部区域内，原子或分子的排列是有规律的 玻璃、液体这样的材料，它们通常是Short Range Order的 Long Range Order #\r在整个材料中，原子或分子的排列是有规律的，并且这种规律性会一直延续到较大的尺度（远远超过单个原子的范围） 晶体材料，如金属和矿物，通常具有长程有序 Simple Cubic #\r简单的一个立方体上的八个角为Atoms的结构 其正方体的边长被定义为a n：1 Atom CN：6 Side Dimension: \\(a=2r\\) Hard Sphere Model #\r所有的Atoms都被当作一个球来模拟，主要是方便描述原子在晶体结构中的排列方式 Reduced Sphere Model #\rReal Graph #\r可以发现这样的Hard Shpere Model看着实在是Messy 所以就将所有Atoms简化为Reduced Sphere Model The Atomic Packing Factor 填充系数 #\r对于一个Unit Cell，APF描述了Atom对于体积的占有率，从100%完全占据到0%一点没有 $$APF=\\frac{Volume_{Spheres}}{Volumn_{Unit~Cell}}$$ 本质上就是一个百分比或者说分数 现在分别计算两个体积 对于Spheres来说，单个的体积为\\(\\frac{4}{3}\\pi R^3\\)，而将他乘上Number of Atoms后便可以得到所有的体积 对于Unit Cell来说，以a为边长，其体积自然为\\(a^3\\) Face Centred Cubic (FCC) Structure #\r结构特点：每个晶胞的八个顶点各有一个原子，此外每个面中心还有一个原子。 n：每个晶胞含有 4 个原子（8 个顶点原子各占 1/8，6 个面心原子各占 1/2）。 CN：12（每个原子有 12 个最近邻原子）。 APF：约 74%（原子占据的体积比例）。 例子：铝、铜、金、银等 在Simple Cubic的基础上加入一些Face Centred Atoms（在每个面中心的Atom） Number of Atoms in FCC #\r对于FCC来说，每一个角上都是\\(\\frac{1}{8}\\)个Atom 每一个面都有\\(\\frac{1}{2}\\)个Atom 所以一共就是\\(\\frac{1}{8}*8+\\frac{1}{2}*6=4\\)个Atom 即\\(n_{FCC}=4\\) Coordination Number of FCC #\r$$CN_{FCC}=12$$\nAtomic Packing Factor of FCC #\r则对于FCC Structure来说，其APF即为 $$APF=\\frac{4\\frac{4}{3}\\pi R^3}{a^3}$$ 现在要将Atom的radius与Unit Cell的边长a做替换好消掉其中一个 有\\(a^2+a^2=(4R)^2\\)，勾股定律 则有\\(2a^2=16R^2\\)，即\\(a_{FCC}=2\\sqrt{2}R\\) 将a带入后便有 $$APF = \\frac{4 \\left( \\frac{4}{3} \\pi R^3 \\right)}{(2\\sqrt{2}R)^3} \\ \\Rightarrow APF_{FCC} = 0.74$$ Avogadro\u0026rsquo;s constant 阿伏伽德罗常数 #\r通常用符号\\(N_A\\)表示，是指在1摩尔物质中包含的微观粒子（如原子、分子、离子等）的数量。其数值大约为： $$N_A=6.022×10^{23 }g\\cdot mol^{−1}$$\nTheoretical Density 理论密度 #\r$$\\text{Mass}{\\text{Atoms in Unit Cell}} = \\text{Number}{\\text{Atoms in Unit Cell}} \\cdot \\frac{\\text{Molar Mass of Atom}}{\\text{Avogadro\u0026rsquo;s Number}}$$\n$$m = n \\cdot \\frac{A}{N_A}$$ $$\\rho = \\frac{nA}{V_C N_A}$$\nDensity密度，即质量和体积的比值 \\(n\\)：Number of Atoms，一个Crystal里的完整分子个数 A: Molar Mass(g/mol)，原子相对质量 \\(V_c\\)：Unit Cell的体积 \\(N_A\\)：阿伏伽德罗常数\\(mol^{-1}\\) \\(\\frac{A}{N_A}\\)：\\(\\frac{g/mol}{mol^{-1}}=g\\)，等于一个Atom有几g，再乘上Atom Nuber得到全部的Mass Rock Salt Structure #\r是一个Common Ceramic的Crystal Structure 结构特点：这是离子晶体结构，通常由两种不同大小的离子（如Na⁺和Cl⁻）组成。大的离子（Cl⁻）形成一个面心立方（FCC）结构，小的离子（Na⁺）填充在八面体间隙中。 A：每个晶胞含有 4 个阳离子和阴离子。 CN：阳离子和阴离子的CN都为6。 APF：约 67%。 例子：氯化钠（NaCl）、氧化镁（MgO）等。 对于Ceramic Structure来说，其拥有多余一种的Atom类型，具体来说有Cation和Anion两种 其是Ionic Compound的一种特有的Crystal Structure 对于Rosk Salt Structure来说，其包含了两种Ion，即Anion（蓝色）与Cation（红色） Anions在Unit Cell的角上（Cation也可以在，他们描述的将是同一种结构，但一般来说体积大的Anion会先占据Unit Cell的角落，将小的Cation挤到中间去） 这个结构看起来像面心立方（FCC），但实际上不是纯粹的FCC，因为阳离子和阴离子之间有相互作用，会推开阴离子，使得阴离子不会像真正的FCC结构那样直接通过面对角线相互接触。 Number of Atoms in Rock Salt (Stoichiometry) #\r对于如NaCl这样的material，其Anion：Cation比都是1：1的，但上图中明显缺少了一个Cation 其正确的位置应该是整个Unit Cell的正中央 Coordination Number for Cations in Rock Salt #\r对于一个Rock Salt来说，拿最中间的Cation举例，可以发现与其接触的Atom有6个 于是就可以说\\(CationCoordinationNumber_{Rock~Salt} = 6\\) Density of Rock Salt #\r对于Rock Slat Structure来说，由于其Atom种类变为了Anion与Cation两个，其[[#Theoretical Density]]的分子也要对应的便为两个的和，即 $$\\rho=\\frac{n_CA_C+n_AA_A}{V_CN_A}$$ \\(n_CA_C\\)：Cation的Number和Moalr Mass，nA同理 对于Vc来说，其a变为了两个Atoms的Radus*2，有\\(V_C=(2R_A+2R_C)^3\\) Theoretical Density of Rock Salt #\r$$\\rho = \\frac{n_C A_C + n_A A_A}{V_C N_A}$$\nAnions #\r在Rock Salt Structure中的Anion看似处于FCC的位置中 但实际上由于附近的Cation和不同Anion之间的相互作用力导致Anion实际上不在精确的FCC位置上 Cations #\rthe cations will always touch their nearest neighbour anions The Body Centred Cubic Crystal (BCC) Structure #\r结构特点：每个晶胞的八个顶点各有一个原子，且晶胞中心还有一个原子。 n：每个晶胞含有 2 个原子（8 个顶点原子各占 1/8，中心原子占 1 个）。 CN：8（每个原子有 8 个最近邻原子）。 APF：约 68%。 例子：铁、钨、铬等。 Number of Atoms in BCC #\r$$\\frac{1}{2}*2+\\frac{1}{8}*8=2$$\nCoordination Number of BCC #\r$$CoordinationNumberBCC​=8$$\nAtomic Packing Factor for BCC #\r$$APF=\\frac{Volume_{Spheres}}{Volumn_{Unit~Cell}}$$\n对于BCC，要取其Unit Cell边长与Atom radius关系得用Cubic Diagonal，即 $$3a^2=16R^2，\\Rightarrow a_{BCC}=\\frac{4}{\\sqrt3}R$$ $$APF = \\frac{2 \\left( \\frac{4}{3} \\pi R^3 \\right)}{(\\frac{4}{\\sqrt3})^3} \\ \\Rightarrow APF_{BCC} = 0.68$$\nInterstitial Sites #\rSpace between other atoms 其体积就代表了Crystal Structure中的间隙的部分 By convention, we name interstitial sites according to the solid geometry that they create Octanhedron Interstitial Site #\r对于Rock Salt中的Anion的Interstitial Sites，将他们命名为Octanhedron Interstitial Site Coordination Number of a Interstitial Site #\r对于Interstitial Site来说，其CN代表了Interstitial Site中心点位置的Atom与最近Atom接触的个数 对于Rock Salt来说，Intersitital Site CN = 6 Simple Cubic Interstitial Site #\r结构特点：每个晶胞的八个顶点各有一个原子，顶点原子通过边连接，但面心和体心没有原子。 n：1Atom（8 个顶点原子各占 1/8）。 CN：6（每个原子有 6 个最近邻原子）。 APF：约 52%。 例子：钋（唯一的自然存在的例子）。 纯的简单立方晶格中，中心位置是空的（这个红点代表的就是Intersititial Site间隙位的大小 但在体心立方或其他某些间隙结构中，这一位置可以被其他原子或离子占据 Coordination Number of Simple Cubic #\r$$CoordinationNumberSimpleCubic​=8$$\nThe Size of Interstitial Sites #\r就Interstitial Site来说，其具有实际大小 前面Rock Salt中提到过Cations will always touch their nearest neighbour anions 阳离子总是会接触它们最近的阴离子，因此阳离子只有在足够大时才会占据晶体结构中的间隙位。如果阳离子太小，它将无法与最近的阴离子接触，因此不会稳定地占据间隙位 $$\\sin 45 = \\frac{2R_A}{2R_A + 2R_C} \\\n(2R_A + 2R_C)\\sin 45 = 2R_A \\\n2R_A\\sin 45 + 2R_C\\sin 45 = 2R_A \\\nR_A\\sin 45 + R_C\\sin 45 = R_A \\\n\\frac{R_A}{R_A}\\sin 45 + \\frac{R_C}{R_A}\\sin 45 = 1 \\\n\\sin 45 + \\frac{R_C}{R_A}\\sin 45 = 1 \\\n\\frac{R_C}{R_A}\\sin 45 = 1 - \\sin 45 \\\n\\frac{R_C}{R_A} = \\frac{1 - \\sin 45}{\\sin 45}=0.414$$\nHexagonal Close Packed (HCP) Structure #\r结构特点：原子以六边形排列，沿c轴有堆叠的结构。原子层是按照ABAB\u0026hellip;的顺序堆叠。 n：每个晶胞含有 6 个原子（从整体堆叠考虑）。 CN：12（类似于FCC，每个原子有12个最近邻原子）。 APF：约 74%。 ex.：镁、钛、锌等 Number of Atoms in HCP #\rCoordination Number of HCP #\rCN = 12 Atomic Packing Factor in HCP #\rHCP有着和FCC一样的APF（都为0.74）所以FCC有时会被称为CCP Different to FCC #\rFCC和HCP的主要区别在于原子的堆积顺序 FCC中的原子堆积顺序是ABCABC 而HCP中的堆积顺序是ABAB 尽管它们的堆积顺序不同，但由于原子排列非常紧密，它们的APF都是0.74 Close Packed #\r要想让Atom排列的更加紧密，需要有Close Packed的结构 当Atom在这种排列下，APF才能更高 在Not Close Packed下，这种排列的方式更加的松散 Closed Packed Plane #\rNot Closed Packed Plane in FCC #\r对于FCC的侧面上的Atom来说，其并不处于Closed Packed状态下 ","date":"Sep 25 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms4.thestructureproperty/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 9/25/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eYoung’s modulus change with Density \r\n    \u003cdiv id=\"youngs-modulus-change-with-density\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#youngs-modulus-change-with-density\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS4.TheStructureProperty/ECMS4.TheStructure-Property-24.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 4. The Structure Property","type":"docs"},{"content":" Last Edit 9/24/24\nHooke’s Law #\r$$F=kx$$\nSpring constant (k): Spring Constant Material properties（材料性能） #\r主要指材料的基本机械性能，如弹性模量、屈服强度、抗拉强度等 它们的计算方式应当剔除几何尺寸的影响 Stress 应力 #\r物体在外力作用下，单位面积上承受的内力 $$Stress(\\sigma)=\\frac{F}{A_0}$$ \\(A_0\\): Initial Cross-Sectional Area 应力的单位通常是帕斯卡（Pa） Strain 应变 #\r应变是材料在外力作用下发生的变形程度 $$Strain(\\epsilon)=\\frac{\\Delta l}{l_0}$$\n\\(\\Delta l\\)：change of material\u0026rsquo;s length \\(l_0\\): Initial length of material Young\u0026rsquo;s Modulus 杨氏模量 #\r是固体在载荷下的刚度或对弹性变形的抵抗力的量度\n材料在压缩或拉伸时会发生Elastic Deformation，而在Unloaded之后则会回到之前的Equilibrium $$E=\\frac{\\sigma}{\\epsilon}=\\frac{\\frac{F}{A_0}}{\\frac{\\Delta l}{l_0}}=\\frac{F\\cdot l_0}{A_0\\cdot \\Delta l}$$\nYoung\u0026rsquo;s Modulus (E)\n单位是Pa\nStructure Independent #\r当说\u0026quot;XXX is xxx independent\u0026quot; 时，代表了某个事物不依赖于某个特定因素 这里则是Young\u0026rsquo;s Modulus是不依赖于Structure Young\u0026rsquo;s Modulus其只与材料有关，与形状无关 具体来说是取决与材料的原子级别的相互作用，而不是材料的宏观或微观结构 Micro Perspective of Stress and Strain #\r从微观角度来看，物体由Atoms组成，其中存在Inter Atomic Forces 当Applied External Force的时候，物体将处于Loaded状态，其Shape将会发生改变 具体来说，Shape发生的改变是由于Atoms之间的间距发生了改变 不过只要整个Stress小于Yield Strength，所有的Deformation都将是Elastic的 代表了，当External Force被撤去的时候，既Unloaded之后，Atoms将会回到他们原来的Equilibrium position 需要注意的是，Atom之间的间距将会回到一开始的\\(r=r_0\\) 所以可以得出一个结论：Elastic Strain is Reversible Stress-Strain Curve 应力-应变图 #\r本图实际上为F-r图，即拉力-原子间半径图，但与Stress-Strain图相似，便用SS图讲解\n对于一个材料，在对其施加Stress的时候，其Strain会出现如此的固定趋势 在Stress等于0的时候，物体处于Equilibrium状态，具体来说其Attractive Force = Repulsive Force 在持续施加Stress后，Atoms最终将到达一个Yield Strength（不可逆点）后将会产生Plastic Deformation（将在下一章提到） 将Stress-Strain图放大到\\(r_0\\)两边后观察 可以发现Stress随Strain(Atomic Spacing)基本呈现Linear Trend，便可以说 $$E\\propto \\frac{dF}{dr}|_r=r_0$$ Young\u0026rsquo;s Modulus is directly propotional to slope of interatomic force speration curve at equilibrium spacing 简单的理解便为：杨氏模量等于与Stress对于Strain的变化率 更具图可以看出Stress对Strain的变化速率越大，其Young\u0026rsquo;s Modulus越大 即在Plastic Deformation前，Stress（External Force）越大材料的Young\u0026rsquo;s Modulus越大 既抵抗Elastic Deformation的作用越大 The way to determine material properties #\rTensile(Tension) Test\n图中展示了拉伸测试的基本原理，即通过对材料施加拉力（Tension），使其伸长（elongating），从而获得应力-应变曲线等相关数据 Grip Region（夹持区）：这是样品被测试机夹持的地方，两端施加拉力 Reduced Section（缩小部分）：这是样品的中间区域，它的截面积被减少，以确保样品在这个区域发生断裂或变形。在该区域内，材料会受到均匀的拉伸应力。 通过拉伸测试，可以获得材料的关键参数，如杨氏模量 (Young\u0026rsquo;s Modulus)、屈服强度 (Yield Strength)、极限抗拉强度 (Ultimate Tensile Strength, UTS) 和断裂延伸率 (Fracture Elongation) Disadvantage of Tensile Test #\r这种测试方法仍然存在局限性，例如Ceramics \u0026amp; Glasses等都不能利用这种方式测试Material Properties，具体来说 Low strain to fracture, almost no deformation before breaking：在断裂之间，几乎不发生Elastic Deformation hard to grip：对于Tensile Test所必须的Grip Region，由于一些Material的特殊性，他们无法在Grip Region内被夹住，继而无法进一步测试 Hard to load on Axis: 当Grip Region滑动的时候，无法使力沿轴拉伸 关于为什么这些会发生，如为什么Ceramics会直接断裂等，参考[[ECMS 3. Plastic Deformations]] ","date":"Sep 24 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms2.elasticbehavior/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 9/24/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eHooke’s Law \r\n    \u003cdiv id=\"hookes-law\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#hookes-law\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e$$F=kx$$\u003c/p\u003e","title":"ECMS 2. Elastic Behavior","type":"docs"},{"content":"","date":"Sep 23 2024","externalUrl":null,"permalink":"/tags/cover/","section":"Tags","summary":"","title":"Cover","type":"tags"},{"content":"\rLast Edit: 9/23/24\nPlastice Deformation (Permanent Deformation) #\rwe use the term plastic to describe permanent deformation 之所以是Plastic，是因为它derives from the Greek plastikos meaning to sculpt Changes After Plastic Deformation #\r在Plastic Deformation后，Atomic Spacing将保持\\(r=r_0\\) 但是Sequence of atoms将进入一个New Equilibrium 即在Marcro Perspective上发生Shape的Deform Tensile Strain将会保持一定非零大小 Micro Perspective of Plastic Deformation #\rDifferent between Elastic and Plastic Deformation #\rElastic #\r对于Elastic Deformation，开始前物理Atom之间间距应为\\(r_0\\) 泄力后仍应该是\\(r_0\\)，并且Atom将会到他们原有的Equilibrium 并且物体从Marco Perspective上并不发生Deformation Plastic #\r结束后Atom之间间距仍应该是\\(r_0\\) 泄力后Atom将进入一个新的Equilibrium 物体在泄力后，他的Tensile Strain将不会便为0而是保持在一定数 即Shape已经发生了Perminant Change Beyond Elastic Region #\r在Elastic Region外，便是完整的[[ECMS 2. Elastic Behavior#Young\u0026rsquo;s Modulus 杨氏模量]]的模型 Yield Strength: 屈服强度是指材料在发生永久变形之前，能够承受的最大应力。 当Strain到达Yield Strength之后，材料会从Elastic Deformation转变为Plastic Deformation，即Material发生Permanent Deformation 在过了Yield Strength之后Strain再增加后到了一定程度之后便会产生Fracture(Broken into pieces) Stress-Strain Curve for different materials #\rMetals #\r图中的绿色曲线 其特点有在一定位置之后开始产生Permanent Deformation 之后在持续的施加Stress之后其Strain变化率降低最后产生Fracture 相比于Ceramic和Polymer，其Young\u0026rsquo;s Modulus处于中间位置，高于Polymer但小于没有Permanent Deformation阶段的Ceramic Polymer #\r相对来说没有什么特点 具有较低的Young\u0026rsquo;s Modulus和Permanent Deformation区间 Ceramic #\r对于陶瓷类的物质，其没有Permanent Deformation的区间 对于他来说也存在Elastic Region 但可以看出整体Young\u0026rsquo;s Modulus非常高，并且呈现线性 在施加了一定的Stress后会直接Load enough and fracture Three-Point-Blending Test #\r底部两个点用作支撑，上方一个力将物体往下压 $$Stress(\\sigma)=\\frac{3FL}{2wh^2}$$\nTempered Glass 钢化玻璃 #\r![[ECMS 3. Plastic Deformations-5.png]]\n再高温下迅速向表面喷冷凝液将其降温 冷却将使玻璃表面收缩的比内部更快，产生了向心的Compress Stress 而内部由于受力将产生反作用力，向外产生张应力 整体的结构处于一个向内部收缩的趋势，导致当其受到了外力的时候，尝试使其Fracture的导致分子之间结构被破坏的力将被抵消 并且由于其内部存在Residual Stress（残余应力）导致了整体结构破坏的时候其Residual Stress将破坏结构至非常小的结构 ","date":"Sep 23 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms3.plasticdeformation/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 9/23/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch4 class=\"relative group\"\u003ePlastice Deformation (Permanent Deformation) \r\n    \u003cdiv id=\"plastice-deformation-permanent-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#plastice-deformation-permanent-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h4\u003e\r\n\u003cul\u003e\n\u003cli\u003ewe use the term \u003cstrong\u003eplastic\u003c/strong\u003e to describe permanent deformation\u003c/li\u003e\n\u003cli\u003e之所以是Plastic，是因为它derives from the Greek \u003cstrong\u003eplastikos\u003c/strong\u003e meaning to sculpt\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch5 class=\"relative group\"\u003eChanges After Plastic Deformation \r\n    \u003cdiv id=\"changes-after-plastic-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#changes-after-plastic-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h5\u003e\r\n\u003cul\u003e\n\u003cli\u003e在Plastic Deformation后，Atomic Spacing将保持\\(r=r_0\\)\u003c/li\u003e\n\u003cli\u003e但是Sequence of atoms将进入一个New Equilibrium\u003c/li\u003e\n\u003cli\u003e即在Marcro Perspective上发生Shape的Deform\u003c/li\u003e\n\u003cli\u003eTensile Strain将会保持一定非零大小\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch5 class=\"relative group\"\u003eMicro Perspective of Plastic Deformation \r\n    \u003cdiv id=\"micro-perspective-of-plastic-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#micro-perspective-of-plastic-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h5\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations-1.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations-2.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 3. Plastic Deformation","type":"docs"},{"content":" Ramsay, S. (2024). Engineering Chemistry \u0026amp; Materials Science. Top Hat. https://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b. ","date":"Sep 23 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/","section":"Docs","summary":"\u003cul\u003e\n\u003cli\u003eRamsay, S. (2024). Engineering Chemistry \u0026amp; Materials Science. Top Hat. \u003ca href=\"https://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b\" target=\"_blank\"\u003ehttps://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e","title":"Engineering Chemistry \u0026 Materials Science","type":"docs"},{"content":" Last Edit 7/4/24\n通货膨胀 #\r钱的贬值 CPI Consumer Price Index 消费者物价指数 #\r美国CPI #\r解释通货膨胀的理论 #\rKeynesian 凯恩斯主义 #\rNeo-Keynesian 新凯恩斯主义 #\rMonetarism 货币主义 #\r根本原因 #\r通货膨胀的可控性 #\r非常难以控制 通货膨胀的后果 #\r当把钱藏在床底下时，其购买力在不断的下降 逼迫人们不断的生产和消费 对于经济体来说，主要目标是提高GDP，人均消费水平 而通过适量的通货膨胀可以有助于提高人们的生产热情，提高GDP，对于整个经济体来说是好的 Misery Index 痛苦指数 #\rDeflation 通货紧缩 #\r今年的钱存着会比明年更加之前 90年代的日本就是典型例子 人们就不会消费，不花钱不生产，进入一个低欲望社会，GDP也就下降 HyperInflation 恶心通货膨胀 #\r对于经济的打击是毁灭性的 Target Inflation Rate 目标通胀 #\r保住通胀是底线，即使可能导致段时间的经济下降 导致Inflation的原因 #\r因素一：Demand-Pull 需求拉动 #\r刺激需求，导致产量的上升与价格的上升，形成一个对于经济上升的良心循环 但可以发现副作用是Inflation 政府可以通过直接发钱达到刺激需求，其可以直接刺激经济，但要面临通货膨胀的风险 可以总结出，Inflation和Economic Growth注定是会绑定在一块的 产能过剩 #\r产能直接影响了刺激需求后Inflation的变化 当产能过剩的情况下，即使总需求上升，由于能有产能，并不需要大幅度提高价格以达到供应需求的目的 相反，当产能不够的情况下，需求的价格必定会上涨导致Inflation的发生 判断产能的方式 - Unemployment Rate 失业率 #\r当失业率高的情况下，说明劳动力很多都在休息，既属于产能过剩的情况 所以一般Inflation发生前都会存在Low Unemployment Rate的情况 Philips Curve 菲利普斯曲线 #\r钱发不到实体经济中 #\r所以最简单的方式就是政府直接发钱，直接发到人手里 Cost-push 成本上涨 #\r成本上涨导致的价格上涨 反而抑制了需求，百害而无一益 俄罗斯通胀 #\r由于卢布的下跌导致的成本上升导致的Cost-push Inflation Money Supply 过量的货币供给 #\r几乎所有的HyperInflation都是政府过度印钱导致的 为什么要不断印钱 #\r政府既然看到了为什么不停止 因为陷入了恶性循环 一般都是外因导致的，如战争 政府无节制印钱后，货币量增加，大家需求也就上涨，来到了Demand-Pull中 但由于政府印钱速度太快，导致了实际上Demand-Pull的Inflation来到了不可抑制的情况，但这时候其实还没达到HyperInflation 这时候人民已经有了足够的钱，并且不会存钱，导致了大家都不再工作，导致产量的下降，导致成本的上升，最终进入恶性循环 预期的Inflation #\r当预期了Inflation时，大家都会超前消费，货币流通速度增加了 通货膨胀就自己发生了 ","date":"Jul 4 2024","externalUrl":null,"permalink":"/docs/economic/inflation/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 7/4/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e通货膨胀 \r\n    \u003cdiv id=\"%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cul\u003e\n\u003cli\u003e钱的贬值\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003eCPI Consumer Price Index 消费者物价指数 \r\n    \u003cdiv id=\"cpi-consumer-price-index-%E6%B6%88%E8%B4%B9%E8%80%85%E7%89%A9%E4%BB%B7%E6%8C%87%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#cpi-consumer-price-index-%E6%B6%88%E8%B4%B9%E8%80%85%E7%89%A9%E4%BB%B7%E6%8C%87%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e美国CPI \r\n    \u003cdiv id=\"%E7%BE%8E%E5%9B%BDcpi\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E7%BE%8E%E5%9B%BDcpi\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/Economic_Static/Inflation/Inflation%282%29.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Inflation 通货膨胀","type":"docs"},{"content":" Last Edit 4/15/24\nRegression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式\n[[Regression 回归]] 3.1.1 线性回归的基本元素 #\rLinear Regression 线性回归可以追溯到19世纪，其基于几个基本的假设 假设自变量与因变量之间为线性关系 假设噪声正常，如遵循正态分布 3.1.1.1 线性模型 #\r[[线性模型]] 线性假设是指目标可以表示为特征的加权和，如下例子 E.X. $$Price=w_{area}\\cdot area+w_{age}\\cdot age+b$$\nw称为Weight权重 b称为Bias，Offset或者Intercept偏置，即特征为0时的预测值 严格来说，上式为输入特征的一个[[Affine Transformation 仿射变换]]\n给定一个数据集，我们的目标即为寻找模型的Weight和Offset 高维数据集 #\r在Deep Learning 领域，我们通常使用的是高纬数据集，建模时采用[[2.3 线性代数]]的表示方法会比较方便。 当我们的输入包含多个特征时，我们将预测结果表示为\\(\\hat{y}\\) 点积形式 #\r可以用点积形式来简洁的表达模型\\(x\\in R^d,w\\in R^d\\) $$\\hat{y}=w^Tx+b$$ Model Parameters 模型参数 #\r在开始寻找最好的模型参数前，我们还需要两个东西 一种模型质量的度量方式 #\r一种能更新模型以提高预测质量的方式 #\r3.1.1.2 损失函数 #\r在开始考虑如何用模型Fit 拟合数据之前，我们需要一个拟合程度的度量 Loss Function 损失函数 #\r量话目标的实际值和预测值之间的差距 通常选用非负数作为Cost，并且数值越小损失越小 平方误差函数 #\r回归问题中最常用的Cost Function是平方误差函数 $$l^{(i)}(w,b)=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$ 常数\\(\\frac{1}{2}\\)的存在不会带来本质的差别，但当我们对这一方程求导后由于\\(\\frac{1}{2}\\)的存在会使常数等于1 由于平方误差函数中的二次方项，会导致估计值和观测值之间较大的差异造成更大的损失。 为了度量模型在整个数据集上的质量，我们需要计算训练集上的样本损失均值 $$L(w,b)= \\frac{1}{n}\\Sigma^n_{i=1}l^{(i)}(w,b)=\\frac{1}{n}\\Sigma^n_{i=1}\\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2$$ 总的来说训练模型就是为了找到一组参数\\(w^,b^\\)，其 $$w^,b^=argmin~L(w,b)$$ 3.1.1.3 解析式 #\r线性回归是一个很简单的优化问题，与大部分模型不同，其解可以用一个公式简单的表达出来，这类解便称为Analytical Solution 解析解 3.1.1.4 随机梯度下降 #\r即使在无法得到解析解的情况下，我们可以有效的训练模型 Gradient Descent 梯度下降 #\r最简单的方法就是计算Cost Function关于模型参数的导数（梯度） 但由于每次操作前都需要遍历整个数据集，导致执行速度非常之慢 所以通常会在每次更新时候抽取一小批样本，即为Minibatch Stochastic Gradient Descent 小批量随机梯度下降 SGD #\rMinibatch Stochastic Gradient Descent 小批量随机梯度下降 每次迭代中，我们首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数，并从当前参数的值中减掉 $$(w,b)\\leftarrow(w,b)-\\frac{\\eta}{|B|}\\Sigma_{i\\in B}\\partial_{w,b}l^{(i)}(w,b)$$ 初始化模型参数的值 从数据集中随机抽取小批量样本在负梯度方向上更新参数，并一直迭代 \\(\\eta\\)表示Learning Rate 学习率 B表示Batch Size 批量大小 Hyperparameter 超参数 #\r这些可以调整但不在训练过程中更新的参数称为超参数 Hyperparameter Tuning为调整Hyperparameter的过程 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的 收敛 #\rLinear Regression只会让预测值无限接近于实际值而却不能在有限的步数内非常精确地达到最小值 Generalization 泛化 #\r寻找到一组合适的Hyperparameter纵然困难，但更加困难的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为Generalization 泛化 3.1.1.5 用模型进行预测 #\r需要指出的是，Deep Learning对于实际值的接近更多是一种Prediction预测而非Inference推断 3.1.2 矢量化加速 #\r为了同时处理整个小批量的样本，同时防止在python中编写开销高昂的for循环 矢量化性能测试 #\r实例化两个全为1的10000维向量，采取两种处理方式，Python的for循环和对+的调用 ###初始化两个Tensor\rn = 10000\ra = torch.ones([n])\rb = torch.ones([n])\r###用for循环完成一次\rc = torch.zeros(n)\rtimer = Timer()\rfor i in range(n):\rc[i] = a[i] + b[i]\rf\u0026#39;{timer.stop():.5f} sec\u0026#39;\r###用线性代数完成矢量化运算\rtimer.start()\rd = a + b\rf\u0026#39;{timer.stop():.5f} sec\u0026#39; 得到的结果为 0.167sec 和0.00042 sec 3.1.3 正态分布与平方损失 #\r接下来，我们通过对噪声分布的假设来解读平方损失目标函数 正态分布 #\r[[Normal Distribution 正态分布]] 均方误差损失函数 #\r均方损失可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布，如下 $$y=w^Tx+b+\\epsilon$$ \\(\\epsilon\\)代表了噪声 Likehood 似然 #\r[[Likehood 似然]] 通过给定的x观测到特定y的似然（likelihood） $$p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)$$ 对于似然函数\\(L(\\theta|data)=P(data|\\theta)= \\Pi^N_{i=1}P(x_i|\\theta)\\) 已知x的正态分布密度函数，也就是x（\\(\\theta\\)取每个值的概率） 要求得给定x（\\(\\theta\\)）（其不固定，但遵循正态分布）观测到特点y的似然，得到公式 \\(L(x|y)=P(y|x)\\) 已知\\(p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)\\) 由于\\(P(y|x)=\\Pi^N_{i=1}P(y^{(i)}|x^{(i)})\\)（由x的参数条件下观测到y的可能性为独立的N个子事件的乘积） 根据[[Maximum Likehood Estimation 极大似然估计]]，参数w和b的最优值是使整个数据集的[[Likehood 似然]]最大的值 但又因为乘积最大化问题十分复杂，并且由于历史遗留问题，优化常说的不是最大化，而是最小化 所以我们需要通过最小化对数似然\\(-logP(y|x)\\)，由此可以得到的数学公式为 $$-logP(y|x)=\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(y^{(i)}-w^Tx^{(i)}-b)^2$$ 推导如下 \\(-logP(y|x)=-log\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)\\) \\(=-log\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)}\\) \\(=-log(\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}})+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) \\(=-log\\sum\\limits^n_{i=1}(2\\pi\\sigma^2)^{-\\frac{1}{2}}+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) = \\(\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) 现在我们只需要假设�是某个固定常数就可以忽略第一项\\(\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)\\)，因为第一项不依赖于w和b 对于第二项，除了常数\\(\\frac{1}{\\sigma^2}\\)外，其余部分与[[#平方误差函数]]是一样的 平方误差函数 #\r$$l^{(i)}(w,b)=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$\n幸运的是，上面式子的解并不依赖于\\(\\sigma\\) 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计 3.1.4 从线性回归到深度网络 #\r到目前，我只谈论了线性模型，而神经网络涵盖了更为丰富的模型，并且我们也可以用描述神经网络的方式来描述线性模型，从而把线性模型看作一个神经网络。可以用“层”符号来重写这个模型 3.1.4.1 神经网络图 #\r制图表可以可视化模型中正在发生的事情，但该图只显示了链接模式，而不包含权重和偏置的值 在上图中，输入为\\(x_1,\\dots x_d\\)，可知输入层的Feature Dimensionality 输入数（或称为特征维度）为d\n网络的输出层为\\(o_1\\)，因此输出层的输出数为1\n需要注意的是，输入值都是已经给定的，并且只有一个_计算_神经元。 由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。\nFully-Connected Layer 全连接层 #\r对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（Fully-connected layer）或称为稠密层（dense layer）。 3.1.4.2 生物学 #\r即使观察真实的神经元，但当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感\n我们援引斯图尔特·罗素和彼得·诺维格在他们的经典人工智能教科书 Artificial Intelligence:A Modern Approach (Russell and Norvig, 2016) 中所说的：虽然飞机可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。 同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。\n","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 4/15/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eRegression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式\u003c/p\u003e","title":"D2L 3.1 Linear Regression","type":"docs"},{"content":"从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。\n3.2.1 生成数据集 #\r为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集 使用线性模型参数\\(w=[2,-3.4]^T,b=4.2\\)和噪声项\\(\\epsilon\\)生成数据集及其标签 $$y=Xw+b+\\epsilon$$ \\(\\epsilon\\)可以视为模型预测和标签时的潜在观测误差 3.2.2 读取数据集 #\r3.2.3 初始化模型参数 #\r通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。 w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\rb = torch.zeros(1, requires_grad=True) 在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据 并运用[[2.5 自动微分]]来计算梯度 3.2.4 定义模型 #\r这里我们用的还是线性模型，即$$\\hat y=w^Tx+b$$ def linreg(X, w, b): #@save\r\u0026#34;\u0026#34;\u0026#34;线性回归模型\u0026#34;\u0026#34;\u0026#34;\rreturn torch.matmul(X, w) + b 3.2.5 定义损失函数 #\r模型建立后，开始使用对原函数的损失函数进行梯度下降 这里我们使用[[3.1_LinearRegression#平方误差函数]] 3.2.6 定义优化算法 #\r使用[[3.1_LinearRegression#Minibatch Stochastic Gradient Descent 小批量随机梯度下降]] 3.2.7 训练 #\r本质为执行一下循环 初始化参数 更新梯度，更新参数 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/","section":"Docs","summary":"\u003cp\u003e从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。\u003c/p\u003e","title":"D2L 3.2 Object-Oriented Design for Implementation","type":"docs"},{"content":"本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型\n3.3.1 生成数据集 #\rimport numpy as np\rimport torch\rfrom torch.utils import data\rfrom d2l import torch as d2l\rtrue_w = torch.tensor([2, -3.4])\rtrue_b = 4.2\rfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000) d2l.synthetic_data(true_w, true_b, 1000) 是 d2l 库中的一个函数调用。这个函数用于生成合成数据，其中包括特征数据和对应的标签数据。 具体地，这个函数接受三个参数： true_w：真实的权重，用于生成特征数据。 true_b：真实的偏置，用于生成特征数据。 1000：生成数据的数量，这里是指生成1000个样本。 3.3.2 读取数据集 #\rdef load_array(data_arrays, batch_size, is_train=True): #@save\r\u0026#34;\u0026#34;\u0026#34;构造一个PyTorch数据迭代器\u0026#34;\u0026#34;\u0026#34;\rdataset = data.TensorDataset(*data_arrays)\rreturn data.DataLoader(dataset, batch_size, shuffle=is_train)\rbatch_size = 10\rdata_iter = load_array((features, labels), batch_size) 3.3.3 定义模型 #\r对于标准深度学习模型，我们可以使用框架的预定义好的层 ## nn是神经网络的缩写\rfrom torch import nn\rnet = nn.Sequential(nn.Linear(2, 1)) nn.Sequential：这是 PyTorch 中用于构建顺序神经网络模型的类。它允许用户按顺序堆叠多个层或模块，构建神经网络模型。 nn.Linear(2, 1)：这里创建了一个全连接层，其中 nn.Linear 是 PyTorch 中用于定义全连接层的类。构造函数 nn.Linear(in_features, out_features) 接受两个参数： in_features：输入特征的数量。在这个例子中，输入特征的数量为 2。 out_features：输出特征的数量。在这个例子中，输出特征的数量为 1。 因此，net 这个模型包含一个具有 2 个输入特征和 1 个输出特征的全连接层。 这样的模型可以用于简单的二分类问题，其中输入特征有 2 个，输出特征有 1 个，代表着模型对样本的分类结果。 3.3.4 初始化模型参数 #\rnet[0].weight.data.normal_(0, 0.01)\rnet[0].bias.data.fill_(0) 在网络的第一层输入参数 3.3.5 定义损失函数 #\r计算均方误差使用的是MSELoss类，也称为平方�2范数。 默认情况下，它返回所有样本损失的平均值。 loss = nn.MSELoss() 3.3.6 定义优化算法 #\rtrainer = torch.optim.SGD(net.parameters(), lr=0.03) 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。 3.3.7 训练 #\r通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。\nnum_epochs = 3\rfor epoch in range(num_epochs):\rfor X, y in data_iter:\rl = loss(net(X) ,y)\rtrainer.zero_grad()\rl.backward()\rtrainer.step()\rl = loss(net(features), labels)\rprint(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) num_epochs = 3：定义了训练的轮数，这里设置为 3 for epoch in range(num_epochs):：使用 for 循环迭代每个训练轮数 for X, y in data_iter:：使用 data_iter 迭代器遍历训练数据集，其中 X 是特征，y 是对应的标签 l = loss(net(X) ,y)：计算模型对当前批次数据的预测值，并计算与真实标签之间的损失 trainer.zero_grad()：梯度清零，以避免梯度累积 l.backward()：反向传播，计算损失函数相对于模型参数的梯度 trainer.step()：更新模型参数，采用优化算法更新参数 l = loss(net(features), labels)：计算当前训练轮数结束后整个训练集上的损失 print(f'epoch {epoch + 1}, loss {l:f}')：打印当前训练轮数和对应的损失值 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/","section":"Docs","summary":"\u003cp\u003e本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型\u003c/p\u003e","title":"D2L 3.3 A concise implementation of linear regression","type":"docs"},{"content":"回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。\n事实上，我们也对_分类_问题感兴趣：不是问“多少”，而是问“哪一个”\n3.4.1 分类问题 #\r[[One-hot encoding 独热编码]] 3.4.2 网格架构 #\r为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的[[Affine Function 仿射函数]] E.X.\n假设现在有3个未规范化的预测(Logit)：\\(o_1,o_2和o_3\\) \\(o_1=x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1\\) \\(o_2=x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2\\) \\(o_1=x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3\\) 3.4.3 全连接层的参数开销 #\r对于任何具有d个输入和q个输出的全连接层[[3.1_LinearRegression#Fully-Connected Layer 全连接层]]，其参数开销为\\(O(dq)\\)，但可以通过超参数减少到\\(O(\\frac{dq}{n})\\) 3.4.4 softmax 运算 #\r我们希望模型的输出\\(\\hat y_j\\)可以视为属于类\\(j\\)的概率，然后选择具有最大输出值的类别\\(argmaxx_jy_j\\)作为我们的预测，例如\\(\\hat y_1,\\hat y_2\\)和\\(\\hat y_3\\)分别为\\(\\hat y={0.1,0.8,0.1}\\)那么我们的预测变为独热编码的\\(y={0,1,0}\\)，即为鸡 能否将未规范化的预测o直接视作我们感兴趣的输出呢 #\r不行 因为将线性层的输出直接视为概率时存在一些问题 我们没有限制这些输出数字的总和为1 根据输入的不同，它们可以为负值 其违反了[[概率论公理]] 概率论 #\r要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率 Calibration 校准 #\r例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别 Softmax 函数 #\r社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上发明的softmax函数 softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式$$\\hat y=softmax(o)，其中\\hat y_j=\\frac{exp(o_j)}{\\sum_kexp(o_k)}=\\frac{e^j}{\\sum_ke^k}$$ 这里，对于所有的j总有\\(0\\leq\\hat y_j\\leq1\\)，因此\\(\\hat y\\)可以视为一个正确的概率分布 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。 3.4.5 小批量样本的矢量化 #\r为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算 3.4.6 损失函数 #\r使用[[Maximum Likehood Estimation 极大似然估计]] 3.4.6.1 对数似然 #\rsoftmax函数给出了一个向量\\(\\hat y\\)， 我们可以将其视为“对给定任意输入x的每个类的条件概率\n通过计算softmax的对数似然，可以推导出他的损失函数\n假设现在有一个数据集 \\({X,Y}\\)，其具有n个样本，其中索引i的样本由特征向量\\(x^{(i)}\\)和独热标签向量\\(y^{(i)}\\)组成，可以将估计值与实际值进行比较$$P(Y|X)=\\prod^n_{i=1}P(y^{(i)}|x^{(i)})$$\n根据[[3.1_LinearRegression#Likehood 似然]]，已知最大化$P(Y|X)，相当于最小化负对数似然 $$P(Y|X)=\\sum^n_{i=1}-logP(y^{(i)}|x^{(i)})=\\sum^n_{i=1}l(y^{(i)},\\hat y^{(i)})$$\n其中对于任何标签y和预测模型\\(\\hat y\\)，损失函数为$$l(y,\\hat y)=-\\sum^{q}_{j=1}y_j\\log \\hat y_j$$\n这个[[3.1_LinearRegression#Loss Function 损失函数]]并没有介绍过，他的名字为Cross-entropy Loss交叉熵损失，将在后面介绍到\n为什么要加入对数，而不是直接取负数 #\r数值稳定性： 在概率模型中，可能会有大量的乘法运算，这可能导致数值下溢或溢出问题，尤其是当概率很小的时候。通过取对数，可以将乘法运算转换为加法运算，从而提高计算的稳定性。 对数函数的导数相对于原函数来说更简单，这使得梯度的计算更加高效。特别是在梯度下降等优化算法中，简化的导数计算可以显著减少计算量。 对数函数的特性使得推导和分析变得更加简单，因为它可以将乘法转换为加法，并且有很多性质，例如对数函数的导数比原函数更容易处理 3.4.6.2 softmax及其导数 #\r由于softmax和相关的损失函数很常见， 因此我们需要更好地理解它的计算方式 将3.4.3带入Cross-entropy Loss Function中，得到 $$\\begin{align}l(y,\\hat y)=-\\sum^{q}{j=1}y_j\\log \\frac{e^{o_j}}{{\\sum^{q}{k=1}e^{o_k}}} \\=-\\sum_{j=1}^{q}y_j[\\ln e^{o_j}-\\ln \\sum^q_{k=1}e^{o_k}] \\=\\sum^q_{j=1}y_j\\log\\sum^q_{k=1}e^{o_k}-\\sum^q_{j=1}y_jo_j \\=\\log \\sum^q_{k=1}e^{o_k}-\\sum^q_{j=1}y_jo_j\\end{align}$$ Softmax结合Cross Entropy的求导过程 #\r已知Cross Entropy Function$$H(y_i,p_i)=-\\sum_iy_i\\log pi$$\n\\(y_i\\)为预测事件，\\(\\log p_i\\)为一个分布的最优编码\n得到[[Home Page]] 3.4.6.3 交叉熵损失 #\r[[Cross-Entropy 交叉熵]] 3.4.7.1 熵 #\r[[Cross-Entropy 交叉熵#2. 熵]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/","section":"Docs","summary":"\u003cp\u003e回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。\u003c/p\u003e","title":"D2L 3.4 Softmax Regression","type":"docs"},{"content":"MNIST数据集 (LeCun et al., 1998) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (Xiao et al., 2017)。\n在此引入这个数据集是因为之后对于算法的评估均给予这一数据集\n%matplotlib inline\rimport sys\rfrom mxnet import gluon\rfrom d2l import mxnet as d2l\rd2l.use_svg_display() 3.5.1 读取数据集 #\r## 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\r## 并除以255使得所有像素的数值均在0～1之间\rtrans = transforms.ToTensor()\rmnist_train = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True)\rmnist_test = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True) Fshion-MNIST中包含的10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。 def get_fashion_mnist_labels(labels): #@save\r\u0026#34;\u0026#34;\u0026#34;返回Fashion-MNIST数据集的文本标签\u0026#34;\u0026#34;\u0026#34;\rtext_labels = [\u0026#39;t-shirt\u0026#39;, \u0026#39;trouser\u0026#39;, \u0026#39;pullover\u0026#39;, \u0026#39;dress\u0026#39;, \u0026#39;coat\u0026#39;,\r\u0026#39;sandal\u0026#39;, \u0026#39;shirt\u0026#39;, \u0026#39;sneaker\u0026#39;, \u0026#39;bag\u0026#39;, \u0026#39;ankle boot\u0026#39;]\rreturn [text_labels[int(i)] for i in labels] Plt 可视化样本 #\rdef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save\r\u0026#34;\u0026#34;\u0026#34;绘制图像列表\u0026#34;\u0026#34;\u0026#34;\rfigsize = (num_cols * scale, num_rows * scale)\r_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\raxes = axes.flatten()\rfor i, (ax, img) in enumerate(zip(axes, imgs)):\rif torch.is_tensor(img):\r# 图片张量\rax.imshow(img.numpy())\relse:\r# PIL图片\rax.imshow(img)\rax.axes.get_xaxis().set_visible(False)\rax.axes.get_yaxis().set_visible(False)\rif titles:\rax.set_title(titles[i])\rreturn axes\rX, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\rshow_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y)); ![[Pasted image 20240331164614.png]]\n3.5.2 读取小批量 #\r为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。 batch_size = 256\rdef get_dataloader_workers(): #@save\r\u0026#34;\u0026#34;\u0026#34;使用4个进程来读取数据\u0026#34;\u0026#34;\u0026#34;\rreturn 4\rtrain_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\rnum_workers=get_dataloader_workers()) 3.5.3. 整合所有组件 #\r现在我们定义load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状 def load_data_fashion_mnist(batch_size, resize=None): #@save\r\u0026#34;\u0026#34;\u0026#34;下载Fashion-MNIST数据集，然后将其加载到内存中\u0026#34;\u0026#34;\u0026#34;\rtrans = [transforms.ToTensor()]\rif resize:\rtrans.insert(0, transforms.Resize(resize))\rtrans = transforms.Compose(trans)\rmnist_train = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True)\rmnist_test = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True)\rreturn (data.DataLoader(mnist_train, batch_size, shuffle=True,\rnum_workers=get_dataloader_workers()),\rdata.DataLoader(mnist_test, batch_size, shuffle=False,\rnum_workers=get_dataloader_workers())) 下面，我们通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能。 train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\rfor X, y in train_iter:\rprint(X.shape, X.dtype, y.shape, y.dtype)\rbreak 我们现在已经准备好使用Fashion-MNIST数据集，便于下面的章节调用来评估各种分类算法 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/","section":"Docs","summary":"\u003cp\u003eMNIST数据集 (\u003ca href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90\"title=\"LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., \u0026amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\" target=\"_blank\"\u003eLeCun \u003cem\u003eet al.\u003c/em\u003e, 1998\u003c/a\u003e) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (\u003ca href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189\"title=\"Xiao, H., Rasul, K., \u0026amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.\" target=\"_blank\"\u003eXiao \u003cem\u003eet al.\u003c/em\u003e, 2017\u003c/a\u003e)。\u003c/p\u003e","title":"D2L 3.5 Image classification datasets","type":"docs"},{"content":"\r3.6.1 初始化模型参数 #\r和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量 在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10 初始化权重w #\r与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0 num_inputs = 784\rnum_outputs = 10\rW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\rb = torch.zeros(num_outputs, requires_grad=True) 3.6.2 定义softmax操作 #\r实现softmax操作由三个步骤组成 对每个项求幂 对每一行求和，得到其规范化常数 每一行除以其规范化常数，保持结果的和为1 $$softmax(X){ij}=\\frac{exp(X{ij})}{\\sum_kexp(X_{ik})}$$ 分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自统计物理学中一个模拟粒子群分布的方程 def softmax(X):\rX_exp = torch.exp(X)\rpartition = X_exp.sum(1, keepdim=True)\rreturn X_exp / partition # 这里应用了广播机制 keepdim=True: 在进行张量操作时，保持原始张量的维度\ntorch.normal(0, 1, (2, 5)) 是用 PyTorch 生成一个服从均值为 0，标准差为 1 的正态分布的张量。\n其中的 (2, 5) 是指生成的张量的形状为 2 行 5 列的矩阵\n3.6.3 定义模型 #\r定义softmax操作后，我们可以实现softmax回归模型 def net(X):\rreturn softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) 3.6.4 定义损失函数 #\r引入[[Cross-Entropy 交叉熵]]损失函数 深度学习中，交叉熵函数最为常见，因为分类问题的数量远远超过了回归问题 y = torch.tensor([0, 2])\ry_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ry_hat[[0, 1], y] y_hat： 包含2个样本在3个类别的预测概率 y：真实类，0代表第一类，1代表第二类，2代表第三类 [[0,1],y]：一种tensor的高级引索功能，其选择了y_hat中的第一行和第二行 而y给予了列的位置，所以输出分别为第一行第0位和第二行第2位 3.6.5 分类精度 #\r给定预测概率分布\\(\\hat y\\)，当我们必须输出Hard Prediciton 硬预测时，我们通常选择概率最高的类 当预测和标签分类y一致时，即是正确的 分类精度指的就是正确预测数量与总预测数量之比 def accuracy(y_hat, y): #@save\r\u0026#34;\u0026#34;\u0026#34;计算预测正确的数量\u0026#34;\u0026#34;\u0026#34;\rif len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1:\ry_hat = y_hat.argmax(axis=1)\rcmp = y_hat.type(y.dtype) == y\rreturn float(cmp.type(y.dtype).sum()) 扩展到任意数据迭代器data_iter可访问的数据集 #\rdef evaluate_accuracy(net, data_iter): #@save\r\u0026#34;\u0026#34;\u0026#34;计算在指定数据集上模型的精度\u0026#34;\u0026#34;\u0026#34;\rif isinstance(net, torch.nn.Module):\rnet.eval() # 将模型设置为评估模式\rmetric = Accumulator(2) # 正确预测数、预测总数, Accmulator在下面定义\rwith torch.no_grad():\rfor X, y in data_iter:\rmetric.add(accuracy(net(X), y), y.numel())\rreturn metric[0] / metric[1] 首先，如果 net 是 torch.nn.Module 的子类，就将模型设置为评估模式，即调用 net.eval()。在评估模式下，模型的行为可能会略有不同，比如 Dropout 层在评估模式下会关闭，以避免随机丢弃部分节点 创建了一个名为 metric 的累加器（Accumulator）。这个累加器用于记录正确预测数和总预测数，初始化为两个元素的列表 [0, 0] Accumulator：这个类在下面定义 使用 torch.no_grad() 上下文管理器，禁用梯度计算 最后就是将评估结果添加至metric中 Accumulator类 #\r这里定义一个实用程序类Accumulator，用于对多个变量进行累加。 在上面的evaluate_accuracy函数中， 我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。 class Accumulator: #@save\r\u0026#34;\u0026#34;\u0026#34;在n个变量上累加\u0026#34;\u0026#34;\u0026#34;\rdef __init__(self, n):\rself.data = [0.0] * n\rdef add(self, *args):\rself.data = [a + float(b) for a, b in zip(self.data, args)]\rdef reset(self):\rself.data = [0.0] * len(self.data)\rdef __getitem__(self, idx):\rreturn self.data[idx] __init__(self, n): 这是类的构造函数，用于初始化累加器。它接受一个参数 n，表示要累加的变量的数量。在初始化时，创建了一个包含 n 个元素的列表，每个元素初始化为 0.0 add(self, *args): 这个方法用于将参数 args 中的值与累加器中的值相加。参数 args 是一个可变参数，可以接受任意数量的参数。通过 zip 函数，将 args 中的值逐个与累加器中对应位置的值相加，并更新累加器中的值 reset(self): 这个方法用于重置累加器的值 __getitem__(self, idx): 这个方法允许通过索引访问累加器中的值。给定一个索引 idx，它返回累加器中对应位置的值 3.6.6 训练 #\r首先，我们定义一个函数来训练一个迭代周期 updater是更新模型参数的常用函数，它接受批量大小作为参数 def train_epoch_ch3(net, train_iter, loss, updater): #@save\r\u0026#34;\u0026#34;\u0026#34;训练模型一个迭代周期（定义见第3章）\u0026#34;\u0026#34;\u0026#34;\r# 将模型设置为训练模式\rif isinstance(net, torch.nn.Module):\rnet.train()\r# 训练损失总和、训练准确度总和、样本数\rmetric = Accumulator(3)\rfor X, y in train_iter:\r# 计算梯度并更新参数\ry_hat = net(X)\rl = loss(y_hat, y)\rif isinstance(updater, torch.optim.Optimizer):\r# 使用PyTorch内置的优化器和损失函数\rupdater.zero_grad()\rl.mean().backward()\rupdater.step()\relse:\r# 使用定制的优化器和损失函数\rl.sum().backward()\rupdater(X.shape[0])\rmetric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\r# 返回训练损失和训练精度\rreturn metric[0] / metric[2], metric[1] / metric[2] if isinstance(net, torch.nn.Module):：检查变量 net 是否是 torch.nn.Module 类的实例 net.train(): 这一行将模型（net）设置为训练模式 metric = Accumulator(3): 创建一个长度为3的累加器 在计算梯度后，根据数据类型，如pytorch类或者自定义类累加处理结果 训练函数 #\rdef train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save\r\u0026#34;\u0026#34;\u0026#34;训练模型（定义见第3章）\u0026#34;\u0026#34;\u0026#34;\ranimator = Animator(xlabel=\u0026#39;epoch\u0026#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9],\rlegend=[\u0026#39;train loss\u0026#39;, \u0026#39;train acc\u0026#39;, \u0026#39;test acc\u0026#39;])\rfor epoch in range(num_epochs):\rtrain_metrics = train_epoch_ch3(net, train_iter, loss, updater)\rtest_acc = evaluate_accuracy(net, test_iter)\ranimator.add(epoch + 1, train_metrics + (test_acc,))\rtrain_loss, train_acc = train_metrics\rassert train_loss \u0026lt; 0.5, train_loss\rassert train_acc \u0026lt;= 1 and train_acc \u0026gt; 0.7, train_acc\rassert test_acc \u0026lt;= 1 and test_acc \u0026gt; 0.7, test_acc ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/","section":"Docs","summary":"\u003ch2 class=\"relative group\"\u003e3.6.1 初始化模型参数 \r\n    \u003cdiv id=\"361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量\u003c/li\u003e\n\u003cli\u003e在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003e初始化权重w \r\n    \u003cdiv id=\"%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enum_inputs = 784\r\nnum_outputs = 10\r\n\r\nW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\r\nb = torch.zeros(num_outputs, requires_grad=True)\n\u003c/code\u003e\u003c/pre\u003e\r\n\r\n\u003ch2 class=\"relative group\"\u003e3.6.2 定义softmax操作 \r\n    \u003cdiv id=\"362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e实现softmax操作由三个步骤组成\u003c/li\u003e\n\u003cli\u003e对每个项求幂\u003c/li\u003e\n\u003cli\u003e对每一行求和，得到其规范化常数\u003c/li\u003e\n\u003cli\u003e每一行除以其规范化常数，保持结果的和为1\n$$softmax(X)\u003cem\u003e{ij}=\\frac{exp(X\u003c/em\u003e{ij})}{\\sum_kexp(X_{ik})}$$\u003c/li\u003e\n\u003cli\u003e分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自\u003ca href=\"https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29\" target=\"_blank\"\u003e统计物理学\u003c/a\u003e中一个模拟粒子群分布的方程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edef softmax(X):\r\n    X_exp = torch.exp(X)\r\n    partition = X_exp.sum(1, keepdim=True)\r\n    return X_exp / partition  # 这里应用了广播机制\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ekeepdim=True: 在进行张量操作时，保持原始张量的维度\u003c/p\u003e","title":"D2L 3.6 Implementation of softmax regression from scratch","type":"docs"},{"content":"在了解多层感知机前，需要先了解[[Perceptron 感知机]]\n4.1.1 隐藏层 #\r在[[3.1_LinearRegression#3.1.1.1 线性模型]]中我们描述了[[Affine Transformation 仿射变换]]，如一次函数一般是一种带有偏置项的线性变换 如果预测值在仿射变换后确实与输入数据有线性关系，那么这种方式确实够用 可是大部分情况下，仿射变换中的线性是一个很强的假设 4.1.1.1 线性模型可能会出错 #\r线性意味着单调假设，权重w在正的情况下，任何特征的增大都会导致模型输出的增大 E.X.\n如果我们试图预测一个人是否会偿还贷款，我们可以认为收入较高的申请人比收入较低的申请人更有可能偿还贷款 但上述例子只阐明了单调性而非线性 收入从0到5万会带来比100万到105万更大的还款可能性 在上例中，我们任然可以通过[[2.2 数据预处理]]的方式使线性更加合理，如对数化处理 但一个违反单调性的例子比如体温和死亡率的关系 对于体温高于37度的人来说，温度越高风险越高 而对于体温低于37度的人来说，温度越低风险就越低 这种情况也可以使用理37度的距离作为特征 分类问题，如对于猫狗分类问题，在位置（13，17）处像素强度进行添加，是否整个图像描绘狗的[[Likehood 似然]]会增加？ 这一评估标准注定会失败，如倒置图像后，类别依然保留 对于上面两个例子来说，猫狗的分类问题无法通过简单的预处理解决 对于[[深度神经网络]]，我们将使用隐藏层 4.1.1.2 在网络中加入隐藏层 #\r我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型 有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡\n4.1.1.3 从线性到非线性 #\r同之前的章节一样，我们通过矩阵\\(X\\in R^{n\\times d}\\)来表示n个样本的小批量，其中每个样本具有d个输入特征\n对于具有h个隐藏单元的单隐藏层多层感知机，用\\(H\\in R^{n\\times h}\\)表示隐藏层的输出，称为Hidden Representatiosn 隐藏表示\n因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重\\(W^{(1)}\\in R^{R\\times h}\\)和隐藏层偏置\\(b^{(1)}\\in R^{1\\times h}\\)以及输出层\\(W^{(2)}\\in R^{h\\times q}\\)和输出层偏置\\(b^{(2)}\\in R^{1\\times q}\\)\n所以形式上，对于单隐藏层的多层感知机的输出\\(O\\in R^{n\\times q}\\)，有 $$\\begin{align} \\ H=WX^{(1)}+b^{(1)} \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n现阶段，隐藏层为输入层的放射函数，而输出层为隐藏层的放射函数，即$$O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW+b$$\n注意到现在在多层感知机的单隐藏层下，模型依然只做到了线性的放射函数\n所以为了发挥多层架构的潜力，我们需要添加一个额外的关键要素：[[Activation Function 激活函数]]，激活函数的输出则称为Activations 活性值\n一般来说，有了激活函数，模型就不会退化成线性模型 $$\\begin{align} \\ H=\\sigma(XW^{(1)}+b^{(1)}) \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，从而产生更有表达能力的模型\n4.1.1.4 通用近似定理 #\r多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的 而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论 4.1.2 激活函数 Activation Function #\r[[Activation Function 激活函数]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/","section":"Docs","summary":"\u003cp\u003e在了解多层感知机前，需要先了解[[Perceptron 感知机]]\u003c/p\u003e","title":"D2L 4.1 MultilayerPerceptron","type":"docs"},{"content":"深度学习的目的是发现Pattern，即做到模型的Generalization 泛化\n[[Overfitting Problem]] 原因很简单：当我们将来部署该模型时，模型需要判断从未见过的患者。 只有当模型真正发现了一种泛化模式时，才会作出有效的预测\n困难在于，当我们训练模型时，我们只能访问数据中的小部分样本。 最大的公开图像数据集包含大约一百万张图像。 而在大部分时候，我们只能从数千或数万个数据样本中学习。 在大型医院系统中，我们可能会访问数十万份医疗记录。 当我们使用有限的样本时，可能会遇到这样的问题： 当收集到更多的数据时，会发现之前找到的明显关系并不成立。\nOverfitting 过拟合 #\r模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合 左： Underfitting 欠拟合 中：拟合 右：Overfitting 过拟合 Regularization 正则化 #\r对抗过拟合的技术称为正则化 [[Regularization 正则化]] 4.4.1 训练误差和泛化误差 #\rTraining Error 训练误差 #\r模型在训练数据集上计算得到的误差 Generalization Error 泛化误差 #\r同样分布样本的无限多个数据的模型误差期望 但问题是对于无限多个数据，我们不可能准确的计算出Genrelization Errorz 4.4.1.1 统计学习理论 #\r我们假设训练数据和测试数据都是从相同的分布中独立提取的。 这通常被称为_独立同分布假设_（i.i.d. assumption） 4.4.1.2 模型复杂性 #\r一个模型的复杂性取决于很多因素 如模型参数，取值范围 Early Stopping 早停 #\r早停（Early Stopping）：这是一种防止过拟合的技术，其中训练过程在验证集上的性能开始恶化时停止。这意味着，如果模型在验证集上的误差开始增加，表明模型可能开始过拟合训练数据，此时停止进一步训练可以避免这种情况。 4.4.2 模型选择 #\r在一个训练中，我们会选择几个候选模型对他们进行评估 4.4.2.1 验证集 #\r训练集，验证集，测试集分别是什么_训练集 验证集 测试集-CSDN博客\n总的来说，对于Superivised Training，一般讲整体划为3个区块 Training Set 训练集 #\r训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数 训练集中的参数直接参与到梯度下降中 Validation Set 验证集 #\rz\n而验证集用于模型的选择，更具体地来说，验证集并不参与学习参数的确定，也就是验证集并没有参与梯度下降的过程 验证集只是为了选择超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数 Test Set 测试集 #\r测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型的评价 4.4.2.2 K-Fold Cross-Validation K折交叉验证 #\r训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集 过程描述 #\r数据分组：首先，整个数据集被随机分成K个大小大致相同的子集。 迭代训练与验证：每次迭代中，选择其中一个子集作为验证集，而其余的K-1个子集合并作为训练集。 性能评估：模型在训练集上训练，并在验证集上进行评估。这个过程重复K次，每次选择不同的子集作为验证集。 平均性能：最终模型的性能是所有K次迭代中验证性能的平均值。这样可以更全面地评估模型的性能。 4.4.3 欠拟合还是过拟合？ #\rGenerlization Error高的模型叫做Underfitting Train Error远低于Validation Error的模型叫做Overfitting 4.4.3.1 模型复杂性 #\r![[Pasted image 20240615153938.png]]\n简单来说，从左到右模型经历了从欠拟合到过拟合的一个过程，也是从高损失到高方差的过程 其是因为模型从没学习过参数到对于微小参数（甚至是随机噪声）严重敏感的一个过程 Lost 损失 #\r定义：偏差是指模型在预测中的系统误差，即模型对学习数据的一般性质的理解程度。 高偏差：通常表示模型过于简单（欠拟合），未能捕捉到数据的关键结构，通常会导致在训练集和测试集上都表现不佳。 Variance 方差 #\r定义：方差是指模型对于训练数据的微小变化的敏感度。 高方差：表示模型过于复杂（过拟合），对训练数据中的随机噪声也进行了学习，这可能使得模型在新的、未见过的数据上表现不佳。 4.4.3.2 数据集大小 #\r训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合 而样本更过通常会减小Gerneralization Error 一般来说，更多的数据不会有什么坏处 4.4.4 多项式回归 #\r拟合一个多项式\n[[4.4 Overfitting Normal \u0026amp; Underfitting - Pytorch]]\n4.4.4.1 生成数据集 #\r![[Pasted image 20240616094316.png]]\n噪声值位均值0到标准差0.1的正态分布 在优化的过程中，我们通常希望避免非常大的梯度值或损失值。 这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因 max_degree = 20 # 多项式的最大阶数\rn_train, n_test = 100, 100 # 训练和测试数据集大小\rtrue_w = np.zeros(max_degree) # 分配大量的空间\rtrue_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\rfeatures = np.random.normal(size=(n_train + n_test, 1))\rnp.random.shuffle(features)\rpoly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\rfor i in range(max_degree):\rpoly_features[:, i] /= math.gamma(i + 1) # gamma(n)=(n-1)!\r# labels的维度:(n_train+n_test,)\rlabels = np.dot(poly_features, true_w)\rlabels += np.random.normal(scale=0.1, size=labels.shape) max_degree = 20 : 即使多项式仅为三阶，但我们需要用一个20纬的多项式去拟合它，这是复杂模型中的一种 features = np.random.normal(size=(n_train + n_test, 1)): 分配200个一维的特征 np.random.shuffle(features): 随机打乱数据 poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))：分配每个特征的高阶数据 使用伽马正则化防止特征的迅速增大 最后点乘特征和真实权重得到Label，并将Label加上合适的噪声 # NumPy ndarray转换为tensor\rtrue_w, features, poly_features, labels = [torch.tensor(x, dtype=\rtorch.float32) for x in [true_w, features, poly_features, labels]]\rfeatures[:2], poly_features[:2, :], labels[:2] 转化为tensor def evaluate_loss(net, data_iter, loss): #@save\r\u0026#34;\u0026#34;\u0026#34;评估给定数据集上模型的损失\u0026#34;\u0026#34;\u0026#34;\rmetric = d2l.Accumulator(2) # 损失的总和,样本数量\rfor X, y in data_iter:\rout = net(X)\ry = y.reshape(out.shape)\rl = loss(out, y)\rmetric.add(l.sum(), l.numel())\rreturn metric[0] / metric[1] def train(train_features, test_features, train_labels, test_labels,\rnum_epochs=400):\rloss = nn.MSELoss(reduction=\u0026#39;none\u0026#39;)\rinput_shape = train_features.shape[-1]\r# 不设置偏置，因为我们已经在多项式中实现了它\rnet = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\rbatch_size = min(10, train_labels.shape[0])\rtrain_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\rbatch_size)\rtest_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\rbatch_size, is_train=False)\rtrainer = torch.optim.SGD(net.parameters(), lr=0.01)\ranimator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, yscale=\u0026#39;log\u0026#39;,\rxlim=[1, num_epochs], ylim=[1e-3, 1e2],\rlegend=[\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;])\rfor epoch in range(num_epochs):\rd2l.train_epoch_ch3(net, train_iter, loss, trainer)\rif epoch == 0 or (epoch + 1) % 20 == 0:\ranimator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\revaluate_loss(net, test_iter, loss)))\rprint(\u0026#39;weight:\u0026#39;, net[0].weight.data.numpy() 欠拟合 #\r# 从多项式特征中选择前2个维度，即1和x\rtrain(poly_features[:n_train, :2], poly_features[n_train:, :2],\rlabels[:n_train], labels[n_train:]) 只给予了前两个特征值 ![[Pasted image 20240704160340.png]] 过拟合 #\r# 从多项式特征中选取所有维度\rtrain(poly_features[:n_train, :], poly_features[n_train:, :],\rlabels[:n_train], labels[n_train:], num_epochs=1500) 将w中的20列全部给到了模型导致了过拟合 ![[Pasted image 20240704160346.png]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/","section":"Docs","summary":"\u003cp\u003e深度学习的目的是发现Pattern，即做到模型的Generalization 泛化\u003c/p\u003e","title":"D2L 4.4 Model Selection, Underfitting, and Overfitting","type":"docs"},{"content":" ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/","section":"Docs","summary":"\u003chr\u003e","title":"Chapter 3. Linear Neural Network","type":"docs"},{"content":" ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/","section":"Docs","summary":"\u003chr\u003e","title":"Chapter 4. Multilayer Perceptron","type":"docs"},{"content":"《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. https://zh-v2.d2l.ai/index.html\n‌ #\r","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/","section":"Docs","summary":"\u003cp\u003e《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. \u003ca href=\"https://zh-v2.d2l.ai/index.html\" target=\"_blank\"\u003ehttps://zh-v2.d2l.ai/index.html\u003c/a\u003e\u003c/p\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e‌ \r\n    \u003cdiv id=\"heading\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#heading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e","title":"Dive Into Deep Learning","type":"docs"},{"content":"","date":"Dec 27 2021","externalUrl":null,"permalink":"/tags/wangyiyun/","section":"Tags","summary":"","title":"Wangyiyun","type":"tags"},{"content":"\r","date":"Dec 27 2021","externalUrl":null,"permalink":"/blogs/wangyiyun/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/wangyiyun/1_hu17702828048897762413.jpg 330w,\r\n        /blogs/wangyiyun/1_hu360113685819379410.jpg 660w,\r\n        /blogs/wangyiyun/1_hu13608319589208325737.jpg 1024w,\r\n        /blogs/wangyiyun/1_hu6717855714386122734.jpg 2x\"\r\n        src=\"/blogs/wangyiyun/1_hu360113685819379410.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"网易云年度总结","type":"blogs"},{"content":"","date":"Dec 26 2021","externalUrl":null,"permalink":"/tags/shanghai/","section":"Tags","summary":"","title":"Shanghai","type":"tags"},{"content":"\r","date":"Dec 26 2021","externalUrl":null,"permalink":"/blogs/theband/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/theband/1_hu12727092779758697027.jpg 330w,\r\n        /blogs/theband/1_hu6728190700059376434.jpg 660w,\r\n        /blogs/theband/1_hu5333335250471856356.jpg 1024w,\r\n        /blogs/theband/1_hu8413573821294788432.jpg 2x\"\r\n        src=\"/blogs/theband/1_hu6728190700059376434.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"外滩2021","type":"blogs"},{"content":"\r","date":"Nov 1 2021","externalUrl":null,"permalink":"/blogs/desk2021/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FoxCsgo/FoxCsgo-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"2021 桌搭","type":"blogs"},{"content":"","date":"Nov 1 2021","externalUrl":null,"permalink":"/tags/csgp/","section":"Tags","summary":"","title":"Csgp","type":"tags"},{"content":"\r","date":"Oct 12 2021","externalUrl":null,"permalink":"/blogs/5e600/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/5e600/1_hu9666354151374759163.jpg 330w,\r\n        /blogs/5e600/1_hu16441751272499849929.jpg 660w,\r\n        /blogs/5e600/1_hu13471086248097805267.jpg 1024w,\r\n        /blogs/5e600/1_hu2577903920060232791.jpg 2x\"\r\n        src=\"/blogs/5e600/1_hu16441751272499849929.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"5e 600分对决","type":"blogs"},{"content":"","date":"Oct 12 2021","externalUrl":null,"permalink":"/tags/desk/","section":"Tags","summary":"","title":"Desk","type":"tags"},{"content":"\r","date":"Sep 3 2021","externalUrl":null,"permalink":"/blogs/foxcsgo/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FoxCsgo/FoxCsgo-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"沙漠之狐","type":"blogs"},{"content":"","date":"Apr 28 2020","externalUrl":null,"permalink":"/tags/qiuqiu/","section":"Tags","summary":"","title":"Qiuqiu","type":"tags"},{"content":"","date":"Apr 28 2020","externalUrl":null,"permalink":"/tags/zhuzi/","section":"Tags","summary":"","title":"Zhuzi","type":"tags"},{"content":"\r","date":"Apr 28 2020","externalUrl":null,"permalink":"/blogs/qiuqiu/young/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Qiuqiu/Young/Young-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"小时候","type":"blogs"},{"content":"\r","date":"Apr 28 2020","externalUrl":null,"permalink":"/blogs/zhuzi/young/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-2.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-3.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-4.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-5.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"小时候","type":"blogs"},{"content":"","date":"Jan 25 2020","externalUrl":null,"permalink":"/blogs/qiuqiu/","section":"Blogs","summary":"","title":"球球","type":"blogs"},{"content":"\r","date":"Dec 21 2018","externalUrl":null,"permalink":"/blogs/fundationchristams/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FundationChristams/FundationChristams-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Fundation Christams","type":"blogs"},{"content":"","date":"Dec 21 2018","externalUrl":null,"permalink":"/blogs/zhuzi/","section":"Blogs","summary":"","title":"竹子","type":"blogs"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Special thanks to those for thier invaluable contributions\n","externalUrl":null,"permalink":"/credits/","section":"Credits","summary":"\u003cp\u003eSpecial thanks to those for thier invaluable contributions\u003c/p\u003e","title":"Credits","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/faker/","section":"Credits","summary":"","title":"Faker","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/ss/","section":"Credits","summary":"","title":"SS","type":"credits"},{"content":"123\nRecord some thinking\n","externalUrl":null,"permalink":"/thoughts/thethreebodyproblem/","section":"Thoughts","summary":"\u003cp\u003e123\u003c/p\u003e\n\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"The Three Body Problem","type":"thoughts"},{"content":"","externalUrl":null,"permalink":"/thoughts/","section":"Thoughts","summary":"","title":"Thoughts","type":"thoughts"},{"content":"","externalUrl":null,"permalink":"/credits/%E6%B3%95%E8%80%81/","section":"Credits","summary":"","title":"法老","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/%E9%82%93%E7%B4%AB%E6%A3%8B/","section":"Credits","summary":"","title":"邓紫棋","type":"credits"}]