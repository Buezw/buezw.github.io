


[{"content":"","date":"Nov 19 2024","externalUrl":null,"permalink":"/","section":"Buezwqwg","summary":"","title":"Buezwqwg","type":"page"},{"content":"","date":"Nov 19 2024","externalUrl":null,"permalink":"/tags/docs/","section":"Tags","summary":"","title":"Docs","type":"tags"},{"content":"","date":"Nov 19 2024","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"","date":"Nov 19 2024","externalUrl":null,"permalink":"/tags/la/","section":"Tags","summary":"","title":"LA","type":"tags"},{"content":" Last Edit: 11/19/24\nSolution 解 #\r对于一个Vector \\(a=[1,2]^T\\)和一个直线\\(y=0\\) 要研究\\(c[1,2]^T=0\\)的问题的时候，很明显不存在Non-Trivial Solution 由于a在\\(R^2\\)中，而\\([1,0]^T\\)仅Span出了\\(R^2\\)中的一个Subspace，其Dim=1 Least Error Solution (Optimization) 最优解 #\r但是对于a到直线的距离仍存在Optimized Solution 最优解出现在\\([1,2]^T\\)的终点在\\([1,0]\\)方向上最短的情况，即一个Error最小的情况 可以从a出发找到无数个到达向量\\([1,0]^T\\)方向的向量 而其中最短的则是\\(\\vec {e}\\) 也可以说\\(\\vec e\\)是Equation Error最小的Solution R^3 Case #\r对于向量\\([1,1,3]^T\\)来说，要计算其到达平面\\(x+y-2z=0\\)的最短距离 \\(\\vec e\\) 则代表了这一个距离 则e的起点在Plane\\(x+y-2z=0\\)上的位置就是这一个最优解 Projection 投影 #\r可以发现，要找到最优解，一个合理的办法是从Projection开始 在上图中p就是a在\\([1,0]^T\\)方向上的投影 则有\\(e=b-p\\) 而最小化这个e就是目标，这个目标通过Orthogonal 正交实现 具体来说从A出发的orthogonal to p的vector e就是这个Optimized Solution \\(R^3\\)中同理，只不过是将投影的改为了Plane 于是便有\\(e=(b-A\\hat x)\\) 要让e垂直于Plane \\(A=[a1,a2]\\) 有\\(a_1^T(b-A\\hat x )=0\\)并且\\(a_2^T(b-A\\hat x )=0\\) 于是可以得到公式 $$A^T(b-A\\hat x)=0\\Rightarrow A^TA\\hat x=A^Tb$$ Least Squares 最小二乘 #\r直接进入例子 对于三个点{(1,1), (2,2), (3,2)} 构建方程\\(y=wx\\) 带入点后得到\\(1=w,2=2w,2=3w\\) 通过\\(A^TA\\hat x=A^Tb\\) $$A^T A = \\begin{bmatrix}\r1 \u0026 2 \u0026 3\r\\end{bmatrix}\r\\begin{bmatrix}\r1 \\\\\r2 \\\\\r3\r\\end{bmatrix}\r= 1^2 + 2^2 + 3^2\r= 1 + 4 + 9\r= 14$$\r$$A^T b = \\begin{bmatrix}\r1 \u0026 2 \u0026 3\r\\end{bmatrix}\r\\begin{bmatrix}\r1 \\\\\r2 \\\\\r2\r\\end{bmatrix}\r= 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 2\r= 1 + 4 + 6\r= 11\r$$\r便有\\(14w=11\\Rightarrow w=\\frac{11}{14}\\) 几何角度 #\r那么上面的公式在几何空间中干的事就是 找到了这个红色的Vector，也就是最小的e 同理运用到最经典的\\(y=wx+b\\)也是一样 \\(1=w+b,2=2w+b,2=3w+b\\) $$A^T A = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 1 \u0026 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 1 \\\\ 2 \u0026 1 \\\\ 3 \u0026 1 \\end{bmatrix} = \\begin{bmatrix} 1^2 + 2^2 + 3^2 \u0026 1 + 2 + 3 \\\\ 1 + 2 + 3 \u0026 3 \\end{bmatrix} = \\begin{bmatrix} 14 \u0026 6 \\\\ 6 \u0026 3 \\end{bmatrix}$$\r$$A^T \\mathbf{b} = \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\\\ 1 \u0026 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 2 \\\\ 1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 5 \\end{bmatrix}$$\r$$\\begin{bmatrix} 14 \u0026 6 \\\\ 6 \u0026 3 \\end{bmatrix} \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 5 \\end{bmatrix} $$\r于是有\\(w=\\frac{1}{2},b=\\frac{2}{3}\\) 同理几何上找到了向量在Plane上的投影之间的最小Error 所以这就是\\(A^TA\\hat x=A^Tb\\) 在Linear Regression的作用 需要知道的是这个方法（Normal Equation）求得的是解析解，在一般在feature \u0026lt; 10000的时候采用，但是过程可能不可逆 ","date":"Nov 19 2024","externalUrl":null,"permalink":"/docs/linearalgebra/leastsquare/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/19/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eSolution 解 \r\n    \u003cdiv id=\"solution-%E8%A7%A3\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#solution-%E8%A7%A3\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e对于一个Vector \\(a=[1,2]^T\\)和一个直线\\(y=0\\)\u003c/li\u003e\n\u003cli\u003e要研究\\(c[1,2]^T=0\\)的问题的时候，很明显不存在Non-Trivial Solution\u003c/li\u003e\n\u003cli\u003e由于a在\\(R^2\\)中，而\\([1,0]^T\\)仅Span出了\\(R^2\\)中的一个Subspace，其Dim=1\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/LinearAlgebra_Static/LeastSquare/LeastSquare-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"LA Least Square","type":"docs"},{"content":"","date":"Nov 19 2024","externalUrl":null,"permalink":"/docs/linearalgebra/","section":"Docs","summary":"","title":"Linear Algebra","type":"docs"},{"content":"","date":"Nov 19 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"Nov 18 2024","externalUrl":null,"permalink":"/tags/chemistry/","section":"Tags","summary":"","title":"Chemistry","type":"tags"},{"content":"","date":"Nov 18 2024","externalUrl":null,"permalink":"/series/ecms/","section":"Series","summary":"","title":"ECMS","type":"series"},{"content":"","date":"Nov 18 2024","externalUrl":null,"permalink":"/tags/ecms/","section":"Tags","summary":"","title":"ECMS","type":"tags"},{"content":" Last Edit: 18/11/24\nLight #\r当我们讨论为什么像聚甲基丙烯酸甲酯（PMMA）这样的材料能够透明，而像玻璃态金属或单晶金属（如硅和镍基合金）则是不透明的时候，理解光的本质及其与材料的交互作用是至关重要的。 材料是否透明，很大程度上取决于其电子结构，这决定了它如何吸收光 PMMA #\rPMMA we were able to have a transparent polymer because PMMA is 100 % amorphous 在非晶态的材料中，没有晶体和非晶体区域之间的界限，这种界限在晶体材料中可能会散射光线。 PMMA中不存在这样的晶界，光线可以自由穿过，从而保持材料的透明性 Electromagnetic Spectrum 电磁光谱 #\r可见光只是在包含了radio waves, microwaves, infrared radiation, ultraviolet radiation, x-rays, and gamma rays在内的大量辐射光谱中的极小部分 在# Electromagnetic Spectrum中，光子是Electromagnetic Radiation的基本粒子单位，承载能量和信息，跨越不同wave length的电磁波，包括无线电波、微波、红外线、可见光、紫外线、X射线和伽马射线 Electromagnetic Radiation 电磁辐射 #\rPhotons 光子 #\r光子可以在空间中传播，不需要介质，是能量从一个地点传到另一个地点的方式 光的能量不是连续的，而是Quantized 量子化的 $$E = \\frac{hC}{\\lambda}$$\n\\(h~(Plank~ ~Constant) = 6.626×10^{−34}J⋅s\\) \\(c~(Light~speed)=3\\times 10^8 m\\cdot s^{-1}\\) \\(λ~(Wavelength)~m\\cdot s^{-1}\\) 其更加常见的形式为 $$E=hv$$ \\(ν\\) : frequency of the light in Hertz \\((1 Hz = \\frac1s)​\\) Photoelectric effect 光电效应 #\r由Albert Einstein于1905年提出 当光照射到金属表面时，光子将其能量传递给金属内的电子，如果这些光子的能量足够高，超过了金属的逸出功，电子就会被释放出金属表面 由于每个光子的能量由其频率决定，其能量和频率的关系则由公式\\(E=hv\\)给出 逸出功（Work Function）：逸出功是指电子从固体表面逃逸到真空中所需的最小能量。不同材料有不同的逸出功。 光电子（Photoelectron）：如果光子的能量高于逸出功，电子就会被释放，成为光电子 电子的动能（Kinetic Energy）：释放的光电子将具有一个最大动能，这可以通过 \\(Kmax=hf−ϕK_{\\text{max}}\\)来计算，其中 \\(\\phi\\) 是逸出功 Wave-Particle Duality 波粒二象性 #\r指物质（如光和电子）在某些情况下表现为波动性，而在其他情况下表现为粒子性。 Photoelectric effect展示了光的粒子性，而光的波动性则通过其他实验如双缝实验（Double-slit Experiment）得到证明 Electron volt 电子伏特（eV） #\r计算光子能量：使用的公式是 \\(E = \\frac{hc}{\\lambda}\\)， 650纳米的红激光，将其转换成米\\(650 \\times 10^{-9} \\text{ m}\\)，计算得到的能量是 \\(3.06 \\times 10^{-19}\\)焦耳 虽然结果是正确的，但由于在量子力学和粒子物理中常常处理非常小的能量数值，使用焦耳（Joules）单位可能会显得笨拙和难以理解 于是引入了Electron Volt的概念 电子伏特是基于电子通过1伏特电势差加速后获得的能量。一个电子伏特定义为 \\(1.602 \\times 10^{-19}\\) 焦耳 \\(E_{\\text{Red Photon}} = \\frac{3.06 \\times 10^{-19} , \\text{J}}{1.602 \\times 10^{-19} , \\text{J/eV}} = 1.91 , \\text{eV}\\) Atom 原子 #\r一个基本的Atom Structure可以概括如下 Most of the mass of an atom comes from the nucleus 原子核 Isotopes 同位素 #\rNumber of proton in nucleus决定了原子是什么元素 所以# of Protones也称为Atomic Number 原子序数 如Carbon-14, 写作\\(C_{14}\\) 对于6个Proton来说是Carbon，5个是Boron 硼，7个是Nitrogen 氮气 The Bohr Model of the Atom 原子的玻尔模型 #\r历史上出现了许多个不同的Atom Structure 第一个取得了重大进展的则是Bohr Model 玻尔模型的基本特征是中心核和轨道电子，电子与原子核以固定的距离运行 能量跃迁：电子可以通过吸收或释放一个量子（quanta）的能量，从一个轨道跳跃到另一个轨道。这个能量的量正好等于两个轨道之间的能量差 Limitation 局限性 #\r适用范围：波尔模型只能准确地计算具有一个电子的原子，如氢原子（H）、氦离子（He⁺）和双电离锂离子（Li²⁺）的行为 The Quantum-Mechanical Model of the Atom 原子的量子力学模型 #\r在现代量子力学模型中，我们需要使用Four quantum numbers来完全描述一个电子的状态 而在尼尔斯·波尔（Niels Bohr）的早期模型中，只使用了一个量子数 Principal Quantum Number 主量子数 (n) #\rDescribes the size of the electron orbit 描述了电子轨道的大小 可以是any integer value beginning at 1 不同的壳层代表电子与原子核的平均距离不同，能量也不同。 K shell 是当 n=1 时的电子层，这是最接近原子核的电子层，也是能量最低的壳层 L shell 是当 n=2时的电子层 M shell 是当 n=3时的电子层 Angular Momentum Quantum Number 角动量量子数 ℓ #\r角动量量子数（有时也称为方位量子数）描述了电子**轨道的形状 角动量量子数可以具有比主量子数小 0 到 1 的任何值, that is \\(ℓ=0,1,2,\u0026hellip;n−1\\) 当ℓ=0 时，轨道是球形的（s-轨道） s亚层可容纳 2 个电子 当l=1时，亚层是 p 轨道，形状是偶极形（类似哑铃）p亚层可容纳 6 个电子 当l=2时，亚层是 d 轨道，形状更加复杂，通常有四叶或更多叶的形状 d亚层可容纳 10 个电子 当l=3时，亚层是 f 轨道，形状更为复杂 f亚层可容纳 14 个电子 当 ℓ=1 时，轨道呈哑铃形（p-轨道） 更高的ℓ值对应更复杂的轨道形状 The Magnetic Quantum Number 磁量子数 ml #\r磁量子数主要用于指定原子轨道在三维空间中的取向，并与外部磁场中的能级分裂有关 Describes how many different ways each subshell can be orientated 磁量子数 mℓ​ 可以取从−ℓ到+ℓ的整数值，包括零，其中 ℓ是角动量量子数。这表示对于给定的角动量量子数，存在2ℓ+1个可能的磁量子数。 例如，如果电子处于p轨道ℓ=1，那么mℓ可以是 -1, 0, 或 +1，对应轨道在空间中的三个不同取向。 Spin Quantum Number 自旋量子数 (s) #\r自旋量子数ms并不直观地描述电子的物理自旋，因为电子实际上并不是在空间中围绕某个轴物理旋转。 将电子描述为“旋转”的说法可能会引起误解 量子数值：自旋量子数的可能值为\\(\\frac{1}{2}\\)​ 或\\(-\\frac{1}{2}\\)​，通常表示为电子的自旋向上（up-spin）和自旋向下（down-spin） Eletron‘s Spin 电子的自旋 #\r电子的“自旋”这一术语实际上并不指电子在空间中像地球绕其轴旋转那样的物理旋转 电子自旋是一种量子力学性质，表现为一种内在的角动量 尽管这个性质被称为“自旋”，但它并不涉及电子在任何可见的或传统意义上的物理旋转 Summarize #\r大概总结一下，对于一个电子，有四个变量能决定他的属性，也就是四个Quantum Numbers 这四个变量的由来是由Pauil Exclusion Principle规定的 即同一层级，同一轨道类型，同一轨道编号下最多存在两个电子 当四个Quantum Nubers 全部相同的时候，则两个电子一致 1. Principle Quantum Number 主量子数 #\r以电子所作在的电子层级区分 n = 1 or 2 or 3 or 4 or 5 2. Angular Momentum Qunatum 角量子数 #\r通过同一层级下的不同轨道（以形状）区分 n = 2 \u0026amp; l = 1 or 2 3. Magnetic Quantum Number 磁量子数 #\r同一层级下，同一类型（形状）轨道，不同轨道编号区分 n = 2 \u0026amp; l = 2 \u0026amp; m = -1 4. Spin Quantum Number 自旋量子数 #\r同一层级，同一轨道类型，同一轨道编号的不同电子 根据Pauil Exclusion Principle，同一层级，同一轨道类型，同一轨道编号下最多存在两个电子，其中一个上旋为1/2，下旋为-1/2 n = 2 \u0026amp; l = 2 \u0026amp; m = -1 \u0026amp; s = 1/2 Electron Configuration #\r但是想要描述一个元素的Electron Configuration，只需要描述到亚层就够了，每一个亚层都有它能存在的固定电子数 The Electron Configuration of Carbon #\r元素周期表中可以确认，我们的老朋友碳的原子序数为 6， 这意味着一个碳原子的原子核中有六个质子 因此，要构建一个中性碳原子，我们需要将六个电子放入原子核周围的外壳和子壳中 要注意1s中的1是Pricipal Quantum Number，s是Angular Momentum Quantum Number 而且一个亚层是可以包含2个电子的，所以一个n = 2中的 l =1 的亚层p可以有6个电子，写作\\(2p^6\\) 那么对于Carbon来说就有 $$1s^2 , 2s^2 , 2p^2$$ 有的时候为了省略，可以从该元素的前一个Nobel Gas的Electron Configuration开始写起 对于Carbon来说则是Helium $$[\\text{He}] 2s^2 , 2p^2 $$ When 4s is Closer Than 3d Writting Format #\r在上面的Electron Configuration里可以看到4s层的能量实际上是低于3d的，这主要是因为电子层级不是主要只依靠其Primary Quantum Number决定 但是在书写时统一按照了国际惯例，即按照Primary Quantum Number顺序书写 Physical Reason Under #\r尽管在填充电子时，4s能量低于3d（因此4s轨道先填满），但在元素离子化或化学反应中，4s电子往往更容易被移除。 Titanium #\r所以Ti的完整规定写法是 $$1s^2 2s^2 2p^6 3s^2 3p^6 3d^2 4s^2 ~ and ~ [\\text{Ar}] 3d^2 4s^2$$ 可以发现前一个的3d应该是10个，但只写了2个就到s了 Few Exceptions 特例 #\r在3d亚层处于半满或全满状态时，即4，9时 系统可以通过重新分配电子来达到更稳定的状态 Vanadium #\r$$1s^2 2s^2 2p^6 3s^2 3p^6 3d^3 4s^2$$\n可以发现3d亚层只有3个，不处于即将Half-Filled or Completely Filled的水平 Chromium #\r$$1s^2 2s^2 2p^6 3s^2 3p^6 3d^5 4s^1$$\n4s（小于3d的层级）的电子被3d拿去了，以达到了Half-Filled的水平 Copper #\r$$1s^2 2s^2 2p^6 3s^2 3p^6 3d^{10} 4s^1$$\n4s（小于3d的层级）的电子被3d拿去了，以达到了Full-Filled的水平 Octet Stability #\r\\(He=1s^2\\) \\(Ne=1s^2 2s^2 2p^6\\) \\(Ar=1s^2 2s^2 2p^6 3s^2 3p^6\\) \\(Kr=1s^2 2s^2 2p^6 3s^2 3p^6 4s^2 4p^6\\) 可以发现，由于Nobel Gas的电子构型使其具有八电子（octet）结构，遵循八隅规则 所以他们的Electron Configuration通常可以表示为\\(ns^2 np^6\\)的形式 Ionic Bond #\r离子键的形成过程 #\rCl的Atomic Number是17，其电子构型为\\(1s^2 2s^2 2p^6 3s^2 3p^5\\) 它缺一个电子就可以达到Octet的稳定结构 Na的Atomic Number是11，电子构型为\\(1s^2 2s^2 2p^6 3s^1\\) 或简写为 \\([Ne]3s^1\\) 它如果失去一个电子，也可以达到类似稀有气体的稳定构型 电子转移与离子形成 #\r钠会失去一个电子，形成带正电的钠离子（Na⁺）。 氯会接受一个电子，形成带负电的氯离子（Cl⁻）。 这种电子的转移使得钠和氯都达到了稳定的电子构型，形成了Ionic Bond 离子键的特性 #\r这种静电吸引力是non-directional，即在所有相邻的正负离子之间普遍存在，使得离子晶体结构非常稳定 在晶体中，所有电子都被紧密束缚在各自的离子中，不自由移动，因此固态的NaCldo not conduct electricity 图中显示了NaCl晶体的结构，红色小球表示Na⁺，蓝色大球表示Cl⁻。 黄色箭头表示离子间的静电吸引力。 由于正负离子的规则排列，晶体内每个离子都被周围的异性离子包围，形成稳定的晶格结构 Colvaent Bond #\rColvaent Bond涉及Electron Sharing，即原子通过共享价电子来达到稳定的Octet结构 甲烷（CH₄）是一个简单的例子：碳和氢通过共价键结合，形成一个稳定的分子。 共价键只在Specific Atoms形成，例如在CH4中，Carbon仅与四个Hydrogen Bond形成共价键，而不会与其他原子相连。这种键称为Directional Bond 共价键的本质区别在于电子共享，而离子键则是电子转移 Metallic Bonding #\r金属键通常用两种模型描述：sea-of-electrons model, and the band theory of solids Sea of Electrons Model #\r电子海模型中，Valence electrons不固定在特定的原子核上，而是自由移动，形成一个电子的“海洋”（sea of electrons） 要注意Inner electrons (non-valence electrons)是不在Electron Sea中的，因为他们并不参与反应 Ion Core周围的蓝色区域全是电子 Valence electrons 价电子 #\r原子最外层的电子，直接参与化学反应和形成化学键。它们决定了元素的化学性质，例如其反应性、与其他元素形成的键类型等 这些自由移动的电子使得金属具有Conductivity和延展性，因为电子可以在整个晶体中自由流动 Ion core 离子核 #\r在金属或其他离子化合物结构中，不参与化学键的原子核和内层电子的组合 Conductivity of Sea of Electrons Model 导电性\n模型中，electrons are free to move past the ion cores (or so-called delocalized) 离域化 自由移动的电子可以在金属内传导电流 相较于Covalent Bond来说，其被局限在特定原子之间，因此像聚合物这样的材料通常是电的绝缘体 Ductility #\r金属晶体受到足够大的应力时，一个原子平面可以滑过另一个原子平面 在金属中，原子是按照晶体结构排列的，周围有自由移动的电子（电子海） 当对金属施加较大的力（如拉伸力或剪切力）时，金属中的一个原子平面会滑动到另一个原子平面之上 即使发生滑动，由于电子海的存在，这些自由电子能够迅速重新分布并填补原子之间的空隙，从而保持金属的整体结构稳定，不会断裂 Ceramic #\r而对于Ionic \u0026amp; Covalent Bonding来说，由于其结构性，导致一旦发生了滑动，其负电荷会和负电荷处于同一平面导致Repulsion 在陶瓷材料中正负离子交替排列形成晶体结构。 当试图使一个原子层滑过另一个时，同性电荷的离子（例如两个正离子或两个负离子）会短暂靠近。 同性电荷靠近时会产生强烈的静电排斥力。 结果：导致了陶瓷在变形前就会发生脆性断裂。 Polymer #\rDuality #\r聚合物主要通过covalent bonds将分子内部连接，而分子间的连接靠次级键（如范德华力或氢键）。 在塑性变形中，Secondary Bonds被克服，聚合物分子链滑动，而共价键不会断裂。 结果：聚合物表现出较大的可变形性（如韧性），而不会像陶瓷那样容易断裂。 Conductivity #\r聚合物中，all of the valence electrons are tightly bound in the strong covalent bonds due to the lack of any free electrons 聚合物是electrically insulating Form of Crystal of Salt #\rI know that NaCl forms an ordered solid, but why?\n这是因为物质趋向于从things tend to proceed from higher energy to lower energy 当某些事情发生（如盐形成晶体）是因为这样的状态对能量来说是更“有利”的，也就是“lower energy” $$Na_{(s)} + \\frac{1}{2}Cl_{2(g)} \\rightarrow NaCl_{(s)}$$ 对于上面的反应，我们将其拆分成子反应 Sublimation of Sodium 钠的升华 #\r$$Na(s)​→Na(g)​$$\n固态钠\\(Na_{(s)}\\)直接转化为气态钠原子\\(Na_{(g)}\\)，称为升华（sublimation） 这种物质从固体转变为气体的过程需要能量，称为升华焓\\(\\Delta H_{\\text{sublimation}}\\)，对于钠为\\(ΔHsublimation​=109kJ/mol\\) 这里正值代表了：系统需要吸收能量便反应从左向右进行 也就是说，需要能量来熔化然后煮沸钠 Ionization of Sodium Atom 钠原子电离 #\r气态钠原子\\(Na_{(g)}\\)进一步被电离为钠离子\\(Na^+_{(g)}\\)和一个电子\\(e^-\\) $$Na(g)​→Na(g)^+​+e^−$$ Ionization energy is \\(IENa​=497kJ/mol\\) 可以发现这目前还是一个Posititve，则代表还需要吸收能量 Bond Dissociation of Chlorine Molecule 氯的解离 #\r$$\\frac{1}{​2}Cl_2(g)​→Cl(g)​$$\n\\(BDE_{Cl_2} = 121 , \\frac{kJ}{mol}\\) 这仍然是吸热过程（需要能量输入） Formation of a chlorine anion #\r$$Cl(g)​+e^−→Cl(g)^−​$$\n这一过程中的Electron Affinity为\\(EACl​=−364\\frac{mol}{kJ}​\\) 可以发现能量第一次变为了负的，这是第一个释放能量的步骤 Forming the ionic crystal #\r$$Na(g)+​Cl(g)^−​→NaCl(s)$$\n这一个过程包含了Crystallization energy\\(Ecrystallization​=−777\\frac{mol}{kJ}​\\) 可以发现这一步消耗了很多能量 如果我们将这些能量项中的每一个相加，形成 NaCl 的总能量变化为−414 虽然第一步是吸热的，但整个反应通过后续的强烈放热步骤补偿了这一点。整体反应的自由能变化 (ΔG\\Delta GΔG) 是负的，因而是自发的。这解释了为什么钠和氯最终可以自然形成盐晶体\nThe Band Theory 能带效应 #\r电子能级由于相互的排斥作用发生分裂\n在孤立的原子中，电子能量被量子化，存在于离散的能级中（如s,p,d,f轨道） 这些能级之间的能量差是固定的，不会受到其他原子的影响。 对于每个原子原本的一个能级，靠近后会产生多个稍微不同的能级。 例如：对于两个原子，一个能级会分裂成两个能级；对于N个原子，会分裂成N个能级 在固体中，原子之间的距离非常近，并且一个晶格中会有\\(10^{23}\\)个原子 原子的数量极其庞大时，原本分裂的离散能级数量非常多，且间距变得极其微小，最终看起来像是连续的能量区域——这就是能带（Energy Band） 原子越多，能带越“密集” 当对于一个原子，其存在多个能级，但当多个原子组合在一起的时候，电子的能量状态不再是离散的，而是形成了一个几乎连续的能量区域 孤立原子：电子有固定的、离散的能量（如轨道能级 s,p,d） 固体中：原子靠得很近，电子能级由于相互的排斥作用发生分裂。 分裂后的能量状态数量非常多，间隔非常小，看起来像是连续的，这就形成了能带 Bonding in Metals Like Copper #\r用Copper的Electron Configuration举例 $$Cu =1s^2 , 2s^2 , 2p^6 , 3s^2 , 3p^6 , 3d^{10} , 4s^1$$ 在4s中的两个Sublayer中只存在一个电子 Conductivity #\r导电的本质是低能量跃迁的累积：导电依赖于大量电子在价带和导带之间进行低能量的跃迁。如果3s电子要跃迁到4s或4p，势必要消耗更多的能量，而这在常温下不容易实现。因此，这些高能跃迁对导电贡献很小，甚至可以忽略不计。 所以说当一个轨道中存在Empty States的时候，Valence Electron才能在其中跃迁 Bonding in Metals Like Magnesium #\r对于Mg来说\\(Mg=1s^2 , 2s^2 , 2p^6 , 3s^2\\)，从表面看，3s轨道已经完全填满，因此看起来它不应该有可用的电子来参与导电 但是可以发现3p轨道是空的，但它并不是不可用的 这就像在剧院里，空座位虽然没有人坐，但仍然在那里，可以被占用 可以被占用。3s轨道和3p轨道的能级相互重叠，因此电子可以从3s轨道很容易地被激发到3p轨道 Bonding in Ceramics and Polymers #\r回想一下，Ceramic往往通过Ionic Bond结合在一起，而Ionic Bond涉及Electron在Atom间的转移 还要记住，这种电子转移的发生是为了让每个原子都能获得填充的Valence Shell 由于Valence Shell是填充的，因此没有紧邻填充态的电子能态。此外，这些电子与原子核紧密结合，因此没有自由电子 Polymer也是如此，只是他是Covalent Bond Valence Band 价带 #\rValence Band是指电子填满的最高能量带 拿Si举例，其Electron Configuration为\\(1s^22s^22p^63s^23p^2\\) 其Valence Band即为\\(3s^2\\)，3p虽然是最高能量带，但他并没有填充满 导电性：满带的电子不能自由流动，因为能量状态已经填满，没有空位供电子移动 Full Band满带 #\rFull Band是指电子完全填满的能量带。 在硅（Si）的例子中，\\(1s^2 2s^2 2p^6\\) 层被完全填满，这些内层电子构成了满带 这些满带主要是低能级的核心电子带，电子在这些带中被完全填满，无法参与到导电过程中 导电性：满带的电子由于能态完全填满，没有额外的空间或能级供电子跃迁，因此不参与导电。这些带在正常条件下对材料的导电性几乎没有贡献 Conduction Band 导带 #\r导带是指紧邻满带之上的未填满能量带 拿Si举例，其Electron Configuration为\\(1s^22s^22p^63s^23p^2\\) 其Valence Band即为\\(3p^2\\) 当电子被激发到导带后，它们可以在材料中自由移动，从而参与导电 Conduction Band通常是空的，或者仅有少量电子占据（在导体中可能存在部分填充） 导带中的电子是Valence electrons AKA Free Electrons，可以在材料中移动并产生电流 Simiconductor 半导体 #\r一些材料，即半导体，具有的Bond Gap没有绝缘体那么大 这很重要，因为这意味着我们可以控制这些材料中的电子流动。这是太阳能光伏、LED 照明和我们所有现代电子产品的基础 但大约 4 eV 通常是一个不错的数字。如果材料的带隙大于 4 eV，我们可以将其视为绝缘体，如果带隙小于 4 eV（但不为零） Conductors, Insulators, and Semiconductors #\r重新根据导电效率定义这三种的区别 从左到右依次为 conductors semiconductors and insulators Back To Light #\r可见光由光子能量在 2-3 eV 之间的光子组成 如果材料的带隙大于 3 eV（例如 SiO₂，二氧化硅），那么可见光光子的能量不足以激发电子从价带跃迁到导带。 结果：光子通过材料时不会被吸收，因此材料对可见光是透明的。 举例：玻璃主要由 SiO₂ 构成，因此玻璃是透明的。 Energy efficiency of the building #\r光穿过窗户进入室内会导致热量积聚，从而增加空调的能耗 解决方案：在窗户上镀金属薄层 金属薄层可以反射部分阳光（尤其是紫外线光子）。 如果金属层够薄，它仍然允许大部分可见光通过，同时减少紫外线和热量的传递。 优点：提高建筑的能源效率，降低室内过热问题。 Light \u0026amp; Metal #\r金属的特点：没有明显的带隙（导带和价带重叠） 结果1：光子容易激发电子：\n可见光光子的能量足够将金属中的自由电子激发到更高能级\n因此，金属吸收光，并且是不透明的\n结果2：金属的光泽（反射性）：\n被光子激发的电子会迅速返回到较低能量状态，在这个过程中重新发射光子\n这种现象导致金属表面反射光，从而看起来有光泽（“金属光泽”）\nSilicon 硅 #\r实验表明，每个硅原子会形成 four identical bonds 但是，根据电子配置，3s 和 3p 轨道的能量不同，这意味着它们的性质不同 可以推出结论“Our model is limited. It can\u0026rsquo;t explain bonding in silicon” sp3 Hybridization 轨道杂化理论 #\r这里提出的解决方法是这样的：让我们只拿一个s轨道和3p轨道并将它们混合在一起 Diamond Cubic #\r现在我们有四个等效的轨道，这些轨道的分布是对称的，彼此之间具有等价性 it wouldn\u0026rsquo;t make sense for, say, three of them to be clumped close to one another and one bond to be all alone 也就是说，它们在空间中的位置分布是均匀的 Tetrahedral Configuration #\rSemiconductors #\r能够控制半导体的导电性对我们来说很重要 可以通过将杂质引入Semiconductors中以改变其导电性 Intrinsic Semiconductors 本征半导体 #\r拿Silicon举例，其在3p轨道上，存在了4个Valence Electron 不同的Silicon则和其他的通过Covalent Bond组合成如下 Intrinsic Semiconductors一种纯净的半导体，没有杂质掺杂 在Absolute Zero的时候可以形成如上图的结构 Hole 空穴 #\r当温度上升了之后，Electrons会被Promoted进入Conduction Band导带 电子从Valence Band跃迁到Conduction Band后，会在原来的位置留下一个hole，即一个缺少电子的位置 其中，Electron和Hole空穴（电子缺失造成的正电荷）的数量是平衡的 没多一个Electron被promote后，都会留下对应的Hole，亦可说他们是成对出现的 种类型的半导体称为Intrinsic Semiconductors，因为所有可用于导电的东西都来自半导体本身，而不是我们添加到其中的任何东西 Intrinsic Semiconductors在实践中并不是特别有用，因为我们几乎总是添加杂质来控制Conductivity 电导率 Conductivity 电导率 #\r计算一个Simiconductor的 Conductivity的公式为 $$\\sigma = nq\\mu_n + pq\\mu_p $$ 而对于Intrinsic Semiconductors的特殊情况来说，由于存在Concentration of holes = Concentration of electons，于是就有 $$\\sigma = nq(\\mu_n + \\mu_p)$$ Concentration of electrons 电子浓度 n #\rSince electrons carry a negative charge 所以可以通过\\(\\frac{noofelectorns}{m^3}\\) Concentration of holes 空穴浓度 #\r在本征半导体中，电子浓度（n）和空穴浓度（p）是相同的 这是因为Intrinsic Semiconductors中的电子和空穴都是由相同数量的价带电子激发到导带产生的 因此，在热平衡状态下，电子的生成和复合是平衡的，所以电子浓度和空穴浓度相等 Mobility 迁移率 #\r对于Mobility来说存在Electron Mobility和Hole Mobility，分别通过\\(\\mu_n和\\mu_p\\)来表示 其单位为\\(\\frac{m^2}{V_s}\\) Charge 电荷 #\rCharge指的是Magnitude of the fundamental charge \\(1.602\\times 10^{-19}C\\) Extrinsic Semiconductors 外延半导体 #\r本征半导体并不是特别实用，因为我们通常会向半导体中添加Impurities（称为dopants）以仔细控制其Conductivity 我们向半导体中添加Small amount of dopant的Dopants时，掺杂剂引入的电导率会压倒任何本征半导体，因此我们称之为Extrinsic Semiconductors 基本上存在两种加入Dopants的方式，使用额外的Electron或者Hole的方法 由于电子是Negataive Charged的，将添加Electron的叫做n- type Semiconductor 而Hole是Positive的（虽然其是Neutral的，但由于Hole存在于Electron的中间，所以看上去是“Positive”的），所以添加Hole的叫做p-type Semiconductor Extrinsic n-Type Semiconductors 外本征n型半导体 #\r想要在Intrinsic Semiconductor中添加Extra Electrons，可以通过Zero Dimension Inpurity的Point Defects来添加Inpurities Atoms，而添加的这一个Atom会带来额外的电子 已知对于Silicon来说其存在4个Valence Electrons：\\(3s^23p^2\\)，一种合适的做法便是将位于元素周期表右侧的Atom加入，即一个存在5个Valence Electron的Atom，这便是P，磷 已知磷的Atomic Number为15，其Electron Configuration为\\(1s^2 2s^2 2p^6 3s^2 3p^3\\) 因此添加到硅晶格中时，当与相邻的硅原子形成四个共价键时，将有一个额外的电子踢来踢去 如上图所示，元素中出现了一个多余的电子，由于没有足够的周围硅原子来形成稳定的共价键，因此这个电子不像其他共价键中的电子那样稳定地束缚 Donor Level供体能级 #\r在Gap Band内，接近导带底部的蓝色线表示磷原子提供的额外电子的能级 这个能级非常接近导带，因此只需很少的能量就可以将电子激发到导带中。 由于Impurity P为Simiconductor贡献的 Charge carriers，亦或者说是价带比通过本征促进产生的电荷载流子多得多 因此我们可以忽略\\(\\mu_p\\)，只用电子浓度和迁移率来计算 n 型半导体的电导率 $$σ_{n−type​}=nqμ_n​$$ Extrinsic p-Type Semiconductors #\r同理对于Silicon来说，选他左边的元素，B 在Extrinsic p-Type Semiconductors中，电子不需要被激发到Conduction Band才能导电 相反，价带中的电子可以轻易被激发到B提供的一个Hole上，从而填补那里的空穴，也就是Bnad Diagram上的Acceptor Level 同理可以忽略\\(\\mu_n\\)的大小 $$σ_{p−type}​=pqμ_p​$$ Solid Ionic Conductivity #\r当我们思考和谈论固体材料中的导电性时，我们会考虑电子的运动，就半导体而言，还会考虑“空穴”的运动 然而，导电性也可以通过Solid Ionic内的运动来实现 ","date":"Nov 18 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms7.lightquantum/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 18/11/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eLight \r\n    \u003cdiv id=\"light\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#light\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e当我们讨论为什么像聚甲基丙烯酸甲酯（PMMA）这样的材料能够透明，而像玻璃态金属或单晶金属（如硅和镍基合金）则是不透明的时候，理解光的本质及其与材料的交互作用是至关重要的。\u003c/li\u003e\n\u003cli\u003e材料是否透明，很大程度上取决于其电子结构，这决定了它如何吸收光\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003ePMMA \r\n    \u003cdiv id=\"pmma\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#pmma\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003ePMMA we were able to have a transparent polymer because PMMA is 100 % amorphous\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS7.Light\u0026amp;Quantum/ECMS7.LIGHT.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 7. Light and Quantum","type":"docs"},{"content":"\rYour browser does not support the video tag.\rfrom manim import *\rimport numpy as np\rimport torch\rimport random\rclass LinearRegression(Scene):\rdef construct(self):\rdef data_generator(w, b, num):\rX = torch.normal(0, 1, (num, len(w)))\ry = torch.matmul(X, w) + b\ry += torch.normal(0, 0.01, y.shape)\rreturn X, y.reshape((-1, 1))\rtrue_w = torch.tensor([2, -3.4])\rtrue_b = 4.2\rfeatures, labels = data_generator(true_w, true_b, 1000)\rhead = Text(\u0026#34;Linear Regression Display - Buezwqwg\u0026#34;)\rhead.set_color(BLUE)\rself.play(Create(head))\rself.wait(1)\rself.play(Uncreate(head))\rfeature_one = features[:, [0]].tolist()\rfeature_two = features[:, [1]].tolist()\rlabels_list = labels.tolist()\raxes_1 = Axes(\rx_range=[min(feature_one)[0]-1, max(feature_one)[0]+1, 1],\ry_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\rx_length=5,\ry_length=5,\raxis_config={\u0026#34;color\u0026#34;: BLUE},\r)\raxes_2 = Axes(\rx_range=[min(feature_two)[0]-1, max(feature_two)[0]+1, 1],\ry_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\rx_length=5,\ry_length=5,\raxis_config={\u0026#34;color\u0026#34;: BLUE},\r)\raxes = VGroup(axes_1, axes_2)\raxes.arrange(RIGHT, buff=1)\rself.play(Create(axes))\rpoints_1 = []\rfor i in range(len(labels_list)):\rdot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\rpoints_1.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\rpoints_2 = []\rfor i in range(len(labels_list)):\rdot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\rpoints_2.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\rpoints_group_1 = VGroup(*points_1)\rpoints_group_2 = VGroup(*points_2)\rself.play(Create(points_group_1), Create(points_group_2))\r# Draw the true model lines\rtrue_line_1 = axes_1.plot(lambda x: true_w[0].item() * x + true_b, color=GREEN)\rtrue_line_2 = axes_2.plot(lambda x: true_w[1].item() * x + true_b, color=GREEN)\rself.play(Create(true_line_1), Create(true_line_2))\r# Initialize parameters\rw = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\rb = torch.zeros(1, requires_grad=True)\rw_1 = w.detach().numpy()[0][0]\rw_2 = w.detach().numpy()[1][0]\rb_0 = b.detach().numpy()[0]\rline_1 = axes_1.plot(lambda x: w_1 * x + b_0, color=BLUE)\rline_2 = axes_2.plot(lambda x: w_2 * x + b_0, color=BLUE)\rself.play(Create(line_1), Create(line_2))\r# Add text to display loss, w, b\rloss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {0:.4f}\u0026#34;)\rloss_text.to_edge(UP)\rw_text = MathTex(f\u0026#34;w_1 = {w_1:.4f},\\\\ w_2 = {w_2:.4f}\u0026#34;)\rw_text.next_to(loss_text, DOWN)\rb_text = MathTex(f\u0026#34;b = {b_0:.4f}\u0026#34;)\rb_text.next_to(w_text, DOWN)\rparam_text = VGroup(loss_text, w_text, b_text)\rself.play(Write(param_text))\rdef data_iter(batch_size, features, labels):\rnum = len(features)\rindex = list(range(num))\rrandom.shuffle(index)\rfor i in range(0, num, batch_size):\rbatch_index = torch.tensor(index[i:min(i+batch_size, num)])\ryield features[batch_index], labels[batch_index]\rbatch_size = 10\rdef linreg(X, w, b):\rreturn torch.matmul(X, w) + b\rdef squared_loss(y_hat, y):\rreturn (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\rdef sgd(params, lr, batch_size):\rwith torch.no_grad():\rfor param in params:\rparam -= lr * param.grad / batch_size\rparam.grad.zero_()\rlr = 0.03\rnum_epochs = 3 # Changed to 3 epochs\rnet = linreg\rloss = squared_loss\rupdate_interval = 1 # Update after every batch\rbatch_counter = 0\r# Start training and update after every backpropagation\rfor epoch in range(num_epochs):\rfor X, y in data_iter(batch_size, features, labels):\rl = loss(net(X, w, b), y)\rl.sum().backward()\rsgd([w, b], lr, batch_size)\rbatch_counter += 1\r# Get the latest parameters\rnew_w_1 = w.detach().numpy()[0][0]\rnew_w_2 = w.detach().numpy()[1][0]\rnew_b_0 = b.detach().numpy()[0]\r# Update lines\rnew_line_1 = axes_1.plot(lambda x: new_w_1 * x + new_b_0, color=BLUE)\rnew_line_2 = axes_2.plot(lambda x: new_w_2 * x + new_b_0, color=BLUE)\rself.play(\rTransform(line_1, new_line_1),\rTransform(line_2, new_line_2),\rrun_time=0.1 # Adjusted animation time for smoother update\r)\r# Update loss and parameter displays\rwith torch.no_grad():\rtrain_l = loss(net(features, w, b), labels)\rcurrent_loss = float(train_l.mean())\rnew_loss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {current_loss:.4f}\u0026#34;)\rnew_loss_text.to_edge(UP)\rnew_w_text = MathTex(f\u0026#34;w_1 = {new_w_1:.4f},\\\\ w_2 = {new_w_2:.4f}\u0026#34;)\rnew_w_text.next_to(new_loss_text, DOWN)\rnew_b_text = MathTex(f\u0026#34;b = {new_b_0:.4f}\u0026#34;)\rnew_b_text.next_to(w_text, DOWN)\rself.play(\rTransform(loss_text, new_loss_text),\rTransform(w_text, new_w_text),\rTransform(b_text, new_b_text),\rrun_time=0.1\r)\r# Output current epoch\u0026#39;s loss\rprint(f\u0026#39;epoch {epoch + 1}, loss {current_loss:f}\u0026#39;)\rself.wait(2) ","date":"Nov 18 2024","externalUrl":null,"permalink":"/docs/linearregression_display/","section":"Docs","summary":"\u003cvideo width=\"640\" height=\"360\" controls\u003e\r\n  \u003csource src=\"LG_Display.mp4\" type=\"video/mp4\"\u003e\r\n  Your browser does not support the video tag.\r\n\u003c/video\u003e\r\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003efrom manim import *\r\nimport numpy as np\r\nimport torch\r\nimport random\r\n\r\nclass LinearRegression(Scene):\r\n    def construct(self):\r\n        def data_generator(w, b, num):\r\n            X = torch.normal(0, 1, (num, len(w)))\r\n            y = torch.matmul(X, w) + b\r\n            y += torch.normal(0, 0.01, y.shape)\r\n            return X, y.reshape((-1, 1))\r\n\r\n        true_w = torch.tensor([2, -3.4])\r\n        true_b = 4.2\r\n        features, labels = data_generator(true_w, true_b, 1000)\r\n\r\n        head = Text(\u0026#34;Linear Regression Display - Buezwqwg\u0026#34;)\r\n        head.set_color(BLUE)\r\n        self.play(Create(head))\r\n        self.wait(1)\r\n        self.play(Uncreate(head))\r\n\r\n        feature_one = features[:, [0]].tolist()\r\n        feature_two = features[:, [1]].tolist()\r\n        labels_list = labels.tolist()\r\n        axes_1 = Axes(\r\n            x_range=[min(feature_one)[0]-1, max(feature_one)[0]+1, 1],\r\n            y_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\r\n            x_length=5,\r\n            y_length=5,\r\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\r\n        )\r\n        axes_2 = Axes(\r\n            x_range=[min(feature_two)[0]-1, max(feature_two)[0]+1, 1],\r\n            y_range=[min(labels_list)[0]-1, max(labels_list)[0]+1, 5],\r\n            x_length=5,\r\n            y_length=5,\r\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\r\n        )\r\n        axes = VGroup(axes_1, axes_2)\r\n        axes.arrange(RIGHT, buff=1)\r\n        self.play(Create(axes))\r\n\r\n        points_1 = []\r\n        for i in range(len(labels_list)):\r\n            dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\r\n            points_1.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\r\n        points_2 = []\r\n        for i in range(len(labels_list)):\r\n            dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\r\n            points_2.append(Dot(point=dot_position, radius=0.03, color=YELLOW))\r\n        points_group_1 = VGroup(*points_1)\r\n        points_group_2 = VGroup(*points_2)\r\n        self.play(Create(points_group_1), Create(points_group_2))\r\n\r\n        # Draw the true model lines\r\n        true_line_1 = axes_1.plot(lambda x: true_w[0].item() * x + true_b, color=GREEN)\r\n        true_line_2 = axes_2.plot(lambda x: true_w[1].item() * x + true_b, color=GREEN)\r\n        self.play(Create(true_line_1), Create(true_line_2))\r\n\r\n        # Initialize parameters\r\n        w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\r\n        b = torch.zeros(1, requires_grad=True)\r\n        w_1 = w.detach().numpy()[0][0]\r\n        w_2 = w.detach().numpy()[1][0]\r\n        b_0 = b.detach().numpy()[0]\r\n        line_1 = axes_1.plot(lambda x: w_1 * x + b_0, color=BLUE)\r\n        line_2 = axes_2.plot(lambda x: w_2 * x + b_0, color=BLUE)\r\n        self.play(Create(line_1), Create(line_2))\r\n\r\n        # Add text to display loss, w, b\r\n        loss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {0:.4f}\u0026#34;)\r\n        loss_text.to_edge(UP)\r\n        w_text = MathTex(f\u0026#34;w_1 = {w_1:.4f},\\\\ w_2 = {w_2:.4f}\u0026#34;)\r\n        w_text.next_to(loss_text, DOWN)\r\n        b_text = MathTex(f\u0026#34;b = {b_0:.4f}\u0026#34;)\r\n        b_text.next_to(w_text, DOWN)\r\n        param_text = VGroup(loss_text, w_text, b_text)\r\n        self.play(Write(param_text))\r\n\r\n        def data_iter(batch_size, features, labels):\r\n            num = len(features)\r\n            index = list(range(num))\r\n            random.shuffle(index)\r\n            for i in range(0, num, batch_size):\r\n                batch_index = torch.tensor(index[i:min(i+batch_size, num)])\r\n                yield features[batch_index], labels[batch_index]\r\n\r\n        batch_size = 10\r\n\r\n        def linreg(X, w, b):\r\n            return torch.matmul(X, w) + b\r\n\r\n        def squared_loss(y_hat, y):\r\n            return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\r\n\r\n        def sgd(params, lr, batch_size):\r\n            with torch.no_grad():\r\n                for param in params:\r\n                    param -= lr * param.grad / batch_size\r\n                    param.grad.zero_()\r\n\r\n        lr = 0.03\r\n        num_epochs = 3  # Changed to 3 epochs\r\n        net = linreg\r\n        loss = squared_loss\r\n\r\n        update_interval = 1  # Update after every batch\r\n        batch_counter = 0\r\n\r\n        # Start training and update after every backpropagation\r\n        for epoch in range(num_epochs):\r\n            for X, y in data_iter(batch_size, features, labels):\r\n                l = loss(net(X, w, b), y)\r\n                l.sum().backward()\r\n                sgd([w, b], lr, batch_size)\r\n\r\n                batch_counter += 1\r\n                # Get the latest parameters\r\n                new_w_1 = w.detach().numpy()[0][0]\r\n                new_w_2 = w.detach().numpy()[1][0]\r\n                new_b_0 = b.detach().numpy()[0]\r\n\r\n                # Update lines\r\n                new_line_1 = axes_1.plot(lambda x: new_w_1 * x + new_b_0, color=BLUE)\r\n                new_line_2 = axes_2.plot(lambda x: new_w_2 * x + new_b_0, color=BLUE)\r\n                self.play(\r\n                    Transform(line_1, new_line_1),\r\n                    Transform(line_2, new_line_2),\r\n                    run_time=0.1  # Adjusted animation time for smoother update\r\n                )\r\n\r\n                # Update loss and parameter displays\r\n                with torch.no_grad():\r\n                    train_l = loss(net(features, w, b), labels)\r\n                    current_loss = float(train_l.mean())\r\n                new_loss_text = MathTex(f\u0026#34;\\\\text{{Loss}} = {current_loss:.4f}\u0026#34;)\r\n                new_loss_text.to_edge(UP)\r\n                new_w_text = MathTex(f\u0026#34;w_1 = {new_w_1:.4f},\\\\ w_2 = {new_w_2:.4f}\u0026#34;)\r\n                new_w_text.next_to(new_loss_text, DOWN)\r\n                new_b_text = MathTex(f\u0026#34;b = {new_b_0:.4f}\u0026#34;)\r\n                new_b_text.next_to(w_text, DOWN)\r\n                self.play(\r\n                    Transform(loss_text, new_loss_text),\r\n                    Transform(w_text, new_w_text),\r\n                    Transform(b_text, new_b_text),\r\n                    run_time=0.1\r\n                )\r\n\r\n            # Output current epoch\u0026#39;s loss\r\n            print(f\u0026#39;epoch {epoch + 1}, loss {current_loss:f}\u0026#39;)\r\n\r\n        self.wait(2)\n\u003c/code\u003e\u003c/pre\u003e","title":"Linear Regression Display","type":"docs"},{"content":"","date":"Nov 18 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"Nov 13 2024","externalUrl":null,"permalink":"/tags/econ/","section":"Tags","summary":"","title":"Econ","type":"tags"},{"content":"Notes taken from 小Lin说的个人空间-小Lin说个人主页-哔哩哔哩视频. space.bilibili.com/520819684?spm_id_from=333.337.search-card.all.click. Requires further completion\n","date":"Nov 13 2024","externalUrl":null,"permalink":"/docs/economic/","section":"Docs","summary":"\u003cp\u003eNotes taken from 小Lin说的个人空间-小Lin说个人主页-哔哩哔哩视频. space.bilibili.com/520819684?spm_id_from=333.337.search-card.all.click.\nRequires further completion\u003c/p\u003e","title":"Economics","type":"docs"},{"content":" Last Edit: 11/13/2024\n交易量最大的市场\n货币 #\r行业标准，每种货币用三个字母来代替 一维理解 #\r假设什么都没有，借1.6mJPY去换10kUSD 这时候可以获得USD的利息，但同时需要支付JPY的利息 但如果USD/JPY涨了，而JPY没变，则赚了对应的JPY Forex的特殊性 #\r其覆盖面极广，同时涉及专业公司与市场中的每一个人 Decentralized 去中心化 #\r对于Stock Market可能存在Nasdaq这类的交易所 但是Foreign exchange没有一个中央的交易机构 Market Maker 做市商 #\r提供交易流动性的场所 由于Foreign Exchange是去中心化的，但为了保持资金的流通一般散户甚至公司都会去找到到Marker Marker来做交易 导致了Market Maker的交易量极大的前提下，参与人数极少 Hedge 对冲 #\r而这些Market Maker并不靠与客户的对赌赚钱 而是通过强大的风险管控能力去做对冲 Censorship 监管 #\r由于其Decentralized的特点，其Censorship一般都比较松 这也导致了在交易中动手脚的可能性上升 Fixing 定盘价 #\r在每天London 4PM的时候前后一分钟交易量算出来的均价 类似于Stock的收盘价 全球大量的衍生产品都将高度依赖于这一个价格 只要交易员每天在指定时间只做指定的一个货币，改货币价格直接就上升了 High Frequency Trading #\r依靠算法套利的公司 一笔赚的非常少但是量大 Foreign Payment #\r对于企业，Foreign Exchange的维度又引入了时间的概念 假设造手机，在成本投入的半年后才能开始实现收益，但未来的汇率是不稳定的 Foreign Exchange Forward Comtract 外汇远期合约 #\r在现在对冲掉未来的风险 对于实际情况可能复杂的多，对于供应链上的供应商来说存在更多的情况 债 #\r在当地企业景气的时候，可能存在外资的涌入 但是外资的投资将采用其货币，而为了规避汇率风险则需要引入Cross Currency Swap Cross Currency Swap 货币利率互换协议 #\r可以实现虽然借的是USD债 但通过互换协议相当于规避了Currency的风险 Swap #\r互换的主要交易时基于未来的 其主要是因为对于未来Currency的不确定性导致的 Forex Products by Trading Volume #\rCurrency #\r对于一个国家，货币就衡量了当下所有商品的标尺 但犹豫利率的存在，导致一个3%年利率的国家一年后的103等价于当下的100 （纯理论） 而将两个维度结合便可以形成一个Plane 而Foreign Exchange 则是不同坐标系的转换 即Basis Change Central Bank \u0026amp; Government 央行和政府 #\r央行可以通过一系列操作影响整个市场，但是其并不能起到做庄的效果 互换协议，双边政府互相给钱，促进双方货币在国际贸易上的占比 也是去美元化的一种方法 ","date":"Nov 13 2024","externalUrl":null,"permalink":"/docs/economic/foreignexchange/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/13/2024\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e交易量最大的市场\u003c/p\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e货币 \r\n    \u003cdiv id=\"%E8%B4%A7%E5%B8%81\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E8%B4%A7%E5%B8%81\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/Economic_Static/ForeignExchange/ForeignExchange%E5%A4%96%E6%B1%87.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Foreign Exchange","type":"docs"},{"content":" Last Edit: 11/10/24\nPolymer 聚合物 #\r前缀“poly-”意指“很多”，暗示了这些聚合物分子结构中有许多重复的部分。 而“mer”指的是“重复单元”或“基元”，这是一种分子的基本单元 Basic Structure #\r与Crystal Structure不同，他们的基本结构是Cubics。并且其会在三维空间（即沿着三个方向）重复排列，形成一个规则的晶体结构 Polymer的结构则不同，它的基元通常只在One Dimnesion上重复，这种重复形成了一条长链 可以看到Polymer的内部结构为长线，而线其实是由很小的分子所组成的 Polyethylene and Polymer #\rPolymer #\r聚合物是一个广义术语，指的是由许多重复单元（基元）组成的长链分子。 聚合物的基元可以是各种各样的分子结构，不限于某一种类型。 Polyethylene #\r聚乙烯是一种具体的聚合物，由重复的乙烯单元（C₂H₄）构成。乙烯单元经过聚合反应形成长链，从而产生聚乙烯。 它是最常见的合成聚合物之一，但只是聚合物的一种类型。 放大上图可以发现 Polyethylene由String结构组成，而String则由分子组成 分子内部，如上图的Polyethylene则是由Hydrogen和Carbon Atoms组成，而Atom之间的作用力则为Intramolecular Bonds（作用在分子内部的相互作用力） Polymer\u0026rsquo;s Deformation #\r在观察超市购买了重物，并将它们放入一个塑料袋中后，可能会发现袋子的把手开始拉伸，甚至感到不适，因为它开始压入手中。随着重量的增加，把手继续拉伸，但在某个点，它似乎停止了延展。有时你甚至能在被拉伸的地方看到颜色的轻微变化 这一现象也可以通过String Model解释 Initial Stage #\r拉伸初期，由于Polymer内部结构仍是Randomly Oriented的Polymer chains没有按照拉伸的方向对齐 于是就可以像拉开松散的线一般将他们分开 Bonds #\rPrimary Bond #\rPrimary bond是 intramolecular bond，存在于一个分子内部的原子之间，例如Covelent Bond或Ionic Bond 这种键非常强，是构成Polymer Chians的基本骨架。 Secondary Bond #\rSecondary bond 是 intermolecular bond，即分子之间的作用力，比如范德华力或氢键 这种键相对较弱，存在于不同分子链之间，允许它们在一定条件下滑动或重新排列 Plastic Deformation Stage #\r随着拉伸的继续，Polymer chains开始发生滑动 滑动可以类比为面条互相滑动的情形 这就是分子间的“Friction”——在Polymer中我们称之为“Intermolecular Force”，也叫“secondary bond” （次级键） Alignment Stage #\r在Polymer Chain到位了之后，Secondary Bond（Inter Molecular Bond）的作用逐渐减小，变为了Primary Bond（Intra Molecular Bond） 所以Polymer的Plastic Deformation所需要的Stress反而会上升 Yield Strength #\r由于对于Polymer来说在发生了Plastic Deformation后其Polymer Chain将会变为Secondary Bond受力，其Stress反而上升了，所以将Curve在Plastic Deformation时候的峰值作为Yield Strength是合理的 The change of mer Units #\r已知Polymer由Polymer Chain组成，而Polymer Chain则是由很多Molecular (Mer)组成的 所以改变Molecular的元素便直接改变了Polymer Polypropylene 聚丙烯 #\r聚丙烯 (PP) 是一种非常常见且有用的聚合物。星巴克® 的那些漂亮的可重复使用杯子就是用 PP 制成的 在塑料瓶的底下可以看到其Recycling Code 为5 通常来说Polypropylene会比Polythylene更加坚固，其来自于额外的\\(CH_3\\) Polyvinylchloride (PVC) #\r出于某些原因，PVC的名字里就出现了Cl，所以其分子当然也包含Cl 而对于PVC中的V来说，其代表了Vinyl，是这种结构 Periodic Table 元素周期表 #\r对于周期表来说，其具有以下特性 周期（横向）趋势：当从左到右观察周期表时，原子半径逐渐减小，电子更接近原子核，因此核对外层电子的吸引力增强，Electronegativity增大 族（纵向）趋势：从周期表的顶端向底部移动时，原子半径增大，外层电子距离原子核更远，核对电子的吸引力减弱，因此Electronegativity减小 Electronegativity 电负性 #\r反应了Atom吸引电子的强弱程度 Bonding 分子键 #\rCovalent Bond 共价键 #\r十分熟悉的一种Bonding Type 一个Molecule中的两个Atom共享Electron Nonpolar Covalent Bond 非极性共价键 #\r当两个原子的Electronegativity几乎相等的时候，形成Nonpolar Covalent Bond 其特性为Electorn将均匀的分布于Atom之间 Polar Covalent Bond 极性共价键 #\r两个Atom的Electronegativity具有明显差异的时候 其仍然是Covalent Bond，但是Sharing的Atom会明显的偏向于其中一个Electronegative更大的Atom 可以发现Cl带有了部分的负电荷（Electron），所以其是具有更高Electronegativity的那个 并且Electrons在Covalent Bond中将会偏向于Cl Ionic Bond 离子键 #\r两个原子间的Electronegativity差极大时，一个Atom将会抢走另外一个的Electron形成Ionic Bond Polytetrafluoroethylene 聚四氟乙烯 #\r一种非反应性的Polymer 每一个Carbon Atom上都结合了大量的Floride，它们很大可以防止PTFE内部不被波坏 虽然Floride的electronegativity很高，但由于其Molecule中的对称性结构，所以形成了一个NonPolar Covalent Bond Hydrophobic 疏水性 #\rPTFE的特殊点在于其为一种Hydrophobic Polymer，具体来说Liquid无法通过其缝隙进入材料，而Gas却可以自由的通过 这就是户外服装在雨中保持干爽的愿意，一般称其为“Breathing\u0026quot; Polymethylmethacrylate 聚甲基丙烯酸甲酯 PMMA #\r一种透明的Polymer 每个合并单元上的大侧基团（通常称为 \u0026ldquo;笨重 \u0026ldquo;侧基）会阻止分子相互靠近组织。 这就确保了聚合物是完全无组织的，或者说是无定形的。 当聚合物结晶时，其折射率与无定形时不同。 如果聚合物中既有无定形的部分，也有结晶的部分（即所谓的半结晶），那么穿过聚合物的光线就不会沿着直接的路径传播，聚合物就会呈现半透明或不透明的状态。 因此，PMMA 之所以是透明的，部分原因是合并单元确保其保持 100% 透明 Length of Polymer Chain #\r前面提到过Polymer Chain是很长的，但却没有给出一个量化的办法 在合成Polymer的时候，控制其分子长度是很困难的 但可以得到一个其长度的分布图 假定一个理想的Polymer Sample，其Polymer Chain的存在需要通过一种分布来描述 描述这个分布的方式并不是通过长度而是重量 具体来说，假设有如下盒子，其里面包含了绳子 木盒子，里面有一段绳子。 你不能打开盒子，你需要确定盒子里绳子的长度。 给你一个空盒子的质量、一根一米长的绳子和一个天平。 你可以用质量来确定盒子里绳子的长度 发现可以通过绳子单位长度的质量计算盒子里绳子的长度 Number Average Molecular Weight 数均分子量 #\r$$\\overline{M}{\\text{number}} = \\frac{\\sum{n=1}^{i} M_n x_n}{\\sum x_n}$$\n所有分子的分子量加总后除以分子总数得到的平均分子量 \\(M_{number}\\)​：数均分子量 \\(M_n​\\)：第n类分子的分子量 \\(x_n\\)：第n类分子的数量比例（即该类别分子数占总分子数的比例） Analogy Candy Box #\rWeight Average Molecular Weight 重均分子量 #\r$$\\overline{M}{\\text{weight}} = \\sum{n=1}^{i} M_n w_n $$\n\\(M_{weight}\\)​：重均分子量 \\(M_n​\\)：第n类分子的分子量 \\(x_n\\)：第n类分子的质量分数（即该类别分子总质量占总所有分子总质量的比例） Analogy #\r用同样的模型说明 Dispersity 分散性 #\r$$\\mathcal{D} = \\frac{\\overline{M}{\\text{weight}}}{\\overline{M}{\\text{number}}} $$\n\\(\\mathcal{D}\\): 分散性 Dispersity \\(M_{weight}\\)​：重均分子量 \\(M_{number}\\)​：数均分子量 当\\(\\mathcal{D} \u0026gt;1\\)时：分子量分布较宽，即多分散（Polydisperse）。随着\\(\\mathcal{D}\\)值增大，分子量的差异越大，分布越宽 Why Molecular Weight Anyway? #\r当面条较长时，很难将一根面条从其他面条中分离出来。 聚合物也是如此，当然，这也是了解分子量如此重要的原因。 随着聚合物分子量的增加，强度也会增加，而且由于长分子的缠结增加，断裂应变通常也会增加。 Ways of molecules stack up #\r聚合物分子虽然通常是线性的，但并非直线。 也就是说，它们是曲线形的 但这并不意味着它们总是完全无序的。 我们还看到，当聚合物发生塑性变形( chain Orientation) 时，会产生一些有序性 Crystalline Polymer 半结晶聚合物 #\r在某些情况下，它们可以在没有任何外部负载的情况下自行有序化 聚乙烯等简单聚合物中的分子通常会在自身上来回折叠 但是由于分子非常长，聚合物永远不可能 100% 结晶 并且由于Crystal Region比Amorphous通过Secondary Bond更牢固地结合在一起，因此这些区域的强度更高 Change of Crystal Density of Polymer 改变聚合物的结晶度 #\r对于Polymer Chain来说，几乎总是有一些所谓的分支从主分子上延伸出来，其仍然是分子的一部分 事实上，我们经常会专门设计一种聚合物，使其具有分支。 低密度聚乙烯LDPE就是这种情况 低密度聚乙烯LDPE中更多和更长的分支降低了分子相互靠近排列的能力，降低了结晶度，从而降低了密度、强度甚至弹性模量 Change of the Intramolecular Bonds #\r想要通过mer unit 改变Polymer整体强度，则可以use elements that are very electronegative We could also ensure that they are bonded to something that is very easy to make positive 于是就可以想到Hydrogen Hydrogen #\r对于Hydrogen来说其有很低的Electronegativity，其Electron很容易被抢走，剩下其Proton 只需要一个有点电负性的元素，氢就会变成正元素 Hydrongen Bond #\r犹豫Hydrogen具有的特小的Electronegativity，导致了其极易产生一个High Strengh Polar Covalent Bond（强偶极子键），所以一种特殊的Bond则尤其命名：Hydrogen Bond Introducing Hydrogen Bond between Molecules #\r将分子之间引入氢键是一种增强分子间相互作用的方式 Cross Link 交联 #\r交联则是通过强的主键（即分子内的共价键）将聚合物分子永久地连接起来，从而显著增强聚合物的强度和弹性。交联聚合物的一个经典例子是天然乳胶橡胶。 在制作弹性体时，交联程度需要适中。如果交联过多，聚合物会变得硬且脆，失去弹性，不再适合作为弹性体 Temperature #\r聚合物的许多特性都是由分子间的弱键造成的 这些键（有时也称为相互作用）更容易被热能破坏，即使温度相对较低：接近室温 在金属或陶瓷中，大部分特性都是由将它们连接在一起的强键、主键的性质决定的（稍后将详细介绍），这些键的键能远远高于接近室温时的热能。 Melting Temperature 熔点温度（Tm​） #\r熔点温度指的是聚合物从固态转变为液态的温度 超过这个温度后，聚合物会像厚液体一样流动 Glass Transition Temperature 玻璃化转变温度 (Tg) #\r通常适用于非晶态或半晶态聚合物 表示的是聚合物从硬脆的“玻璃态”转变为柔软、易变形的“橡胶态”的温度 低于Tg的温度下，聚合物链段的运动受到限制，材料表现出类似玻璃的刚性 高于Tg的温度下，链段有更多的活动空间，材料变得柔软。 Melting Process #\r在加热过程中，热能将首先在Amorphous Region开始破坏分子间的键能。 随着持续加热，热能最终将足以破坏Crystal Region的分子间键 因此，Amorphous Region被破坏时的较低温度被称为Glass Transition Temperature 当Polymer低于Tg时，其像普通窗玻璃一样又硬又脆 Loading Time 施加负载的时间 #\r快速施加负荷：当它被快速拉伸时会像脆性材料一样断裂，且无永久变形。这是因为其分子链较短，在快速拉伸时分子之间没有足够的时间进行重新排列，导致聚合物直接破裂。 长时间施加负荷，聚合物会发生蠕变，即随着时间的延长，材料会逐渐变形 弹性变形（Elastic Deformation） #\r特点：弹性变形是瞬时的，即加载后立即产生变形，卸载后立即恢复原状。 变形性质：弹性变形是可逆的，即材料可以恢复到原始形状，不会有永久的变形残留。 应用场景：在桌子短暂放置在地毯上的情况下，地毯纤维受到压力后会产生弹性变形，但桌子移开后，地毯几乎立即恢复原状。 分子运动：在弹性变形中，聚合物分子链段的变形非常有限，分子间没有发生显著的滑动或重新排列。 粘性变形（Viscous Deformation） #\r特点：粘性变形是随时间累积的，即需要长时间加载才能显现。 变形性质：粘性变形是不可逆的，即变形在卸载后不完全恢复，会留下永久变形。 应用场景：当桌子长时间放置在地毯上（例如一年），地毯纤维会缓慢移动或滑动，导致永久变形，即使桌子移开后，地毯也无法完全恢复原状。 分子运动：在粘性变形中，聚合物分子链段逐渐滑动，重新排列，表现为类似液体流动的行为，这个过程不可逆。 粘弹性变形（Viscoelastic Deformation） #\r聚合物材料通常表现出粘弹性变形，即兼具弹性和粘性变形的特性。它们在短时间内表现为弹性变形，但在长时间加载下逐渐表现出粘性变形。不同聚合物在粘弹性方面有所差异：\nLimits of the noodle model #\r一个模型几乎总是有缺点的。 如果不是这样，我们就会称之为定律 再次考虑前面提到的Transparent Glass， 我们说过，部分原因是由于这种聚合物是完全无定形的，这是事实 但是，这并不能解释为什么Amorphous Polymer本身就应该是透明的 要真正理解这一点，我们需要进一步了解可见光的本质以及光与材料中电子的相互作用 这是因为材料的透明性主要取决于光在其中的传播方式 当可见光照射到材料上时，光会与材料中的电子发生相互作用，而这种相互作用的方式决定了光是被吸收、反射还是通过材料 在透明的非晶态聚合物（如Plexiglas®）中，分子的电子结构允许可见光穿过，而不会显著散射或吸收光，因此表现出透明性。 相比之下，在非晶态金属中，电子结构密集且自由度较高，能够大量吸收和反射光，从而使材料表现为不透明和反光 ","date":"Nov 10 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms6.plastics/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/10/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003ePolymer 聚合物 \r\n    \u003cdiv id=\"polymer-%E8%81%9A%E5%90%88%E7%89%A9\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#polymer-%E8%81%9A%E5%90%88%E7%89%A9\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e前缀“poly-”意指“很多”，暗示了这些聚合物分子结构中有许多重复的部分。\u003c/li\u003e\n\u003cli\u003e而“mer”指的是“重复单元”或“基元”，这是一种分子的基本单元\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eBasic Structure \r\n    \u003cdiv id=\"basic-structure\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#basic-structure\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e与Crystal Structure不同，他们的基本结构是Cubics。并且其会在三维空间（即沿着三个方向）重复排列，形成一个规则的晶体结构\u003c/li\u003e\n\u003cli\u003ePolymer的结构则不同，它的基元通常只在One Dimnesion上重复，这种重复形成了一条长链\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS6.Plastics/ECMS6.Plastics.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"MCMS 6. Plastics","type":"docs"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/tags/computer-science/","section":"Tags","summary":"","title":"Computer Science","type":"tags"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/series/d2l/","section":"Series","summary":"","title":"D2L","type":"series"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/tags/d2l/","section":"Tags","summary":"","title":"D2L","type":"tags"},{"content":" Last Edit: 11/4/2024\nYour browser does not support the video tag. Full Code is Provided\nimport numpy as np import torch import random class LinearRegression(Scene): def construct(self): def data_generator(w,b,num): X = torch.normal(0, 1, (num, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2,-3.4]) true_b = 4.2 features, labels = data_generator(true_w,true_b,1000) normal_data = features[:,[0]].numpy() #plt.hist(normal_data,bins=100,density=True,color=\u0026#39;lightblue\u0026#39;) head = Text(\u0026#34;Linear Regression - Buezwqwg\u0026#34;) head.set_color(BLUE) self.play(Create(head)) head_0 = Text(\u0026#34;In one process of Linear Regression, there bascially includes 5 steps\u0026#34;,font_size=30) self.play(Uncreate(head),Write(head_0)) self.play(head_0.animate.move_to(UP*3.5)) head_1 = Text(\u0026#34;1. Initial Parameters\u0026#34;,font_size=30) head_2 = Text(\u0026#34;2. Defining Model and Loss Function\u0026#34;,font_size=30) head_3 = Text(\u0026#34;3. Optimization\u0026#34;,font_size=30) head_4 = Text(\u0026#34;4. Loop\u0026#34;,font_size=30) head = VGroup(head_1,head_2,head_3,head_4) head.arrange(DOWN) self.play(Write(head)) # -------------------------------------------------------------------------------------------- head_5 = Text(\u0026#34;In this animate, we start with generating the data\u0026#34;,font_size=30) head_5.move_to(UP*3.5) self.play(Uncreate(head),Uncreate(head_0),Write(head_5)) code_text = \u0026#39;\u0026#39;\u0026#39; def data_generator(w, b, num): X = torch.normal(0, 1, (num, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2,-3.4]) true_b = 4.2 features, labels = data_generator(true_w,true_b,1000) \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) self.play(Write(code),Uncreate(head_5),run_time=3) self.wait(3) self.play(Unwrite(code)) axes = Axes( x_range=[-4, 4, 1], y_range=[0, 0.5, 0.1], axis_config={\u0026#34;color\u0026#34;: BLUE}, ).add_coordinates() # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2) normal_curve = axes.plot( lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2), color=YELLOW ) # 绘制均值为0的竖线 mean_line = DashedLine( start=axes.c2p(0, 0), end=axes.c2p(0, (1 / (2 * PI) ** 0.5)), color=RED ) # 添加图形和标注 self.play(Create(axes)) self.play(Create(normal_curve), Create(mean_line)) # 标注均值和标准差 mean_label = MathTex(r\u0026#34;\\mu=0\u0026#34;).next_to(mean_line, DOWN) std_label = MathTex(r\u0026#34;\\sigma=1\u0026#34;).next_to(normal_curve, UP, buff=0.5) self.play(Write(mean_label), Write(std_label)) # 展示最终效果 self.wait(2) self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line)) # -------------------------------------------------------------------------------------------- head = Text(\u0026#34;Displaying the distribution of features\u0026#34;) feature_one = features[:,[0]].tolist() feature_two = features[:,[1]].tolist() labels = labels.tolist() axes_1 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=5, # x轴的长度 y_length=5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_2 = Axes( x_range=[min(feature_two)[0], max(feature_two)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=5, # x轴的长度 y_length=5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes = VGroup(axes_1,axes_2) axes.arrange(RIGHT,buff=1) self.play(Create(axes)) points_1 = [] for i in range(len(labels)): dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0]) points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_1 = [Create(dot) for dot in points_1] points_2 = [] for i in range(len(labels)): dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0]) points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_2 = [Create(dot) for dot in points_2] self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005)) # 抽取样本-------------------------------------------------------------------------------------------- head = Text(\u0026#39;Shuffle the data and divided into samples(batches)\u0026#39;,font_size=30) self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2))) head.move_to(UP*3.5) code_text = \u0026#39;\u0026#39;\u0026#39; def data_iter(batch_size,features,labels): num = len(features) index = list(range(num)) random.shuffle(index) for i in range(0,num,batch_size): batch_index = torch.tensor(index[i:min(i+batch_size,num)]) yield features[batch_index], labels[batch_index] \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) self.play(Write(code)) self.wait(2) self.play(Uncreate(code),Unwrite(head)) def data_iter(batch_size,features,labels): num = len(features) index = list(range(num)) random.shuffle(index) for i in range(0,num,batch_size): batch_index = torch.tensor(index[i:min(i+batch_size,num)]) return batch_index.tolist() axes_1 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_2 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_3 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_4 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_5 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5) axes.arrange(RIGHT) sample_1 = data_iter(10,features,labels) points_1 = [] for i in sample_1: dot_position = axes_1.coords_to_point(features[i][0],labels[i][0]) points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_1 = [Create(dot) for dot in points_1] sample_2 = data_iter(10,features,labels) points_2 = [] for i in sample_2: dot_position = axes_2.coords_to_point(features[i][0],labels[i][0]) points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_2 = [Create(dot) for dot in points_2] sample_3 = data_iter(10,features,labels) points_3 = [] for i in sample_3: dot_position = axes_3.coords_to_point(features[i][0],labels[i][0]) points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_3 = [Create(dot) for dot in points_3] sample_4 = data_iter(10,features,labels) points_4 = [] for i in sample_4: dot_position = axes_4.coords_to_point(features[i][0],labels[i][0]) points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_4 = [Create(dot) for dot in points_4] sample_5 = data_iter(10,features,labels) points_5 = [] for i in sample_5: dot_position = axes_5.coords_to_point(features[i][0],labels[i][0]) points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_5 = [Create(dot) for dot in points_5] head = Text(\u0026#34;Display five of Sample Batches (Batch Size = 10)\u0026#34;,font_size=30) head.set_color(BLUE) head.move_to(UP*2.5) self.play(Write(head)) self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05)) self.wait(3) self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5))) # 定义模型-------------------------------------------------------------------------------------------- head_1 = Text(\u0026#39;Define the Function\u0026#39;) head_1.set_color(BLUE) code_text_1 = \u0026#39;\u0026#39;\u0026#39; def linreg(X, w, b): return torch.matmul(X, w) + b \u0026#39;\u0026#39;\u0026#39; code_1 = Code(code=code_text_1,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head_2 = Text(\u0026#39;Define the Loss Function\u0026#39;) head_2.set_color(BLUE) code_text_2 = \u0026#39;\u0026#39;\u0026#39; def squared_loss(y_hat, y): return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 \u0026#39;\u0026#39;\u0026#39; code_2 = Code(code=code_text_2,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head = VGroup(head_1,code_1,head_2,code_2) head.arrange(DOWN,buff=1) self.play(Write(head)) self.wait(2) self.play(Unwrite(head)) # 展示MSE-------------------------------------------------------------------------------------------- head = MathTex(r\u0026#34;Lose~Function~MSE~:(y_i - \\hat{y}_i)^2\u0026#34;) head.set_color(BLUE) head.move_to(UP*3) self.play(Write(head)) axes = Axes( x_range=[-10, 10, 2.5], y_range=[0, 100, 20], x_length=10, y_length=5, axis_config={\u0026#34;color\u0026#34;: GREEN}, ) # 定义MSE函数 mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10]) mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10]) # 将元素添加到场景中 self.play(Create(axes),Create(mse_curve)) self.wait(2) self.play(Uncreate(head)) head = Text(\u0026#34;The MSE Derivative indicates that loss will be increasing as it increase\u0026#34;,font_size=30) head.set_color(BLUE) head.move_to(UP*3) self.play(Write(head),Create(mse_der)) self.wait(3) self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes)) # 展示SGD-------------------------------------------------------------------------------------------- head = Text(\u0026#34;Now Conduct the Optimization Method\u0026#34;) head.move_to(UP*3) head.set_color(BLUE) code_text = \u0026#39;\u0026#39;\u0026#39; def sgd(params, lr, batch_size): with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head_2 = Text(\u0026#39;Apply this optimization method for each batch\u0026#39;) head_2.set_color(BLUE) sgd = MathTex(r\u0026#34;(w,b)\\leftarrow (w,b)-\\eta g\u0026#34;) main = VGroup(head,code,head_2,sgd) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(2) self.play(Uncreate(main),run_time=0.1) # 计算梯度-------------------------------------------------------------------------------------------- head = Text(\u0026#34;Now Calculate the Gradient\u0026#34;) head.set_color(BLUE) head.move_to(UP*3) grad = MathTex(r\u0026#34;\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w} \\left( y_i - (w x_i + b) \\right)^2\u0026#34;) grad_1 = MathTex(r\u0026#34;= \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - (wx_i + b)) \\cdot (-x_i)\u0026#34;) grad_2 = MathTex(r\u0026#34;= -\\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#34;) main = VGroup(head,grad,grad_1,grad_2) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(2) self.play(Uncreate(main),run_time=0.01) head = Text(\u0026#34;Then apllies the formula for 1000/10=100 Times\u0026#34;,font_size=45) head.set_color(BLUE) grad = MathTex(r\u0026#39;w := w + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#39;) grad_1 = MathTex(r\u0026#34;b := b + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i)\u0026#34;) main = VGroup(head,grad,grad_1) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(3) self.play(Uncreate(main),run_time=0.01) # 总结-------------------------------------------------------------------------------------------- code_text = \u0026#39;\u0026#39;\u0026#39; lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) l.sum().backward() sgd([w, b], lr, batch_size) with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;) \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head = Text(\u0026#34;Then applies the whole process in epochs and that\u0026#39;s linear regression\u0026#34;,font_size=30) main = VGroup(head,code) main.arrange(DOWN,buff=1) self.play(Write(main)) --- ","date":"Nov 4 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/linearregression/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/4/2024\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cvideo width=\"640\" height=\"360\" controls\u003e\n  \u003csource src=\"LinearRegression.mp4\" type=\"video/mp4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\u003cp\u003eFull Code is Provided\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-from\" data-lang=\"from\"\u003eimport numpy as np\nimport torch\nimport random\n\nclass LinearRegression(Scene):\n    def construct(self):\n        def data_generator(w,b,num):\n            X = torch.normal(0, 1, (num, len(w)))\n            y = torch.matmul(X, w) + b\n            y += torch.normal(0, 0.01, y.shape)\n            return X, y.reshape((-1, 1))\n\n        true_w = torch.tensor([2,-3.4])\n        true_b = 4.2\n        features, labels = data_generator(true_w,true_b,1000)\n        normal_data = features[:,[0]].numpy()\n        #plt.hist(normal_data,bins=100,density=True,color=\u0026#39;lightblue\u0026#39;)  \n         \n\n        head = Text(\u0026#34;Linear Regression - Buezwqwg\u0026#34;)\n        head.set_color(BLUE)\n        self.play(Create(head))\n\n        head_0 = Text(\u0026#34;In one process of Linear Regression, there bascially includes 5 steps\u0026#34;,font_size=30)\n        self.play(Uncreate(head),Write(head_0))\n        self.play(head_0.animate.move_to(UP*3.5))\n        head_1 = Text(\u0026#34;1. Initial Parameters\u0026#34;,font_size=30)\n        head_2 = Text(\u0026#34;2. Defining Model and Loss Function\u0026#34;,font_size=30)\n        head_3 = Text(\u0026#34;3. Optimization\u0026#34;,font_size=30)\n        head_4 = Text(\u0026#34;4. Loop\u0026#34;,font_size=30)\n        head = VGroup(head_1,head_2,head_3,head_4)\n        head.arrange(DOWN)\n        self.play(Write(head))\n\n        # --------------------------------------------------------------------------------------------\n\n        head_5 = Text(\u0026#34;In this animate, we start with generating the data\u0026#34;,font_size=30)\n        head_5.move_to(UP*3.5)\n        self.play(Uncreate(head),Uncreate(head_0),Write(head_5))\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def data_generator(w, b, num):\n            X = torch.normal(0, 1, (num, len(w)))\n            y = torch.matmul(X, w) + b\n            y += torch.normal(0, 0.01, y.shape)\n            return X, y.reshape((-1, 1))\n            \n        true_w = torch.tensor([2,-3.4])\n        true_b = 4.2\n        features, labels = data_generator(true_w,true_b,1000)\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        self.play(Write(code),Uncreate(head_5),run_time=3)\n        self.wait(3)\n        self.play(Unwrite(code))\n        axes = Axes(\n            x_range=[-4, 4, 1],\n            y_range=[0, 0.5, 0.1],\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\n        ).add_coordinates()\n\n        # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2)\n        normal_curve = axes.plot(\n            lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2),\n            color=YELLOW\n        )\n\n        # 绘制均值为0的竖线\n        mean_line = DashedLine(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(0, (1 / (2 * PI) ** 0.5)),\n            color=RED\n        )\n\n        # 添加图形和标注\n        self.play(Create(axes))\n        self.play(Create(normal_curve), Create(mean_line))\n        \n        # 标注均值和标准差\n        mean_label = MathTex(r\u0026#34;\\mu=0\u0026#34;).next_to(mean_line, DOWN)\n        std_label = MathTex(r\u0026#34;\\sigma=1\u0026#34;).next_to(normal_curve, UP, buff=0.5)\n        self.play(Write(mean_label), Write(std_label))\n\n        # 展示最终效果\n        self.wait(2)\n        self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line))\n\n        # --------------------------------------------------------------------------------------------\n\n        head = Text(\u0026#34;Displaying the distribution of features\u0026#34;)\n        feature_one = features[:,[0]].tolist()\n        feature_two = features[:,[1]].tolist()\n        labels = labels.tolist()\n        axes_1 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=5,  # x轴的长度\n            y_length=5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_2 = Axes(\n            x_range=[min(feature_two)[0], max(feature_two)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=5,  # x轴的长度\n            y_length=5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes = VGroup(axes_1,axes_2)\n        axes.arrange(RIGHT,buff=1)\n        self.play(Create(axes))\n\n        points_1 = []\n        for i in range(len(labels)):\n            dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\n            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_1 = [Create(dot) for dot in points_1]\n        points_2 = []\n        for i in range(len(labels)):\n            dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\n            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_2 = [Create(dot) for dot in points_2]\n        self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005))\n\n        # 抽取样本-------------------------------------------------------------------------------------------- \n\n        head = Text(\u0026#39;Shuffle the data and divided into samples(batches)\u0026#39;,font_size=30)\n        self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)))\n        head.move_to(UP*3.5)\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def data_iter(batch_size,features,labels):\n            num = len(features)\n            index = list(range(num))\n            random.shuffle(index)\n            for i in range(0,num,batch_size):\n                batch_index = torch.tensor(index[i:min(i+batch_size,num)])\n                yield features[batch_index], labels[batch_index]\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        self.play(Write(code))\n        self.wait(2)\n        self.play(Uncreate(code),Unwrite(head))\n\n        def data_iter(batch_size,features,labels):\n            num = len(features)\n            index = list(range(num))\n            random.shuffle(index)\n            for i in range(0,num,batch_size):\n                batch_index = torch.tensor(index[i:min(i+batch_size,num)])\n                return batch_index.tolist()\n\n\n\n        axes_1 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        \n\n        axes_2 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        \n        axes_3 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_4 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_5 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5)\n        axes.arrange(RIGHT)\n\n        sample_1 = data_iter(10,features,labels)\n        points_1 = []\n        for i in sample_1:\n            dot_position = axes_1.coords_to_point(features[i][0],labels[i][0])\n            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_1 = [Create(dot) for dot in points_1]\n\n        sample_2 = data_iter(10,features,labels)\n        points_2 = []\n        for i in sample_2:\n            dot_position = axes_2.coords_to_point(features[i][0],labels[i][0])\n            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_2 = [Create(dot) for dot in points_2]\n\n        sample_3 = data_iter(10,features,labels)\n        points_3 = []\n        for i in sample_3:\n            dot_position = axes_3.coords_to_point(features[i][0],labels[i][0])\n            points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_3 = [Create(dot) for dot in points_3]\n\n        sample_4 = data_iter(10,features,labels)\n        points_4 = []\n        for i in sample_4:\n            dot_position = axes_4.coords_to_point(features[i][0],labels[i][0])\n            points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_4 = [Create(dot) for dot in points_4]\n\n        sample_5 = data_iter(10,features,labels)\n        points_5 = []\n        for i in sample_5:\n            dot_position = axes_5.coords_to_point(features[i][0],labels[i][0])\n            points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_5 = [Create(dot) for dot in points_5]   \n        head = Text(\u0026#34;Display five of Sample Batches (Batch Size = 10)\u0026#34;,font_size=30)\n        head.set_color(BLUE)\n        head.move_to(UP*2.5)\n        self.play(Write(head))\n        self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05))\n        self.wait(3)\n        self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5)))\n\n        # 定义模型-------------------------------------------------------------------------------------------- \n\n        head_1 = Text(\u0026#39;Define the Function\u0026#39;)\n        head_1.set_color(BLUE)\n        code_text_1 = \u0026#39;\u0026#39;\u0026#39;\n        def linreg(X, w, b):\n            return torch.matmul(X, w) + b\n        \u0026#39;\u0026#39;\u0026#39;\n        code_1 = Code(code=code_text_1,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head_2 = Text(\u0026#39;Define the Loss Function\u0026#39;)\n        head_2.set_color(BLUE)\n        code_text_2 = \u0026#39;\u0026#39;\u0026#39;\n        def squared_loss(y_hat, y):\n            return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n        \u0026#39;\u0026#39;\u0026#39;\n\n\n        code_2 = Code(code=code_text_2,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head = VGroup(head_1,code_1,head_2,code_2)\n        head.arrange(DOWN,buff=1)\n        self.play(Write(head))\n        self.wait(2)\n        self.play(Unwrite(head))\n\n        # 展示MSE--------------------------------------------------------------------------------------------    \n        head = MathTex(r\u0026#34;Lose~Function~MSE~:(y_i - \\hat{y}_i)^2\u0026#34;)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        self.play(Write(head))\n        axes = Axes(\n            x_range=[-10, 10, 2.5],\n            y_range=[0, 100, 20],\n            x_length=10,\n            y_length=5,\n            axis_config={\u0026#34;color\u0026#34;: GREEN},\n        )\n        \n        # 定义MSE函数\n        mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10])\n        mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10])\n        # 将元素添加到场景中\n        self.play(Create(axes),Create(mse_curve))\n        self.wait(2)\n        self.play(Uncreate(head))\n        head = Text(\u0026#34;The MSE Derivative indicates that loss will be increasing as it increase\u0026#34;,font_size=30)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        self.play(Write(head),Create(mse_der))\n        self.wait(3)\n        self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes))\n\n\n        # 展示SGD--------------------------------------------------------------------------------------------\n        head = Text(\u0026#34;Now Conduct the Optimization Method\u0026#34;)\n        head.move_to(UP*3)\n        head.set_color(BLUE)\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def sgd(params, lr, batch_size):\n        with torch.no_grad():\n            for param in params:\n                param -= lr * param.grad / batch_size\n                param.grad.zero_()\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head_2 = Text(\u0026#39;Apply this optimization method for each batch\u0026#39;)\n        head_2.set_color(BLUE)\n        sgd = MathTex(r\u0026#34;(w,b)\\leftarrow (w,b)-\\eta g\u0026#34;)\n        main = VGroup(head,code,head_2,sgd)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(2)\n        self.play(Uncreate(main),run_time=0.1)\n        \n        # 计算梯度--------------------------------------------------------------------------------------------\n        head = Text(\u0026#34;Now Calculate the Gradient\u0026#34;)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        grad = MathTex(r\u0026#34;\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w} \\left( y_i - (w x_i + b) \\right)^2\u0026#34;)\n        grad_1 = MathTex(r\u0026#34;= \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - (wx_i + b)) \\cdot (-x_i)\u0026#34;)\n        grad_2 = MathTex(r\u0026#34;= -\\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#34;)\n\n        main = VGroup(head,grad,grad_1,grad_2)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(2)\n        self.play(Uncreate(main),run_time=0.01)\n        head = Text(\u0026#34;Then apllies the formula for 1000/10=100 Times\u0026#34;,font_size=45)\n        head.set_color(BLUE)\n        grad = MathTex(r\u0026#39;w := w + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#39;)\n        grad_1 = MathTex(r\u0026#34;b := b + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i)\u0026#34;)\n        main = VGroup(head,grad,grad_1)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(3)\n        self.play(Uncreate(main),run_time=0.01)\n\n        # 总结--------------------------------------------------------------------------------------------\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        lr = 0.03\n        num_epochs = 3\n        net = linreg\n        loss = squared_loss\n\n        for epoch in range(num_epochs):\n            for X, y in data_iter(batch_size, features, labels):\n                l = loss(net(X, w, b), y)\n                l.sum().backward()\n                sgd([w, b], lr, batch_size)\n            with torch.no_grad():\n                train_l = loss(net(features, w, b), labels)\n                print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;)\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head = Text(\u0026#34;Then applies the whole process in epochs and that\u0026#39;s linear regression\u0026#34;,font_size=30)\n        main = VGroup(head,code)\n        main.arrange(DOWN,buff=1)\n        self.play(Write(main))\n---\n\u003c/code\u003e\u003c/pre\u003e","title":"Linear Regression","type":"docs"},{"content":" Last Edit: 10/24/24\nIntroduction to Determinate #\r行列式是一个每个方阵都具有的数值 Determinate measures the factor by which the area of a given region increases or decreases The \u0026ldquo;determinant\u0026rdquo; of a transformation Determinate in R^2 #\rDeterminant计算的是Linear Transformation改变的Basis Vector所围成的面积的大小\n而对于一个Linear Transformation，大部分情况下Basis Vector围成的面积都是一个长方形\n而[[MAT188 Chapter 2 Linear Transformations#2.2 Linear Transformations in Geometry]]中存在一种Sheer Transformation，即对于一个Basis Vector来说，其出现了不属于其方向上的分量\n如上图中的\\(\\vec e_2\\)来说，其为\u0026lt;2,2\u0026gt;，即产生了Sheer\n在这种情况下，所围成的面积便成为了Parallelogram\n于是就有了两种计算\\(\\mathbb R^2\\)行列式的办法 $$det(A)=|A||B|sin\\theta$$ 这个平行四边形同时适用于Cross Product于Determinate\n这一个公式同样也是[[Cross Product 向量叉乘]]的大小（Norm），同时也是一个3x3Determinate的大小（体积）\n如果说Cross Product要找的是一个向量，Determinate要找的则是一个体积\n总的来说，要找\\(R^2\\)中的Determinate的值，其本质在求Linear Transformation后Basis Vector围成的面积\n而这一个面积可以通过\\(|A||B|sin\\theta\\)求，其同时也可以通过\n两个向量的Position Vector上的点的差值求 其最后化简之后便有 $$\\text{det} \\left( \\begin{bmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{bmatrix} \\right) = (a + b)(c + d) - ac - bd - 2bc = ad - bc $$ 这便是Determinate最初的定义 The Determinate of a 3x3 Matrix #\r$$A = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \\end{bmatrix} = \\begin{bmatrix} | \u0026amp; | \u0026amp; | \\ \\vec{u} \u0026amp; \\vec{v} \u0026amp; \\vec{w} \\ | \u0026amp; | \u0026amp; | \\end{bmatrix} $$\n3x3的Matrix的Determinate的几何意义为在\\(\\mathbb R^3\\)中的Parallelepiped的Volume 而对于\\(\\mathbb R^3\\)中的三个向量来说，如果它们线性相关（如共面，即两个向量可以通过Lienar Conbination得到第三个向量的情况下），则他们在3x3Determinate的几何意义也是就是体积便不再存在，其是一个高度为0的长方体 具体来说判断的方法便是\\(\\vec{u} \\cdot (\\vec{v} \\times \\vec{w}) = 0\\)，见下图 Determinate为0的几何意义 #\r具体来说，当\\(det(A)=0\\)的时候，其相当于一个Linear Transformation至少压缩了一个维度 而在维度被压缩之后，此过程并不可逆，见下图 Definition 6.1.1 Determinant of a 3 × 3 matrix, in terms of the Columns #\r在上面提到过了行列式的几何意义为体积，而\\(\\vec u,\\vec v\\)并不会一直出现在xy Plane中，要计算其体积，基本上要用底面积乘以高的形式 而底面积则可以通过\\(\\vec c= \\vec v\\times \\vec w\\)的Cross Product，同时求出其大小与方向 具体来说，其大小即为\\(|u|\\)，而其方向应该是垂直于vw Plane的 而将\\(\\vec u\\cdot \\vec c\\)时，则可以得到Determinate中的第三个Vetor在一个垂直于vw Plane的同时具有方向和大小的向量\\(\\vec c\\)上的Projection长度乘以其向量\\(\\vec c\\)（本身Norm为vw所围成的平行四边形的面积） 则最终得到Determinate中的第三个向量\\(\\vec u\\)在一个垂直于vw Plane的方向上的分量，在几何意义上来说为平行六面体的高 和一个\\(\\vec v\\times \\vec w\\)所得到的两个向量围成的平行四边形的长度，即平行六面体的底面积 两者相乘便可以得到该3x3 Matrix Determinate的值，即这三个Vector所围成的Parallelepiped体积\\ $$\\begin{align}\\text{det} , A = \\vec{u} \\cdot (\\vec{v} \\times \\vec{w}) \\ = \\begin{bmatrix} a_{11} \\ a_{21} \\ a_{31} \\end{bmatrix} \\cdot \\left( \\begin{bmatrix} a_{12} \\ a_{22} \\ a_{32} \\end{bmatrix} \\times \\begin{bmatrix} a_{13} \\ a_{23} \\ a_{33} \\end{bmatrix} \\right) \\ = \\begin{bmatrix} a_{11} \\ a_{21} \\ a_{31} \\end{bmatrix} \\cdot \\begin{bmatrix} a_{22}a_{33} - a_{32}a_{23} \\ a_{32}a_{13} - a_{12}a_{33} \\ a_{12}a_{23} - a_{22}a_{13} \\end{bmatrix} \\ = a_{11}(a_{22}a_{33} - a_{32}a_{23}) + a_{21}(a_{32}a_{13} - a_{12}a_{33}) + a_{31}(a_{12}a_{23} - a_{22}a_{13}) \\ = a_{11}a_{22}a_{33} - a_{11}a_{32}a_{23} + a_{21}a_{32}a_{13} - a_{21}a_{12}a_{33} + a_{31}a_{12}a_{23} - a_{31}a_{22}a_{13}\\end{align} $$\n上述介绍的所有都是有助于理解Determinant的而非考试的重点，意义在于理解，正式的内容将从下面开始\nProperties of Determinant #\rLinearity of Determinant #\r行列式对任何一列或一行都是线性的 也就是说，当我们把一列（或一行）表示为两个向量的和或乘以一个标量时，行列式也可以相应地拆分为两个行列式的和，或乘以标量 当有如下Determinate时 $$L(\\vec{x}) = \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x}+\\vec y \u0026 -\r\\end{bmatrix}\r\\right)$$\r- Matrix23位置的值为一个Variable x，而因为det在任意一行，列中都是线性的，即其也满足Linear的两个定义\r$$L(\\vec{x} + \\vec{y}) = L(\\vec{x}) + L(\\vec{y}) \\quad \\text{and} \\quad L(k\\vec{x}) = kL(\\vec{x})$$\r- 在Determinate中有\r$$\\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x} + \\vec{y} \u0026-\r\\end{bmatrix}\r\\right)\r= \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x} \u0026 -\r\\end{bmatrix}\r\\right)\r+ \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{y} \u0026 -\r\\end{bmatrix}\r\\right)\r$$\r$$\\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 k\\vec{x} \u0026 -\r\\end{bmatrix}\r\\right)\r= k \\, \\text{det} \\left( \\begin{bmatrix}\r- \u0026 \\vec{v}_1 \u0026 - \\\\\r- \u0026 \\vec{v}_2 \u0026 - \\\\\r- \u0026 \\vec{x} \u0026 -\r\\end{bmatrix}\r\\right)\r$$\r- 如果在矩阵的一行乘上 t而剩下的n-1行保持不变，则行列式的值就要乘上 t\r$$\\left| \\begin{array}{cc} ta \u0026 tb \\\\ c \u0026 d \\end{array} \\right| = t \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r$$\r- 同理对于Linear Transformation的另外一个性质也通用 $$\\left| \\begin{array}{cc} a + a' \u0026 b + b' \\\\ c \u0026 d \\end{array} \\right| = \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right| + \\left| \\begin{array}{cc} a' \u0026 b' \\\\ c \u0026 d \\end{array} \\right|\r$$\r- 需要知道的是，这并不是在说$$det(A+B)=det(A)+det(B)$$\r- 而是对于Square Matrxi的每一行来说是Linear的\rChange of matrix\u0026rsquo;s effect on Determinate #\r当交换矩阵的两行，Determinant的值将会变号 如果你交换一个3×3矩阵的两行，行列式的值也会反号 同理也能知道 $$\\left| \\begin{array}{cc} 0 \u0026 1 \\\\ 1 \u0026 0 \\end{array} \\right| = -1\r$$\rNon-Squre Matrix Can\u0026rsquo;t Have Determinate #\rNon-Squre的Matrix会出现在当有两行是完全相同的时候 其证明可以是，当交换了两个相同的Matrix的Row的时候，其根据[[#Change of matrix\u0026rsquo;s effect on Determinate]]会发生变号，而可以观察发现新的Matrix和原来的没有区别，有Det=-Det，所以det=0 Row Operation\u0026rsquo;s influence on Determinate #\r从矩阵的某行 k 减去另一行 i 的倍数，并不改变行列式的数值（消元的过程不改变行列式） $$\\left| \\begin{array}{cc} a \u0026 b \\\\ c - ta \u0026 d - tb \\end{array} \\right| = \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right| - \\left| \\begin{array}{cc} a \u0026 b \\\\ ta \u0026 tb \\end{array} \\right|\r$$\r$$= \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r- t \\left| \\begin{array}{cc} a \u0026 b \\\\ a \u0026 b \\end{array} \\right|\r$$\r- 根据[[#Change of matrix's effect on Determinate]]，后一项的Determinant为0，即整体Det不变\rZero Rows Determinant #\r矩阵 A 的某一行都是 0，则其行列式为 0 根据[[#Linearity of Determinate]]可以知道，当t=0的时候， $$\\left| \\begin{array}{cc} ta \u0026 tb \\\\ c \u0026 d \\end{array} \\right| = t \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|$$\r- 有$$\\left| \\begin{array}{cc} 0\\cdot a \u0026 0\\cdot b \\\\ c \u0026 d \\end{array} \\right|= 0 \\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|$$\r即Determinant为0 Trangular Matrix\u0026rsquo;s Determinant #\r$$\\left| \\begin{array}{cccc} d_1 \u0026 * \u0026 \\cdots \u0026 * \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 * \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right| = \\left| \\begin{array}{cccc} d_1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right|\r= d_1 d_2 \\cdots d_n\r\\left| \\begin{array}{cccc} 1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 1 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 1 \\end{array} \\right| = d_1 d_2 \\cdots d_n\r$$\r根据[[#Row Operation\u0026rsquo;s influence on Determinate]]，当是通过Row Operation得到Triangular Matrix的时候，正负号可能发生改变 对于非Diagonal上的元素，根据[[#Row Operation\u0026rsquo;s influence on Determinate]]可以做Row Operation在不改变Determinant的前提下将他们全部消掉，有 $$\\left| \\begin{array}{cccc} d_1 \u0026 * \u0026 \\cdots \u0026 * \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 * \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right|= \\left| \\begin{array}{cccc} d_1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 d_2 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 d_n \\end{array} \\right|$$\r- 再通过[[#Linearity of Determinate]]提取出每个Row Pivot上的d\r$$= d_1 d_2 \\cdots d_n\r\\left| \\begin{array}{cccc} 1 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 1 \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 \\cdots \u0026 1 \\end{array} \\right| = d_1 d_2 \\cdots d_n$$\rSingular Matrix\u0026rsquo;s Determinant #\r对于Rank小于Row的Square Matrix，其Determinant为0 Numerial Approach of Determinant #\r当有一个Matrix的时候，想要计算其Determinant，即需要将其化为Triangular Matrix，最简单的方式即为化为Upper Triangular Matrix 而要消成Upper Triangular Matrix的方式即为将C化为0（拿2x2Matrix举例） $$\\left[ \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cc} a \u0026 b \\\\ 0 \u0026 d - \\frac{c}{a}b \\end{array} \\right]\r$$\r$$\\left| \\begin{array}{cc} a \u0026amp; b \\ c \u0026amp; d \\end{array} \\right| = a \\left( d - \\frac{c}{a}b \\right) = ad - bc $$\nDeterminant of Product #\r\\(det(AB)=det(A)\\cdot det(B)\\) \\(det(A+B)\\neq det(A)+det(B)\\) Determinant of Inverse #\r\\(det(A^{-1})\\) 已知\\(A^{-1}A=I\\)，即\\(det(A^{-1})det(A)=det(I)=1\\) 则有\\(det(A^{-1})=\\frac{1}{det(A)}\\) Determinant of Square #\r\\(det(A^2)=det(A)^2=det(A)\\cdot det(A)\\) Determinant of Coefficient before Matrix #\r\\(det(2A)=2^ndet(A)\\) 对于nxn Matrix来说，犹豫每一个Row都乘上的Coefficient 2，即存在\\(2^n\\)的总系数 Determinant of Transpose #\r\\(det(A^T)=det(A)\\) $$\\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r= \\left| \\begin{array}{cc} a \u0026 c \\\\ b \u0026 d \\end{array} \\right|\r= ad - bc\r$$\rFormular for Determinant #\r2x2 #\r$$\\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r=\r\\left| \\begin{array}{cc} a \u0026 0 \\\\ c \u0026 d \\end{array} \\right|\r+ \\left| \\begin{array}{cc} 0 \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r=\r\\left| \\begin{array}{cc} a \u0026 0 \\\\ c \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{cc} 0 \u0026 d \\\\ 0 \u0026 d \\end{array} \\right|\r+ \\left| \\begin{array}{cc} 0 \u0026 b \\\\ 0 \u0026 d \\end{array} \\right|\r=\r0 + ad - cb + 0\r=\rad - bc\r$$\r3x3 #\r将每一行拆成3部分，每一部分都对应了不同的行上的不同元素 总共会得到\\(3^3\\)个Matrix即27个，而其中大部分Matrix由于行或列上全为0有Det=0 而其中的非零情况出现在每一列都有的情况下（因为我们是从行出发开始分解的，所以每一行都保证了有值） $$\\left| \\begin{array}{ccc} a_{11} \u0026 a_{12} \u0026 a_{13} \\\\ a_{21} \u0026 a_{22} \u0026 a_{23} \\\\ a_{31} \u0026 a_{32} \u0026 a_{33} \\end{array} \\right|\r=\r\\left| \\begin{array}{ccc} a_{11} \u0026 0 \u0026 0 \\\\ 0 \u0026 a_{22} \u0026 0 \\\\ 0 \u0026 0 \u0026 a_{33} \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} a_{11} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 a_{23} \\\\ 0 \u0026 a_{32} \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 a_{12} \u0026 0 \\\\ a_{21} \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 a_{33} \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 a_{12} \u0026 0 \\\\ 0 \u0026 a_{22} \u0026 0 \\\\ a_{31} \u0026 0 \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 0 \u0026 a_{13} \\\\ a_{21} \u0026 0 \u0026 0 \\\\ 0 \u0026 a_{32} \u0026 0 \\end{array} \\right|\r+ \\left| \\begin{array}{ccc} 0 \u0026 0 \u0026 a_{13} \\\\ 0 \u0026 a_{22} \u0026 0 \\\\ a_{31} \u0026 0 \u0026 0 \\end{array} \\right|\r$$\r$$=\ra_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}$$\r- 其中所有的负值，都是用了[[#Change of matrix's effect on Determinate]]将Matrix做Row Exchange成Uppertriangular Matrix而导致的Determinant的变号\r但这个做法再4x4中并不通用，所以需要从2x2，3x3中推导出nxn的公式 Big Formula A #\r$$det(A)=\\sum_{n!}\\pm a_{1\\alpha}a_{2\\beta}a_{3\\gamma}\\dots a_{n\\omega}$$\r\\(n!\\)：由于我们是用Row做的，即在Row1中有n个Column可以选，而到了Row2中，只有n-1个Column可以选，以此类推可能性即为\\(n!\\) \\(\\alpha,\\beta,\\gamma,\\omega\\)：列标号中的任何值 $$\\left| \\begin{array}{cccc} 0 \u0026 0 \u0026 1 \u0026 1 \\\\ 0 \u0026 1 \u0026 1 \u0026 0 \\\\ 1 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 0 \u0026 1 \\end{array} \\right| =\r\\left| \\begin{array}{cccc} 0 \u0026 0 \u0026 0 \u0026 1 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 0 \u0026 0 \\end{array} \\right|\r+\r\\left| \\begin{array}{cccc} 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\end{array} \\right|\r$$\r分解出的两个Matrix中，第一个需要做两次Row Exchange得到Identity，即为1，而第二个则需要1次Row Exchange就可以得到Identify，即为-1，1-1=0 Cofactor Formula 代数余子式 #\r3x3 #\r代数余子式是用较小的矩阵的行列式来写出 n 阶行列式的公式 $$\\text{det} \\left( \\mathbf{A} \\right) = a_{11} \\left( a_{22}a_{33} - a_{23}a_{32} \\right)\r+ a_{12} \\left( -a_{21}a_{33} + a_{23}a_{31} \\right)\r+ a_{13} \\left( a_{21}a_{32} - a_{22}a_{31} \\right)\r$$\r$$=\\left| \\begin{array}{ccc} a_{11} \u0026 0 \u0026 0 \\\\ 0 \u0026 a_{22} \u0026 a_{23} \\\\ 0 \u0026 a_{32} \u0026 a_{33} \\end{array} \\right|\r+\r\\left| \\begin{array}{ccc} 0 \u0026 a_{12} \u0026 0 \\\\ a_{21} \u0026 0 \u0026 a_{23} \\\\ a_{31} \u0026 0 \u0026 a_{33} \\end{array} \\right|\r+\r\\left| \\begin{array}{ccc} 0 \u0026 0 \u0026 a_{13} \\\\ a_{21} \u0026 a_{22} \u0026 0 \\\\ a_{31} \u0026 a_{32} \u0026 0 \\end{array} \\right|$$\r由于第二个Martri在化为Identity时只需要一次Row Exchange而其他的都需要两次，所以第二个Cofactor为减去 Cofactor Formula #\r将原公式中属于矩阵第一行的\\(a_{ij}\\)提出来，其系数即为代数余子式，是一个低阶行列式的值。这个低阶行列式是由原矩阵去掉\\(a_{ij}\\)所在的行和列组成的。 对矩阵中任意元素\\(a_{ij}\\)而言，其代数余子式\\(C_{ij}\\)j就是矩阵的行列式的公式中\\(a_{ij}\\)的系数 \\(C_{ij}\\)等于原矩阵移除第i行和第j列后剩余元素组成的n-1阶矩阵的行列式数值乘以\\((-1)^{i+j}\\) \\(C_{ij}\\)在 i+j 为偶数时为正，奇数时为负数 则可以总结对于n阶Square Matrix来说，有 $$\\text{det} \\left( \\mathbf{A} \\right) = a_{11} C_{11} + a_{12} C_{12} + \\cdots + a_{1n} C_{1n}\r$$\rex. in 2x2 #\rCofactor Formula最简单的应用即为在2x2 Matrix中，有 $$\\left| \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right|\r= ad + b(-c)$$\rex. 三对角阵（tridiagonal matrix） #\r只在Tridiagonal Matrix这种特殊结构中可行 $$\\mathbf{A_4} = \\left[ \\begin{array}{cccc} 1 \u0026 1 \u0026 0 \u0026 0 \\\\ 1 \u0026 1 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 1 \u0026 1 \\\\ 0 \u0026 0 \u0026 1 \u0026 1 \\end{array} \\right]\r$$\r![[LA6.Determinats-9.png]]\rFormula for A Inverse #\r已知2阶Matrix的Inverse为 $$\\left[ \\begin{array}{cc} a \u0026 b \\\\ c \u0026 d \\end{array} \\right]\r= \\frac{1}{ad - bc} \\left[ \\begin{array}{cc} d \u0026 -b \\\\ -c \u0026 a \\end{array} \\right]\r$$\r- 通过观察上2x2的例子可以得出\r$$\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top$$\r通过观察可以发现d是a的C，-b是c的C，-c是b的C，a是d的C Adjoint Matrix 伴随矩阵 #\r此处的Cofactor的Transpose\\(C^T\\)便可以称为Adjoint Martirx，即伴随矩阵 对于Adjoint Matrix来说，其大小总是原Matrix的Dimension-1即为n-1 Proof of A Inverse #\r已知Gauss Jordan Elimination提到\\([A|I]\\)在A被消成I后，I会变成\\(A^{-1}\\) 同时\\(A^{-1}A=I\\)，现在将\\(\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top\\)带入 $$A\\cdot \\mathbf{A}^{-1} = A\\cdot\\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top=I$$\r$$A\\cdot C^T=det(A)\\cdot I$$\r如果上式成立，则\\(\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^\\top\\)为真命题 $$\\mathbf{AC}^T = \\begin{bmatrix} a_{11} \u0026 \\cdots \u0026 a_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ a_{n1} \u0026 \\cdots \u0026 a_{nn} \\end{bmatrix}\r\\begin{bmatrix} C_{11} \u0026 \\cdots \u0026 C_{n1} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ C_{1n} \u0026 \\cdots \u0026 C_{nn} \\end{bmatrix}\r$$\r- 对于所有结果矩阵对角线上的的元素来说都有\r$$\\sum_{j=1}^{n} a_{1j} C_{1j} = \\det(\\mathbf{A})$$\r即他们本身就是det(A)的展开式 而现在要研究结果矩阵非对角线上的内容 可以发现每一个非对角线上的元素都将为0 $$AC^T = \\begin{bmatrix}\r\\det A \u0026 0 \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 \\det A \u0026 0 \u0026 \\cdots \u0026 0 \\\\\r0 \u0026 0 \u0026 \\ddots \u0026 \\cdots \u0026 0 \\\\\r\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\\r0 \u0026 0 \u0026 0 \u0026 \\cdots \u0026 \\det A\r\\end{bmatrix} = \\det(A)I\r$$\rCramer’s Rule 克莱姆法则 #\r对于问题Ax=b来说，其解法很简单的就等于\\(x=A^{-1}B\\) 而在知道了\\(A^{-1}\\)的值之后有 $$\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{C}^T \\mathbf{b}$$\r选择乘上\\(C^T\\) 的不再是A而是b了，但一个Matrix乘以一个Cofactor Matrix的做法又令人想到了Determinant，可以发现 $$x_j = \\frac{\\det(\\mathbf{B}_j)}{\\det(\\mathbf{A})}\r$$\r其中每一个\\(B_i\\)都是一个第i列被\\(B_i\\)所替换的Matrix A，具体来说有 $$\\mathbf{B}_1 = \\begin{bmatrix}\rb_1 \u0026 a_{12} \u0026 \\cdots \u0026 a_{1n} \\\\\rb_2 \u0026 a_{22} \u0026 \\cdots \u0026 a_{2n} \\\\\rb_3 \u0026 a_{32} \u0026 \\ddots \u0026 \\vdots \\\\\r\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 a_{n-1\\,n} \\\\\rb_n \u0026 a_{n2} \u0026 \\cdots \u0026 a_{nn}\r\\end{bmatrix}\r, \\quad\r\\mathbf{B}_n = \\begin{bmatrix}\ra_{11} \u0026 \\cdots \u0026 a_{1\\,n-1} \u0026 b_1 \\\\\ra_{21} \u0026 \\cdots \u0026 a_{2\\,n-1} \u0026 b_2 \\\\\r\\vdots \u0026 \\ddots \u0026 \\vdots \u0026 \\vdots \\\\\ra_{n-1\\,1} \u0026 \\cdots \u0026 a_{n-1\\,n-1} \u0026 b_{n-1} \\\\\ra_{n1} \u0026 \\cdots \u0026 a_{n2} \u0026 b_n\r\\end{bmatrix}\r$$\r可以发现等式中的\\(C^T_i \\cdot B_i\\)正好等于\\(B_i\\)的Determinant 其实相比于消元法，采用克莱姆法则计算方程的解效率较低。。。\n","date":"Oct 24 2024","externalUrl":null,"permalink":"/docs/linearalgebra/la6.determiants/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 10/24/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eIntroduction to Determinate \r\n    \u003cdiv id=\"introduction-to-determinate\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#introduction-to-determinate\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e行列式是一个每个方阵都具有的数值\u003c/li\u003e\n\u003cli\u003eDeterminate measures the factor by which the area of a given region increases or decreases\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/LinearAlgebra_Static/LA6.Determinants/LA6.Determinats.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"LA 6. Determinants","type":"docs"},{"content":"\rYour browser does not support the video tag.\r","date":"Oct 22 2024","externalUrl":null,"permalink":"/docs/projectilemotion/","section":"Docs","summary":"\u003cvideo width=\"640\" height=\"360\" controls\u003e\r\n  \u003csource src=\"Projectile.mp4\" type=\"video/mp4\"\u003e\r\n  Your browser does not support the video tag.\r\n\u003c/video\u003e","title":"Projectile Motion when air resisitance is propftional to velocity","type":"docs"},{"content":" “这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想” - 三体\nYour browser does not support the video tag.\r","date":"Oct 17 2024","externalUrl":null,"permalink":"/docs/montecarlomethod/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003e“这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想” - 三体\u003c/p\u003e","title":"Monte Carlo Approach to Calculate π","type":"docs"},{"content":" Last Edit: 10/16/24\nStress-Strain Curve #\r当回头重新看Stress-Strain Curve的时候可以发现一些特殊的点 如Proportion Limit，Ultimate Tensile Strength两个点 Proportional Limit #\r在曲线的最初阶段，存在一个接近于Linear的区域，其代表了Linear Elastic的Region 而也必会存在一个点象征着Linear Elastic Region的结束 但犹豫一些测量的误差或者精度上的问题，将导致最终的这一个Linear Elastic结束的点无法被确定，所以需要一个约定俗成的方法 0.2% Offset Yield Strength #\r在\\(\\epsilon=0.002\\)的位置画一个平行于Linear Elastic Region的直线，其交于SS Curve的位置即为Yield Strength ex. #\rA hypothetical metal has a 0.2% offset yield strength of 358 MPa, an ultimate tensile strength of 522 MPa, and a fracture strength of 460 MPa. A sample of this metal, originally 1 m in length with a cross section of 2 mm × 2 mm is loaded along its long axis. Just before fracture, while the load is still applied, the length is 1.3 m and when the load is released, the length is 1.18 m. Calculate the modulus of elasticity in GPa.（这我做集贸啊）\n之前有公式\\(\\sigma=E\\epsilon\\) 现在需要将其推广到\\(E=\\frac{\\Delta \\sigma}{\\epsilon}\\)上 \\(\\Delta \\sigma\\)：有460MPa-0MPa=460MPa \\(\\epsilon\\)：有0.3-0.18=0.12 Uniform deformation #\r在经历了Linear Elastic Region后，出现的便是Uniform Platic区间 这个阶段内，材料的塑性变形均匀地分布在整个试样或构件中 其具体的Unifrom体现在了变形过程中结构的完整性上 在Non-Unifrom Region中，材料已经发生了局部的Fracture，即产生了Neck 这也解释了为什么在过了Ultimate Tensile strength后Stress开始下降，即当金属样品承受的应力值逐渐增大时，最终会开始失效。 在拉伸过程中，当金属原子间的一些键断裂时，就会出现这种情况 The Dislocation #\rDisloaction广泛存在于各种晶体材料中，不仅限于金属。 它们在不同类型材料中的运动机制和对材料性能的影响各不相同。 例如，在金属中位错运动相对容易，而在陶瓷和半导体中则较为困难，且其作用更为复杂 具体来说存在有4种不同的Imperfections，来自不同的维度 Dislocation Density #\rDislocation Density: 材料中单位体积内存在的位错数量 Dislocation Density越高，材料的强度和硬度可能会有所增加，但延展性会降低 Metal #\rDislocations are always present in metals. 可以通过加热的方式更改Metal的dislocation density Zero-Dimensional Imperfections or Point Defects #\r点缺陷（Point Defect）是指材料的晶体结构中，由于原子或离子位置上的异常，导致的局部晶体结构缺陷 Interstitial Impurities #\rInterstitial Impurities指的是一些较小的Atom（比如碳、氮等）进入了Crystal中本来空着的间隙位置 Substitutional Impurities #\r当一个外来原子取代了晶体中正常位置上的原子时，形成Substitutional Impurities。 这种缺陷在合金中常见，例如铜和锌形成的黄铜 Vacancies #\r在晶体中，一个本应有原子的位置上缺少了一个原子 它会导致周围的原子重新调整位置，影响材料的物理性质 但Vacancies的产生并不会导致Material的Strength发生改变s Zero-Dimension\u0026rsquo;s Influence on Higher Dimension #\r当Zero Dimension Impurity发生的时候，其会对周围的Crystal Structure产生一个Strain Fields 这个Strain Field将会Repel其他的Atom进入Dislocation 其通常会造成一种One-Dimensional Imperfection Ratio of the Number of Vacancies #\r$$\\frac{N_v}{N} = e^{\\frac{-Q_v}{kT}} \\tag{1}$$\nNv​：这是Crystal中Number of Vancancies，即晶体结构中缺失原子的位置数。\nN：Number of Atoms\nQv​：表示生成一个Vacancies所需的Energy，通常以电子伏特（eV）为单位。\nk：这是Boltzmann Constant，数值约为 \\(1.38\\times 10^{-23}J/K\\)，用于将温度与能量联系起来。\nT：这是Thermodynamic temperature，以开尔文（K）为单位，是热力学温度是根据热力学原理来衡量系统绝对温度的一个度量，其零点对应理论上的绝对零度，即系统的分子运动几乎完全停止、能量达到最低的状态。\n从本质上讲，原子在它们的晶格位点上拼命振动，试图跳出它们的位点。 它们真的很努力。 我的意思是说，每秒大约有 1013 次！ 在固态中，由于结合能强于热能，大多数情况下它们都不会成功（实际上，原子形成有序固体有很大的节能作用，但我们稍后会详细介绍）。 但偶尔也会有原子从其晶格位置成功跃迁，并移动到晶格的其他位置。 这就会留下一个缺失的原子或空位。 因此，我们可以将空位的形成看作是将原子固定在晶格位点上的结合能与将原子从晶格位点上挤出的热能之间的持续斗争\nThermodynamics 热力学 #\rThermodynamics说明了在一个物体中，能量是分布在Atoms上而不是对于所有Atom都具有相同能量的 因此，单个原子可能有足够的能量跳出其晶格位置，而其余大多数原子则没有，这是有道理的 如图 10 所示，随着温度的升高，我们发现有更多的原子进入了高能态。 同时，随着温度的降低，高能态原子的数量也在减少。 Boltzmann Distribution #\r在绝对零度（0 K）：所有原子都会处于最低能量状态，因为此时系统没有足够的能量让原子占据更高的能量状态。 在无限温度（∞ K）：系统的温度极高，粒子的能量足以占据任何能量状态。因此所有能量状态的粒子数都会趋于相等，也就是所有能量状态均等分布 在Boltzmann Distribution中，不会出现所有粒子只占据最高能量状态的情况，即使是在极高温度下 One-Dimensional Imperfections or Dislocations #\rCold Work #\r当我们对金属进行Plastic Deformation时，会产生新的Dislocation。 这增加了Dislocation Density，这将在未来中增加Dislocation的难度 当我们观察金属的Stress-Strain Curve，发现应力在Yield Strength之后继续增加时，我们首次观察到金属通过Plastic Deformation而得到强化。 事实上，如果我们Unload一个sample并重新load，其在stress水平达到我们在前一个循环中留下的Stress之前不会开始Plastic Deformation 这是一种通过塑性变形进行的强化，不过在工业上，我们通常是通过轧制或拉动金属零件，或将金属零件压入模具来实现塑性变形，而不是通过简单的拉伸来拉动零件 Hot Work #\r材料被加热到再结晶温度以上，这意味着减少了Dislocation Density Two-Dimensional Imperfections #\rFree Surfaces #\r自由表面（Free Surfaces） 是指材料的外部表面, 这些表面与外界环境直接接触 原子排列不规则：在材料的自由表面，原子周围的配位数（与其他原子结合的数量）比材料内部的原子要少 Grain Boundaries #\r是指多晶材料中不同Crystal Structure交界的地方 当Dislocation在Crystal中移动，其必定要穿过Grain Boundary，然而这对位错来说是一个挑战 如果我们减小金属的晶粒尺寸，就会产生更多的晶界和更多的位错运动障碍，从而有望提高金属的强度 This strengthening mechanism has a pretty self-explanatory name: grain size reduction. Three-Dimensional Imperfections or Second Phase Particles #\rThree-dimensional imperfections occur any time we have a second phase within a solid 从本质上讲，如果固体中存在晶体结构不同的区域，就会出现第二相或三维缺陷 ","date":"Oct 16 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms5.furtheronstressstrain/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 10/16/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eStress-Strain Curve \r\n    \u003cdiv id=\"stress-strain-curve\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#stress-strain-curve\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS5.FurtherOnStressStrain/MCMS5.FurtherOnStressStrain.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e当回头重新看Stress-Strain Curve的时候可以发现一些特殊的点\u003c/li\u003e\n\u003cli\u003e如Proportion Limit，Ultimate Tensile Strength两个点\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eProportional Limit \r\n    \u003cdiv id=\"proportional-limit\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#proportional-limit\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e在曲线的最初阶段，存在一个接近于Linear的区域，其代表了Linear Elastic的Region\u003c/li\u003e\n\u003cli\u003e而也必会存在一个点象征着Linear Elastic Region的结束\u003c/li\u003e\n\u003cli\u003e但犹豫一些测量的误差或者精度上的问题，将导致最终的这一个Linear Elastic结束的点无法被确定，所以需要一个约定俗成的方法\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003e0.2% Offset Yield Strength \r\n    \u003cdiv id=\"02-offset-yield-strength\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#02-offset-yield-strength\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS5.FurtherOnStressStrain/MCMS5.FurtherOnStressStrain-1.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"MCMS 5. Further On Stress Strain","type":"docs"},{"content":"\r元数据 #\r[!abstract] 三体（全集）\n书名： 三体（全集） 作者： 刘慈欣 简介： 每个人的书架上都该有套《三体》！关于宇宙最狂野的想象！就是它！征服世界的中国科幻神作！包揽九项世界顶级科幻大奖！出版16个语种，横扫30国读者！奥巴马、雷军、马化腾、周鸿祎、潘石屹、扎克伯格……强推！刘慈欣获得2018年度克拉克想象力贡献社会奖！刘慈欣是中国科幻小说的最主要代表作家，亚洲首位世界科幻大奖“雨果奖”得主，被誉为中国科幻的领军人物。 出版时间： 2018-12-01 00:00:00 ISBN： 9785214589626 分类： 精品小说-科幻小说 出版社： 海南省电子音像出版社 PC地址：https://weread.qq.com/web/reader/ce032b305a9bc1ce0b0dd2a 高亮划线 #\r第十三章 红岸之二 #\r📌 雷政委说完站起来，迈着军人的稳健步伐离去。叶文洁的双眼盈满了泪水，透过眼泪，屏幕上的代码变成了一团团跳动的火焰。自父亲死后，这是她第一次流泪。 叶文洁开始熟悉监听部的工作，她很快发现，自己在这里远不如在发射部顺利，她已有的计算机知识早已落后，大部分软件技术都得从头学起。虽然有雷政委的信任，但对她的限制还是很严的，她可以看程序源代码，但不许接触数据库。 在日常工作中，叶文洁更多是接受杨卫宁的领导，他对她更加粗暴了，动不动就发火。雷政委多次劝他也没用，好像一见到叶文洁，他就充满了一种无名的焦虑。 ⏱ 2024-08-30 09:52:10 ^695233-17-5851\n第十六章 三体、哥白尼、宇宙橄榄球、三日凌空 #\r📌 很好，哥白尼，很好，你这种现实的、符合实验科学思想的想法是大多数学者不具备的，就凭这一点，你的理论也值得听一听。” 教皇对汪淼点点头，“说说看吧。” 汪淼走到长桌的另一端，让自己镇定了一下，说：“其实很简单：太阳的运行之所以没有规律，是因为我们的世界中有三颗太阳，它们在相互引力的作用下，做着无法预测的三体运动。当我们的行星围绕着其中的一颗太阳做稳定运行时，就是恒纪元；当另外一颗或两颗太阳运行到一定距离内，其引力会将行星从它围绕的太阳边夺走，使其在三颗太阳的引力范围内游移不定时，就是乱纪元；一段不确定的时间后，我们的行星再次被某一颗太阳捕获，暂时建立稳定的轨道，恒纪元就又开始了。这是一场宇宙橄榄球赛，运动员是三颗太阳，我们的世界就是球！” ⏱ 2024-08-30 10:05:15 ^695233-20-2574\n第十七章 三体问题 #\r📌 这时，我就像一个半生寻花问柳的放荡者突然感受到了爱情。 “你不知道庞加莱[插图]吗？”汪淼打断魏成问。 当时不知道，学数学的不知道庞加莱是不对，但我不敬仰大师，自己也不想成大师，所以不知道。但就算当时知道庞加莱，我也会继续对三体问题的研究。全世界都认为这人证明了三体问题不可解，可我觉得可能是个误解，他只是证明了初始条件的敏感性，证明了三体系统是一个不可积分的系统，但敏感性不等于彻底的不确定，只是这种确定性包含着数量更加巨大的不同形态。现在要做的是找到一种新的算法。当时我立刻想到了一样东西：你听说过“蒙特卡洛法”吗？哦，那是一种计算不规则图形面积的计算机程序算法，具体做法是在软件中用大量的小球随机击打那块不规则图形，被击中的地方不再重复打击，这样，达到一定的数量后，图形的所有部分就会都被击中一次，这时统计图形区域内小球的数量，就得到了图形的面积，当然，球越小结果越精确。 ⏱ 2024-08-30 10:16:26 ^695233-21-4188\n📌 这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想。这就是我解决三体问题的策略。我研究三体运动的任何一个时间断面，在这个断面上，各个球的运动矢量有无限的组合，我将每一种组合看做一种类似于生物的东西，关键是要确定一个规则：哪种组合的运行趋势是“健康的”和“有利的”，哪种是“不利的”和“有害的”，让前者获得生存的优势，后者则产生生存困难，在计算中就这样优胜劣汰，最后生存下来的就是对三体下一断面运动状态的正确预测。 ⏱ 2024-08-30 10:16:27 ^695233-21-4775\n第三十二章 古筝行动 #\r📌 叶文洁：如果他们能够跨越星际来到我们的世界，说明他们的科学已经发展到相当的高度，一个科学如此昌明的社会，必然拥有更高的文明和道德水准。 审问者：你认为这个结论，本身科学吗？ 叶文洁：…… ⏱ 2024-09-02 13:19:12 ^695233-36-12335-12543\n危机纪年第20年，三体舰队距太阳系4.15光年 #\r📌 人类大脑的进化需要两万至二十万年才能实现明显的改变，而人类文明只有五千年历史，所以我们目前拥有的仍然是原始人的大脑 ⏱ 2024-10-12 11:38:59 ^695233-47-9290-9347\n📌 政治思想工作是通过科学的理性思维来建立信念。 ⏱ 2024-10-12 11:39:43 ^695233-47-10801-10823\n下部 黑暗森林 #\r📌 普通人的目光，是他们所在地区和时代的文明程度的最好反映。他曾经看到过一组由欧洲摄影师拍摄的清朝末年的照片，最深的印象就是照片上的人呆滞的目光，在那些照片上，不论是官员还是百姓，眼睛中所透出的只有麻木和愚钝，看不到一点生气。 ⏱ 2024-10-12 12:13:19 ^695233-48-2540-2651\n📌 给岁月以文明，而不是给文明以岁月。 ⏱ 2024-10-12 13:19:45 ^695233-48-32277-32294\n📌 章北海停下手中的笔，抬头看着舱外的东方延绪，他的目光平静如水，“同为军人，知道我们之间最大的区别在哪里吗？你们按照可能的结果来决定自己的行动；而我们，不管结果如何，必须尽责任，这是唯一的机会，所以我就做了。 ⏱ 2024-10-12 13:59:12 ^695233-48-84364-84467\n📌 两个多世纪前，阿瑟·克拉克在他的科幻小说《2001：太空奥德赛》中描述了一个外星超级文明留在月球上的黑色方碑，考察者用普通尺子量方碑的三道边，其长度比例是1∶3∶9，以后，不管用何种更精确的方式测量，穷尽了地球上测量技术的最高精度，方碑三边的比例仍是精确的1∶3∶9，没有任何误差。克拉克写道：那个文明以这种方式，狂妄地显示了自己的力量。 ⏱ 2024-10-12 14:10:45 ^695233-48-100308-100477\n危机纪年第208年，三体舰队距太阳系2.07光年 #\r📌 因为在昨天晚上的演讲中，你说人类迟迟未能看清宇宙的黑暗森林状态，并不是由于文明进化不成熟而缺少宇宙意识，而是因为人类有爱。 “这不对吗？” 对，虽然“爱”这个词用在科学论述中涵义有些模糊，但你后面的一句话就不对了，你说很可能人类是宇宙中唯一拥有爱的种族，正是这个想法，支撑着你走完了自己面壁者使命中最艰难的一段。 “当然，这只是一种表达方式，一种不严格的……比喻而已。” 至少我知道三体世界也是有爱的，但因其不利于文明的整体生存而被抑制在萌芽状态，但这种萌芽的生命力很顽强，会在某些个体身上成长起来。 “请问您是……” 我们以前不认识，我是两个半世纪前曾向地球发出警告的监听员。 “天啊，您还活着？”庄颜惊叫道。 也活不了多长时间了，我一直处于脱水状态，但这么长的岁月，脱水的机体也会老化。不过我真的看到了自己想看的未来，我感到很幸福。 “请接受我们的敬意。”罗辑说。 我只是想和您讨论一种可能：也许爱的萌芽在宇宙的其他地方也存在，我们应该鼓励她的萌发和成长。 “为此我们可以冒险。” 对，可以冒险。 “我有一个梦，也许有一天，灿烂的阳光能照进黑暗森林。” 这时，这里的太阳却在落下去，现在只在远山露出顶端的一点，像山顶上镶嵌着的一块光灿灿的宝石。孩子已经跑远，同草地一起沐浴在金色的晚霞之中。 太阳快落下去了，你们的孩子居然不害怕？ “当然不害怕，她知道明天太阳还会升起来的。” ⏱ 2024-10-12 15:08:48 ^695233-49-13253-14291\n危机纪元4年，云天明 #\r📌 程心似乎听到了他心中的话，她慢慢抬起头来，他们的目光第一次这么近地相遇，比他梦中的还近，她那双因泪水而格外晶莹的美丽眼睛让他心碎。 但接着，程心说出一句完全意外的话：“天明，知道吗？安乐死法是为你通过的。” ⏱ 2024-10-14 12:31:36 ^695233-55-21409-21540\n危机纪元1-4年，程心 #\r📌 “你会把你妈卖给妓院吗？”维德问。 ⏱ 2024-10-14 12:31:06 ^695233-56-2751-2768\n威慑纪元61年，执剑人 #\r📌 因为杀的人太少了。杀一个人是要被判死刑的，杀几个几十个更是如此，如果杀了几千几万人，那就罪该万死；但如果再多些，杀了几十万人呢？当然也该判死刑，但对于有些历史知识的人，这个回答就不是太确定了；再进一步，如果杀了几百万人呢？那可以肯定这人不会被判死刑，甚至不会受到法律的惩处，不信看看历史就知道了，那些杀人超过百万的人，好像都被称为伟人和英雄；更进一步，如果这人毁灭了一个世界，杀死了其中的所有生命，那他就成了救世主！ ⏱ 2024-10-15 01:16:23 ^695233-60-4506-4714\n📌 “很复杂，直接原因是：那个恒星系，就是他向宇宙广播了坐标导致其被摧毁的那个，不知道其中有没有生命，但肯定存在有的可能，所以他被指控有世界灭绝罪的嫌疑。这是现代法律中最重的罪了。” ⏱ 2024-10-16 14:17:58 ^695233-60-4831-4920\n📌 如果说面壁计划是人类历史上首次出现的怪物，那黑暗森林威慑和执剑人在历史上却有过先例。公元20世纪华约和北约两大军事集团的冷战就是一个准终极威慑。冷战中的1974年，苏联启动Perimeter计划，建立了一个后来被称为末日系统的预警系统，其目的是在北约核突袭中，当政府决策层和军队高级指挥层均被消灭、国家已失去大脑的情况下，仍具备启动核反击的能力。它利用核爆监测系统监控苏联境内的核爆迹象，所有的数据会汇整到中央计算机，经过逻辑判读决定是否要启动核反击。这个系统的核心是一个绝密的位于地层深处的控制室，当系统做出反击的判断时，将由控制室内的一名值班人员启动核反击。公元2009年，一位曾参加过Perimeter战略值班的军官对记者披露，他当时竟然只是一名刚从伏龙芝军事学院毕业的二十五岁的少尉！当系统做出反击判断时，他是毁灭的最后一道屏障。这时，苏联全境和东欧已在火海之中，他在地面的亲人和朋友都已经死亡，如果他按下启动反击的按钮，北美大陆在半个小时后也将同样成为生命的地狱，随之而来的覆盖全球的辐射尘和核冬天将是整个人类的末日。那一时刻，人类文明的命运就掌握在他手中。后来，人们问他最多的话就是：如果那一时刻真的到来，你会按下按钮吗？ 这位历史上最早的执剑人说：我不知道。 ⏱ 2024-10-16 14:26:20 ^695233-60-9487-10051\n📌 人们发现威慑纪元是一个很奇怪的时代，一方面，人类社会达到空前的文明程度，民主和人权得到前所未有的尊重；另一方面，整个社会却笼罩在一个独裁者的阴影下。有学者认为，科学技术一度是消灭极权的力量之一，但当威胁文明生存的危机出现时，科技却可能成为催生新极权的土壤。在传统的极权中，独裁者只能通过其他人来实现统治，这就面临着低效率和无数的不确定因素，所以，在人类历史上，百分之百的独裁体制从来没有出现过。但技术却为这种超级独裁的实现提供了可能，面壁者和持剑者都是令人忧虑的例子。超级技术和超级危机结合，有可能使人类社会退回黑暗时代。 ⏱ 2024-10-16 14:28:31 ^695233-60-10907-11168\n📌 “看，她是圣母玛丽亚，她真的是！”年轻母亲对人群喊道，然后转向程心，热泪盈眶地双手合十，“美丽善良的圣母，保护这个世界吧，不要让那些野蛮的嗜血的男人毁掉这美好的一切。” ⏱ 2024-10-16 23:16:52 ^695233-60-20469-20553\n威慑纪元62年，奥尔特星云外，“万有引力”号 #\r📌 “三维，在弦理论中，不算时间维，宇宙有十个维度，可只有三个维度释放到宏观，形成我们的世界，其余的都卷曲在微观中。” ⏱ 2024-10-16 23:24:46 ^695233-61-13721-13778\n威慑纪元最后十分钟，62年11月28日16：17：34至16：27：58，威慑控制中心 #\r📌 在程心的潜意识中，她是一个守护者，不是毁灭者；她是一个女人，不是战士。她将用自己的一生守护两个世界的平衡，让来自三体的科技使地球越来越强大，让来自地球的文化使三体越来越文明，直到有一天，一个声音对她说：放下红色开关，到地面上来吧，世界不再需要黑暗森林威慑，不再需要执剑人了。 ⏱ 2024-10-17 01:26:30 ^695233-63-1797-1934\n威慑后一小时，失落的世界 #\r📌 “这都是为什么？”程心喃喃地问，更像是问自己。 “因为宇宙不是童话。” ⏱ 2024-10-17 01:30:38 ^695233-64-4901-4964\n第三部 #\r📌 安逸的美梦彻底破灭，黑暗森林理论得到了最后的证实，三体世界被摧毁了。 ⏱ 2024-10-17 15:58:32 ^695233-70-5401-5435\n广播纪元7年，云天明的童话 #\r📌 AA拿过程心叠好的带篷的小纸船，称赞很漂亮，然后示意程心也进浴室。在盥洗台上，她用小刀片从香皂上切下了小小的一片，然后把小纸船的尾部扎了一个小孔，把那一小片香皂插入小孔中，抬头对程心神秘地一笑，轻轻地把纸船放进已灌满水并且水面已经平静下来的浴缸中。 小船向前移动了，在这片小小的水面上，从此岸航向彼岸。 程心立刻明白了原理：香皂在水中溶解后，降低了小船后方水面的张力，但船前方水面的张力不变，小船就被前方水面的张力拉过去了￼。但这个想法转瞬即逝，程心的思想随即被一道闪电照亮！在她的眼中，浴缸中平静的水面变成了漆黑的太空，白色的小纸船在这无际的虚空中以光速航行… ⏱ 2024-10-17 22:30:43 ^695233-73-48751-49192\n📌 每秒十六点七千米，太阳系的第三宇宙速度，如果达不到这个速度就不可能飞出太阳系。 光也一样。 如果太阳系的真空光速降到每秒十六点七千米以下，光将无法逃脱太阳的引力，太阳系将变成一个黑洞￼。 ⏱ 2024-10-17 22:40:01 ^695233-73-63863-64231\n广播纪元8年，命运的抉择 #\r📌 这让我想起了那天夜里峨眉山的云海，”瓦西里说，“那是中国的一座山，在那山的顶上看月亮是最美的景致。那天夜里，山下全是云海，望不到边，被上空的满月照着，一片银色，很像现在看到的样子。” ⏱ 2024-10-17 22:55:27 ^695233-74-19329-19420\n📌 “其实吧，从科学角度讲，毁灭一词并不准确，没有真正毁掉什么，更没有灭掉什么，物质总量一点不少都还在，角动量也还在，只是物质的组合方式变了变，像一副扑克牌，仅仅重洗而已……可生命是一手同花顺，一洗什么都没了。” ⏱ 2024-10-17 22:55:16 ^695233-74-19474-19578\n第五部 #\r📌 剩下的事就是清理了，歌者再次从仓库中取出那个质量点。他突然想到清理弹星者是不能用质量点的，这个星系的结构与前面已死的那个星系不同，有死角，用质量点可能清理不干净，甚至白费力气，这要用二向箔才行。可是歌者没有从仓库里取二向箔的权限，要向长老申请。 ⏱ 2024-10-18 13:45:16 ^695233-79-5867-5989\n掩体纪元66年，太阳系外围 #\r📌 白Ice笑了起来，“再简单不过的事，你忘记《古兰经》中的故事了？如果大山不会走向穆罕默德，穆罕默德可以走向大山。” ⏱ 2024-10-18 14:26:27 ^695233-81-7378-7435\n📌 丁仪接着说：“在危机初期，当智子首次扰乱加速器时，有几个人自杀。我当时觉得他们不可理喻，对于搞理论的，看到那样的实验数据应该兴奋才对。但现在我明白了，这些人知道的比我多，比如杨冬，她知道的肯定比我多，想得也比我远，她可能知道一些我们现在都不知道的事。难道制造假象的只有智子？难道假象只存在于加速器末端？难道宇宙的其他部分都像处女一样纯真，等着我们去探索？可惜，她把她知道的都带走了。” ⏱ 2024-10-18 14:45:18 ^695233-81-10117-10309\n📌 “我说别傲慢，弱小和无知不是生存的障碍，傲慢才是，想想水滴吧！” ⏱ 2024-10-18 15:50:30 ^695233-81-12293-12325\n📌 “现在逃离，就像在瀑布顶端附近的河面上划船，除非超过一个逃逸速度，否则不论怎样划，迟早都会坠入瀑布，就像在地面向上扔石头，不管扔多高总会落回来。整个太阳系都在跌落区，从中逃离必须达到逃逸速度。” “逃逸速度是多少？” “我反复计算过四遍，应该没错。” “逃逸速度是多少？！” “启示”号和“阿拉斯加”号上的人们屏息凝神，替全人类倾听末日判决，白Ice把这判决平静地说出来： “光速。” ⏱ 2024-10-18 15:55:33 ^695233-81-15419-15751\n掩体纪元67年，二维太阳系 #\r📌 程心现在回想起两次看到《星空》时奇怪的感觉：画面中星空之外的部分，那火焰般的树，暗夜中的村庄和山脉，都呈现出明显的透视和纵深；但上方的星空却丝毫没有立体感，像挂在夜空中的一幅巨画。 因为星空是二维的。 他是怎么画出来的？1889年的凡·高，精神第二次崩溃的凡·高，难道真的用分裂和谵妄的意识，跨越五个多世纪的时空，看到了现在？！或者反过来，他早就看到了未来，这最后审判日的景象才是他精神崩溃和自杀的真正原因？！ ⏱ 2024-10-18 16:17:09 ^695233-83-16582-16843\n📌 隧洞前的罗辑笑了笑，“我要是想走，刚才就跟你们走了，我这样岁数的人，不适合远航了。孩子们，不要为我操心了，我说过的，我什么都没有失去。准备启动空间曲率驱动。” 罗辑的最后一句话是对飞船A.I.说的。 “航线参数？”A.I.问。 “目前航线的延长线吧，我也不知道你们要去哪儿，我想现在你们自己也不知道，要是想起了目的地，在星图上指出来就行了，半径五万光年内的大部分恒星，飞船都可以自动导航到达。” “指令执行中，空间曲率驱动引擎三十秒后启动。”A.I.说。 ⏱ 2024-10-18 16:19:44 ^695233-83-19371-19739\n📌 在宇宙中，曲率驱动航迹既可以成为危险标志，也能成为安全声明。如果航迹在一个世界旁边，是前者；如果把这个世界包裹在其中，则是后者。就像一个手拿绞索的人，他是危险的；但如果他把绞索套到自己的脖子上，他就变成安全的了。 ⏱ 2024-10-18 16:25:06 ^695233-83-22164-22270\n📌 “你们有个约会！”AA说。 “是的，我们有个约会。”程心机械地回答，感情的激荡使她处于呆滞状态。 “那就去你们的星星！” “好的，去我们的星星。”程心激动地对AA说。然后她问飞船A.I.，“能够定位DX3906恒星吗，这是危机纪元初的编号？” “可以，这颗恒星现在的编号是S74390E2，请确认。” ⏱ 2024-10-18 16:27:08 ^695233-83-26179-26441\n第六部 #\r📌 大气成分：氧35%，氮63%，二氧化碳2%，还有微量惰性气体，可以呼吸，但大气压只有0.53个地球标准气压，出舱后不要剧烈活动。”飞船A.I.说。 “站在飞船附近的那个生物是什么？”AA问。 “正常人类。”A.I.简单地回答。 ⏱ 2024-10-18 16:29:13 ^695233-84-4555-4724\n📌 也有人叫它末日飞船。那些光速飞船没有目的地，只是把曲率引擎开到最大功率疯狂加速，无限接近光速，目的就是用相对论效应跨越时间，直达宇宙末日。据他们计算，十年内就可以跨越五百亿年，那他们现在已经到了，哦，当然是以他们的参照系。其实，并不需要有意识地做这事，比如在飞船加速到光速后，曲率引擎出现无法修复的故障，使飞船不能减速，你也可能在有生之年到达宇宙末日。” ⏱ 2024-10-18 16:43:21 ^695233-84-13148-13325\n读书笔记 #\r本书评论 #\r","date":"Oct 16 2024","externalUrl":null,"permalink":"/notes/thethreebodyproblem/","section":"Thoughts","summary":"\u003ch1 class=\"relative group\"\u003e元数据 \r\n    \u003cdiv id=\"%E5%85%83%E6%95%B0%E6%8D%AE\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%85%83%E6%95%B0%E6%8D%AE\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cblockquote\u003e\n\u003cp\u003e[!abstract] 三体（全集）\u003c/p\u003e","title":"The Three Body Problem","type":"notes"},{"content":"Record some thinking\n","date":"Oct 16 2024","externalUrl":null,"permalink":"/notes/","section":"Thoughts","summary":"\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"Thoughts","type":"notes"},{"content":"Record some thinking\n","date":"Oct 14 2024","externalUrl":null,"permalink":"/blogs/","section":"Blogs","summary":"\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"Blogs","type":"blogs"},{"content":"","date":"Oct 14 2024","externalUrl":null,"permalink":"/tags/ontario/","section":"Tags","summary":"","title":"Ontario","type":"tags"},{"content":"\r","date":"Oct 14 2024","externalUrl":null,"permalink":"/blogs/ontariolake/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/OntarioLake/Ontario%20Lake-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/OntarioLake/Ontario%20Lake-2.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Ontario Lake","type":"blogs"},{"content":"","date":"Oct 14 2024","externalUrl":null,"permalink":"/tags/pic/","section":"Tags","summary":"","title":"Pic","type":"tags"},{"content":" Last Edit: 9/25/24\nYoung’s modulus change with Density #\rOrdered Solids #\r大多数固体材料都是Polycrystaline 多晶体的 尤其是Metal在原子尺度上是按照Crystal Structure排列的 Atomic scale尺度一般在\\(10^{-10}\\)的级别 Unit Cell #\r最小的Convenient Building Block Grains #\r多晶材料的这些晶体被称为晶粒（grains）。每个晶粒内部的原子排列是有序的，但不同晶粒之间的原子排列方向各不相同，这种微观结构对材料的机械性能和物理性质有重要影响 Short Range Order #\r指的是在局部区域内，原子或分子的排列是有规律的 玻璃、液体这样的材料，它们通常是Short Range Order的 Long Range Order #\r在整个材料中，原子或分子的排列是有规律的，并且这种规律性会一直延续到较大的尺度（远远超过单个原子的范围） 晶体材料，如金属和矿物，通常具有长程有序 Simple Cubic #\r简单的一个立方体上的八个角为Atoms的结构 其正方体的边长被定义为a n：1 Atom CN：6 Side Dimension: \\(a=2r\\) Hard Sphere Model #\r所有的Atoms都被当作一个球来模拟，主要是方便描述原子在晶体结构中的排列方式 Reduced Sphere Model #\rReal Graph #\r可以发现这样的Hard Shpere Model看着实在是Messy 所以就将所有Atoms简化为Reduced Sphere Model The Atomic Packing Factor 填充系数 #\r对于一个Unit Cell，APF描述了Atom对于体积的占有率，从100%完全占据到0%一点没有 $$APF=\\frac{Volume_{Spheres}}{Volumn_{Unit~Cell}}$$ 本质上就是一个百分比或者说分数 现在分别计算两个体积 对于Spheres来说，单个的体积为\\(\\frac{4}{3}\\pi R^3\\)，而将他乘上Number of Atoms后便可以得到所有的体积 对于Unit Cell来说，以a为边长，其体积自然为\\(a^3\\) Face Centred Cubic (FCC) Structure #\r结构特点：每个晶胞的八个顶点各有一个原子，此外每个面中心还有一个原子。 n：每个晶胞含有 4 个原子（8 个顶点原子各占 1/8，6 个面心原子各占 1/2）。 CN：12（每个原子有 12 个最近邻原子）。 APF：约 74%（原子占据的体积比例）。 例子：铝、铜、金、银等 在Simple Cubic的基础上加入一些Face Centred Atoms（在每个面中心的Atom） Number of Atoms in FCC #\r对于FCC来说，每一个角上都是\\(\\frac{1}{8}\\)个Atom 每一个面都有\\(\\frac{1}{2}\\)个Atom 所以一共就是\\(\\frac{1}{8}*8+\\frac{1}{2}*6=4\\)个Atom 即\\(n_{FCC}=4\\) Coordination Number of FCC #\r$$CN_{FCC}=12$$\nAtomic Packing Factor of FCC #\r则对于FCC Structure来说，其APF即为 $$APF=\\frac{4\\frac{4}{3}\\pi R^3}{a^3}$$ 现在要将Atom的radius与Unit Cell的边长a做替换好消掉其中一个 有\\(a^2+a^2=(4R)^2\\)，勾股定律 则有\\(2a^2=16R^2\\)，即\\(a_{FCC}=2\\sqrt{2}R\\) 将a带入后便有 $$APF = \\frac{4 \\left( \\frac{4}{3} \\pi R^3 \\right)}{(2\\sqrt{2}R)^3} \\ \\Rightarrow APF_{FCC} = 0.74$$ Avogadro\u0026rsquo;s constant 阿伏伽德罗常数 #\r通常用符号\\(N_A\\)表示，是指在1摩尔物质中包含的微观粒子（如原子、分子、离子等）的数量。其数值大约为： $$N_A=6.022×10^{23 }g\\cdot mol^{−1}$$\nTheoretical Density 理论密度 #\r$$\\text{Mass}{\\text{Atoms in Unit Cell}} = \\text{Number}{\\text{Atoms in Unit Cell}} \\cdot \\frac{\\text{Molar Mass of Atom}}{\\text{Avogadro\u0026rsquo;s Number}}$$\n$$m = n \\cdot \\frac{A}{N_A}$$ $$\\rho = \\frac{nA}{V_C N_A}$$\nDensity密度，即质量和体积的比值 \\(n\\)：Number of Atoms，一个Crystal里的完整分子个数 A: Molar Mass(g/mol)，原子相对质量 \\(V_c\\)：Unit Cell的体积 \\(N_A\\)：阿伏伽德罗常数\\(mol^{-1}\\) \\(\\frac{A}{N_A}\\)：\\(\\frac{g/mol}{mol^{-1}}=g\\)，等于一个Atom有几g，再乘上Atom Nuber得到全部的Mass Rock Salt Structure #\r是一个Common Ceramic的Crystal Structure 结构特点：这是离子晶体结构，通常由两种不同大小的离子（如Na⁺和Cl⁻）组成。大的离子（Cl⁻）形成一个面心立方（FCC）结构，小的离子（Na⁺）填充在八面体间隙中。 A：每个晶胞含有 4 个阳离子和阴离子。 CN：阳离子和阴离子的CN都为6。 APF：约 67%。 例子：氯化钠（NaCl）、氧化镁（MgO）等。 对于Ceramic Structure来说，其拥有多余一种的Atom类型，具体来说有Cation和Anion两种 其是Ionic Compound的一种特有的Crystal Structure 对于Rosk Salt Structure来说，其包含了两种Ion，即Anion（蓝色）与Cation（红色） Anions在Unit Cell的角上（Cation也可以在，他们描述的将是同一种结构，但一般来说体积大的Anion会先占据Unit Cell的角落，将小的Cation挤到中间去） 这个结构看起来像面心立方（FCC），但实际上不是纯粹的FCC，因为阳离子和阴离子之间有相互作用，会推开阴离子，使得阴离子不会像真正的FCC结构那样直接通过面对角线相互接触。 Number of Atoms in Rock Salt (Stoichiometry) #\r对于如NaCl这样的material，其Anion：Cation比都是1：1的，但上图中明显缺少了一个Cation 其正确的位置应该是整个Unit Cell的正中央 Coordination Number for Cations in Rock Salt #\r对于一个Rock Salt来说，拿最中间的Cation举例，可以发现与其接触的Atom有6个 于是就可以说\\(CationCoordinationNumber_{Rock~Salt} = 6\\) Density of Rock Salt #\r对于Rock Slat Structure来说，由于其Atom种类变为了Anion与Cation两个，其[[#Theoretical Density]]的分子也要对应的便为两个的和，即 $$\\rho=\\frac{n_CA_C+n_AA_A}{V_CN_A}$$ \\(n_CA_C\\)：Cation的Number和Moalr Mass，nA同理 对于Vc来说，其a变为了两个Atoms的Radus*2，有\\(V_C=(2R_A+2R_C)^3\\) Theoretical Density of Rock Salt #\r$$\\rho = \\frac{n_C A_C + n_A A_A}{V_C N_A}$$\nAnions #\r在Rock Salt Structure中的Anion看似处于FCC的位置中 但实际上由于附近的Cation和不同Anion之间的相互作用力导致Anion实际上不在精确的FCC位置上 Cations #\rthe cations will always touch their nearest neighbour anions The Body Centred Cubic Crystal (BCC) Structure #\r结构特点：每个晶胞的八个顶点各有一个原子，且晶胞中心还有一个原子。 n：每个晶胞含有 2 个原子（8 个顶点原子各占 1/8，中心原子占 1 个）。 CN：8（每个原子有 8 个最近邻原子）。 APF：约 68%。 例子：铁、钨、铬等。 Number of Atoms in BCC #\r$$\\frac{1}{2}*2+\\frac{1}{8}*8=2$$\nCoordination Number of BCC #\r$$CoordinationNumberBCC​=8$$\nAtomic Packing Factor for BCC #\r$$APF=\\frac{Volume_{Spheres}}{Volumn_{Unit~Cell}}$$\n对于BCC，要取其Unit Cell边长与Atom radius关系得用Cubic Diagonal，即 $$3a^2=16R^2，\\Rightarrow a_{BCC}=\\frac{4}{\\sqrt3}R$$ $$APF = \\frac{2 \\left( \\frac{4}{3} \\pi R^3 \\right)}{(\\frac{4}{\\sqrt3})^3} \\ \\Rightarrow APF_{BCC} = 0.68$$\nInterstitial Sites #\rSpace between other atoms 其体积就代表了Crystal Structure中的间隙的部分 By convention, we name interstitial sites according to the solid geometry that they create Octanhedron Interstitial Site #\r对于Rock Salt中的Anion的Interstitial Sites，将他们命名为Octanhedron Interstitial Site Coordination Number of a Interstitial Site #\r对于Interstitial Site来说，其CN代表了Interstitial Site中心点位置的Atom与最近Atom接触的个数 对于Rock Salt来说，Intersitital Site CN = 6 Simple Cubic Interstitial Site #\r结构特点：每个晶胞的八个顶点各有一个原子，顶点原子通过边连接，但面心和体心没有原子。 n：1Atom（8 个顶点原子各占 1/8）。 CN：6（每个原子有 6 个最近邻原子）。 APF：约 52%。 例子：钋（唯一的自然存在的例子）。 纯的简单立方晶格中，中心位置是空的（这个红点代表的就是Intersititial Site间隙位的大小 但在体心立方或其他某些间隙结构中，这一位置可以被其他原子或离子占据 Coordination Number of Simple Cubic #\r$$CoordinationNumberSimpleCubic​=8$$\nThe Size of Interstitial Sites #\r就Interstitial Site来说，其具有实际大小 前面Rock Salt中提到过Cations will always touch their nearest neighbour anions 阳离子总是会接触它们最近的阴离子，因此阳离子只有在足够大时才会占据晶体结构中的间隙位。如果阳离子太小，它将无法与最近的阴离子接触，因此不会稳定地占据间隙位 $$\\sin 45 = \\frac{2R_A}{2R_A + 2R_C} \\\n(2R_A + 2R_C)\\sin 45 = 2R_A \\\n2R_A\\sin 45 + 2R_C\\sin 45 = 2R_A \\\nR_A\\sin 45 + R_C\\sin 45 = R_A \\\n\\frac{R_A}{R_A}\\sin 45 + \\frac{R_C}{R_A}\\sin 45 = 1 \\\n\\sin 45 + \\frac{R_C}{R_A}\\sin 45 = 1 \\\n\\frac{R_C}{R_A}\\sin 45 = 1 - \\sin 45 \\\n\\frac{R_C}{R_A} = \\frac{1 - \\sin 45}{\\sin 45}=0.414$$\nHexagonal Close Packed (HCP) Structure #\r结构特点：原子以六边形排列，沿c轴有堆叠的结构。原子层是按照ABAB\u0026hellip;的顺序堆叠。 n：每个晶胞含有 6 个原子（从整体堆叠考虑）。 CN：12（类似于FCC，每个原子有12个最近邻原子）。 APF：约 74%。 ex.：镁、钛、锌等 Number of Atoms in HCP #\rCoordination Number of HCP #\rCN = 12 Atomic Packing Factor in HCP #\rHCP有着和FCC一样的APF（都为0.74）所以FCC有时会被称为CCP Different to FCC #\rFCC和HCP的主要区别在于原子的堆积顺序 FCC中的原子堆积顺序是ABCABC 而HCP中的堆积顺序是ABAB 尽管它们的堆积顺序不同，但由于原子排列非常紧密，它们的APF都是0.74 Close Packed #\r要想让Atom排列的更加紧密，需要有Close Packed的结构 当Atom在这种排列下，APF才能更高 在Not Close Packed下，这种排列的方式更加的松散 Closed Packed Plane #\rNot Closed Packed Plane in FCC #\r对于FCC的侧面上的Atom来说，其并不处于Closed Packed状态下 ","date":"Sep 25 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms4.thestructureproperty/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 9/25/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eYoung’s modulus change with Density \r\n    \u003cdiv id=\"youngs-modulus-change-with-density\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#youngs-modulus-change-with-density\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS4.TheStructureProperty/ECMS4.TheStructure-Property-24.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 4. The Structure Property","type":"docs"},{"content":" Last Edit 9/24/24\nHooke’s Law #\r$$F=kx$$\nSpring constant (k): Spring Constant Material properties（材料性能） #\r主要指材料的基本机械性能，如弹性模量、屈服强度、抗拉强度等 它们的计算方式应当剔除几何尺寸的影响 Stress 应力 #\r物体在外力作用下，单位面积上承受的内力 $$Stress(\\sigma)=\\frac{F}{A_0}$$ \\(A_0\\): Initial Cross-Sectional Area 应力的单位通常是帕斯卡（Pa） Strain 应变 #\r应变是材料在外力作用下发生的变形程度 $$Strain(\\epsilon)=\\frac{\\Delta l}{l_0}$$\n\\(\\Delta l\\)：change of material\u0026rsquo;s length \\(l_0\\): Initial length of material Young\u0026rsquo;s Modulus 杨氏模量 #\r是固体在载荷下的刚度或对弹性变形的抵抗力的量度\n材料在压缩或拉伸时会发生Elastic Deformation，而在Unloaded之后则会回到之前的Equilibrium $$E=\\frac{\\sigma}{\\epsilon}=\\frac{\\frac{F}{A_0}}{\\frac{\\Delta l}{l_0}}=\\frac{F\\cdot l_0}{A_0\\cdot \\Delta l}$$\nYoung\u0026rsquo;s Modulus (E)\n单位是Pa\nStructure Independent #\r当说\u0026quot;XXX is xxx independent\u0026quot; 时，代表了某个事物不依赖于某个特定因素 这里则是Young\u0026rsquo;s Modulus是不依赖于Structure Young\u0026rsquo;s Modulus其只与材料有关，与形状无关 具体来说是取决与材料的原子级别的相互作用，而不是材料的宏观或微观结构 Micro Perspective of Stress and Strain #\r从微观角度来看，物体由Atoms组成，其中存在Inter Atomic Forces 当Applied External Force的时候，物体将处于Loaded状态，其Shape将会发生改变 具体来说，Shape发生的改变是由于Atoms之间的间距发生了改变 不过只要整个Stress小于Yield Strength，所有的Deformation都将是Elastic的 代表了，当External Force被撤去的时候，既Unloaded之后，Atoms将会回到他们原来的Equilibrium position 需要注意的是，Atom之间的间距将会回到一开始的\\(r=r_0\\) 所以可以得出一个结论：Elastic Strain is Reversible Stress-Strain Curve 应力-应变图 #\r本图实际上为F-r图，即拉力-原子间半径图，但与Stress-Strain图相似，便用SS图讲解\n对于一个材料，在对其施加Stress的时候，其Strain会出现如此的固定趋势 在Stress等于0的时候，物体处于Equilibrium状态，具体来说其Attractive Force = Repulsive Force 在持续施加Stress后，Atoms最终将到达一个Yield Strength（不可逆点）后将会产生Plastic Deformation（将在下一章提到） 将Stress-Strain图放大到\\(r_0\\)两边后观察 可以发现Stress随Strain(Atomic Spacing)基本呈现Linear Trend，便可以说 $$E\\propto \\frac{dF}{dr}|_r=r_0$$ Young\u0026rsquo;s Modulus is directly propotional to slope of interatomic force speration curve at equilibrium spacing 简单的理解便为：杨氏模量等于与Stress对于Strain的变化率 更具图可以看出Stress对Strain的变化速率越大，其Young\u0026rsquo;s Modulus越大 即在Plastic Deformation前，Stress（External Force）越大材料的Young\u0026rsquo;s Modulus越大 既抵抗Elastic Deformation的作用越大 The way to determine material properties #\rTensile(Tension) Test\n图中展示了拉伸测试的基本原理，即通过对材料施加拉力（Tension），使其伸长（elongating），从而获得应力-应变曲线等相关数据 Grip Region（夹持区）：这是样品被测试机夹持的地方，两端施加拉力 Reduced Section（缩小部分）：这是样品的中间区域，它的截面积被减少，以确保样品在这个区域发生断裂或变形。在该区域内，材料会受到均匀的拉伸应力。 通过拉伸测试，可以获得材料的关键参数，如杨氏模量 (Young\u0026rsquo;s Modulus)、屈服强度 (Yield Strength)、极限抗拉强度 (Ultimate Tensile Strength, UTS) 和断裂延伸率 (Fracture Elongation) Disadvantage of Tensile Test #\r这种测试方法仍然存在局限性，例如Ceramics \u0026amp; Glasses等都不能利用这种方式测试Material Properties，具体来说 Low strain to fracture, almost no deformation before breaking：在断裂之间，几乎不发生Elastic Deformation hard to grip：对于Tensile Test所必须的Grip Region，由于一些Material的特殊性，他们无法在Grip Region内被夹住，继而无法进一步测试 Hard to load on Axis: 当Grip Region滑动的时候，无法使力沿轴拉伸 关于为什么这些会发生，如为什么Ceramics会直接断裂等，参考[[ECMS 3. Plastic Deformations]] ","date":"Sep 24 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms2.elasticbehavior/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 9/24/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eHooke’s Law \r\n    \u003cdiv id=\"hookes-law\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#hookes-law\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e$$F=kx$$\u003c/p\u003e","title":"ECMS 2. Elastic Behavior","type":"docs"},{"content":"","date":"Sep 23 2024","externalUrl":null,"permalink":"/tags/cover/","section":"Tags","summary":"","title":"Cover","type":"tags"},{"content":"\rLast Edit: 9/23/24\nPlastice Deformation (Permanent Deformation) #\rwe use the term plastic to describe permanent deformation 之所以是Plastic，是因为它derives from the Greek plastikos meaning to sculpt Changes After Plastic Deformation #\r在Plastic Deformation后，Atomic Spacing将保持\\(r=r_0\\) 但是Sequence of atoms将进入一个New Equilibrium 即在Marcro Perspective上发生Shape的Deform Tensile Strain将会保持一定非零大小 Micro Perspective of Plastic Deformation #\rDifferent between Elastic and Plastic Deformation #\rElastic #\r对于Elastic Deformation，开始前物理Atom之间间距应为\\(r_0\\) 泄力后仍应该是\\(r_0\\)，并且Atom将会到他们原有的Equilibrium 并且物体从Marco Perspective上并不发生Deformation Plastic #\r结束后Atom之间间距仍应该是\\(r_0\\) 泄力后Atom将进入一个新的Equilibrium 物体在泄力后，他的Tensile Strain将不会便为0而是保持在一定数 即Shape已经发生了Perminant Change Beyond Elastic Region #\r在Elastic Region外，便是完整的[[ECMS 2. Elastic Behavior#Young\u0026rsquo;s Modulus 杨氏模量]]的模型 Yield Strength: 屈服强度是指材料在发生永久变形之前，能够承受的最大应力。 当Strain到达Yield Strength之后，材料会从Elastic Deformation转变为Plastic Deformation，即Material发生Permanent Deformation 在过了Yield Strength之后Strain再增加后到了一定程度之后便会产生Fracture(Broken into pieces) Stress-Strain Curve for different materials #\rMetals #\r图中的绿色曲线 其特点有在一定位置之后开始产生Permanent Deformation 之后在持续的施加Stress之后其Strain变化率降低最后产生Fracture 相比于Ceramic和Polymer，其Young\u0026rsquo;s Modulus处于中间位置，高于Polymer但小于没有Permanent Deformation阶段的Ceramic Polymer #\r相对来说没有什么特点 具有较低的Young\u0026rsquo;s Modulus和Permanent Deformation区间 Ceramic #\r对于陶瓷类的物质，其没有Permanent Deformation的区间 对于他来说也存在Elastic Region 但可以看出整体Young\u0026rsquo;s Modulus非常高，并且呈现线性 在施加了一定的Stress后会直接Load enough and fracture Three-Point-Blending Test #\r底部两个点用作支撑，上方一个力将物体往下压 $$Stress(\\sigma)=\\frac{3FL}{2wh^2}$$\nTempered Glass 钢化玻璃 #\r![[ECMS 3. Plastic Deformations-5.png]]\n再高温下迅速向表面喷冷凝液将其降温 冷却将使玻璃表面收缩的比内部更快，产生了向心的Compress Stress 而内部由于受力将产生反作用力，向外产生张应力 整体的结构处于一个向内部收缩的趋势，导致当其受到了外力的时候，尝试使其Fracture的导致分子之间结构被破坏的力将被抵消 并且由于其内部存在Residual Stress（残余应力）导致了整体结构破坏的时候其Residual Stress将破坏结构至非常小的结构 ","date":"Sep 23 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms3.plasticdeformation/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 9/23/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch4 class=\"relative group\"\u003ePlastice Deformation (Permanent Deformation) \r\n    \u003cdiv id=\"plastice-deformation-permanent-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#plastice-deformation-permanent-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h4\u003e\r\n\u003cul\u003e\n\u003cli\u003ewe use the term \u003cstrong\u003eplastic\u003c/strong\u003e to describe permanent deformation\u003c/li\u003e\n\u003cli\u003e之所以是Plastic，是因为它derives from the Greek \u003cstrong\u003eplastikos\u003c/strong\u003e meaning to sculpt\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch5 class=\"relative group\"\u003eChanges After Plastic Deformation \r\n    \u003cdiv id=\"changes-after-plastic-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#changes-after-plastic-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h5\u003e\r\n\u003cul\u003e\n\u003cli\u003e在Plastic Deformation后，Atomic Spacing将保持\\(r=r_0\\)\u003c/li\u003e\n\u003cli\u003e但是Sequence of atoms将进入一个New Equilibrium\u003c/li\u003e\n\u003cli\u003e即在Marcro Perspective上发生Shape的Deform\u003c/li\u003e\n\u003cli\u003eTensile Strain将会保持一定非零大小\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch5 class=\"relative group\"\u003eMicro Perspective of Plastic Deformation \r\n    \u003cdiv id=\"micro-perspective-of-plastic-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#micro-perspective-of-plastic-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h5\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations-1.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations-2.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 3. Plastic Deformation","type":"docs"},{"content":" Ramsay, S. (2024). Engineering Chemistry \u0026amp; Materials Science. Top Hat. https://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b. ","date":"Sep 23 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/","section":"Docs","summary":"\u003cul\u003e\n\u003cli\u003eRamsay, S. (2024). Engineering Chemistry \u0026amp; Materials Science. Top Hat. \u003ca href=\"https://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b\" target=\"_blank\"\u003ehttps://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e","title":"Engineering Chemistry \u0026 Materials Science","type":"docs"},{"content":" Last Edit 7/4/24\n通货膨胀 #\r钱的贬值 CPI Consumer Price Index 消费者物价指数 #\r美国CPI #\r解释通货膨胀的理论 #\rKeynesian 凯恩斯主义 #\rNeo-Keynesian 新凯恩斯主义 #\rMonetarism 货币主义 #\r根本原因 #\r通货膨胀的可控性 #\r非常难以控制 通货膨胀的后果 #\r当把钱藏在床底下时，其购买力在不断的下降 逼迫人们不断的生产和消费 对于经济体来说，主要目标是提高GDP，人均消费水平 而通过适量的通货膨胀可以有助于提高人们的生产热情，提高GDP，对于整个经济体来说是好的 Misery Index 痛苦指数 #\rDeflation 通货紧缩 #\r今年的钱存着会比明年更加之前 90年代的日本就是典型例子 人们就不会消费，不花钱不生产，进入一个低欲望社会，GDP也就下降 HyperInflation 恶心通货膨胀 #\r对于经济的打击是毁灭性的 Target Inflation Rate 目标通胀 #\r保住通胀是底线，即使可能导致段时间的经济下降 导致Inflation的原因 #\r因素一：Demand-Pull 需求拉动 #\r刺激需求，导致产量的上升与价格的上升，形成一个对于经济上升的良心循环 但可以发现副作用是Inflation 政府可以通过直接发钱达到刺激需求，其可以直接刺激经济，但要面临通货膨胀的风险 可以总结出，Inflation和Economic Growth注定是会绑定在一块的 产能过剩 #\r产能直接影响了刺激需求后Inflation的变化 当产能过剩的情况下，即使总需求上升，由于能有产能，并不需要大幅度提高价格以达到供应需求的目的 相反，当产能不够的情况下，需求的价格必定会上涨导致Inflation的发生 判断产能的方式 - Unemployment Rate 失业率 #\r当失业率高的情况下，说明劳动力很多都在休息，既属于产能过剩的情况 所以一般Inflation发生前都会存在Low Unemployment Rate的情况 Philips Curve 菲利普斯曲线 #\r钱发不到实体经济中 #\r所以最简单的方式就是政府直接发钱，直接发到人手里 Cost-push 成本上涨 #\r成本上涨导致的价格上涨 反而抑制了需求，百害而无一益 俄罗斯通胀 #\r由于卢布的下跌导致的成本上升导致的Cost-push Inflation Money Supply 过量的货币供给 #\r几乎所有的HyperInflation都是政府过度印钱导致的 为什么要不断印钱 #\r政府既然看到了为什么不停止 因为陷入了恶性循环 一般都是外因导致的，如战争 政府无节制印钱后，货币量增加，大家需求也就上涨，来到了Demand-Pull中 但由于政府印钱速度太快，导致了实际上Demand-Pull的Inflation来到了不可抑制的情况，但这时候其实还没达到HyperInflation 这时候人民已经有了足够的钱，并且不会存钱，导致了大家都不再工作，导致产量的下降，导致成本的上升，最终进入恶性循环 预期的Inflation #\r当预期了Inflation时，大家都会超前消费，货币流通速度增加了 通货膨胀就自己发生了 ","date":"Jul 4 2024","externalUrl":null,"permalink":"/docs/economic/inflation/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 7/4/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e通货膨胀 \r\n    \u003cdiv id=\"%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cul\u003e\n\u003cli\u003e钱的贬值\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003eCPI Consumer Price Index 消费者物价指数 \r\n    \u003cdiv id=\"cpi-consumer-price-index-%E6%B6%88%E8%B4%B9%E8%80%85%E7%89%A9%E4%BB%B7%E6%8C%87%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#cpi-consumer-price-index-%E6%B6%88%E8%B4%B9%E8%80%85%E7%89%A9%E4%BB%B7%E6%8C%87%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e美国CPI \r\n    \u003cdiv id=\"%E7%BE%8E%E5%9B%BDcpi\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E7%BE%8E%E5%9B%BDcpi\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/Economic_Static/Inflation/Inflation%282%29.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Inflation 通货膨胀","type":"docs"},{"content":" Last Edit 4/15/24\nRegression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式\n[[Regression 回归]] 3.1.1 线性回归的基本元素 #\rLinear Regression 线性回归可以追溯到19世纪，其基于几个基本的假设 假设自变量与因变量之间为线性关系 假设噪声正常，如遵循正态分布 3.1.1.1 线性模型 #\r[[线性模型]] 线性假设是指目标可以表示为特征的加权和，如下例子 E.X. $$Price=w_{area}\\cdot area+w_{age}\\cdot age+b$$\nw称为Weight权重 b称为Bias，Offset或者Intercept偏置，即特征为0时的预测值 严格来说，上式为输入特征的一个[[Affine Transformation 仿射变换]]\n给定一个数据集，我们的目标即为寻找模型的Weight和Offset 高维数据集 #\r在Deep Learning 领域，我们通常使用的是高纬数据集，建模时采用[[2.3 线性代数]]的表示方法会比较方便。 当我们的输入包含多个特征时，我们将预测结果表示为\\(\\hat{y}\\) 点积形式 #\r可以用点积形式来简洁的表达模型\\(x\\in R^d,w\\in R^d\\) $$\\hat{y}=w^Tx+b$$ Model Parameters 模型参数 #\r在开始寻找最好的模型参数前，我们还需要两个东西 一种模型质量的度量方式 #\r一种能更新模型以提高预测质量的方式 #\r3.1.1.2 损失函数 #\r在开始考虑如何用模型Fit 拟合数据之前，我们需要一个拟合程度的度量 Loss Function 损失函数 #\r量话目标的实际值和预测值之间的差距 通常选用非负数作为Cost，并且数值越小损失越小 平方误差函数 #\r回归问题中最常用的Cost Function是平方误差函数 $$l^{(i)}(w,b)=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$ 常数\\(\\frac{1}{2}\\)的存在不会带来本质的差别，但当我们对这一方程求导后由于\\(\\frac{1}{2}\\)的存在会使常数等于1 由于平方误差函数中的二次方项，会导致估计值和观测值之间较大的差异造成更大的损失。 为了度量模型在整个数据集上的质量，我们需要计算训练集上的样本损失均值 $$L(w,b)= \\frac{1}{n}\\Sigma^n_{i=1}l^{(i)}(w,b)=\\frac{1}{n}\\Sigma^n_{i=1}\\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2$$ 总的来说训练模型就是为了找到一组参数\\(w^,b^\\)，其 $$w^,b^=argmin~L(w,b)$$ 3.1.1.3 解析式 #\r线性回归是一个很简单的优化问题，与大部分模型不同，其解可以用一个公式简单的表达出来，这类解便称为Analytical Solution 解析解 3.1.1.4 随机梯度下降 #\r即使在无法得到解析解的情况下，我们可以有效的训练模型 Gradient Descent 梯度下降 #\r最简单的方法就是计算Cost Function关于模型参数的导数（梯度） 但由于每次操作前都需要遍历整个数据集，导致执行速度非常之慢 所以通常会在每次更新时候抽取一小批样本，即为Minibatch Stochastic Gradient Descent 小批量随机梯度下降 SGD #\rMinibatch Stochastic Gradient Descent 小批量随机梯度下降 每次迭代中，我们首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数，并从当前参数的值中减掉 $$(w,b)\\leftarrow(w,b)-\\frac{\\eta}{|B|}\\Sigma_{i\\in B}\\partial_{w,b}l^{(i)}(w,b)$$ 初始化模型参数的值 从数据集中随机抽取小批量样本在负梯度方向上更新参数，并一直迭代 \\(\\eta\\)表示Learning Rate 学习率 B表示Batch Size 批量大小 Hyperparameter 超参数 #\r这些可以调整但不在训练过程中更新的参数称为超参数 Hyperparameter Tuning为调整Hyperparameter的过程 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的 收敛 #\rLinear Regression只会让预测值无限接近于实际值而却不能在有限的步数内非常精确地达到最小值 Generalization 泛化 #\r寻找到一组合适的Hyperparameter纵然困难，但更加困难的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为Generalization 泛化 3.1.1.5 用模型进行预测 #\r需要指出的是，Deep Learning对于实际值的接近更多是一种Prediction预测而非Inference推断 3.1.2 矢量化加速 #\r为了同时处理整个小批量的样本，同时防止在python中编写开销高昂的for循环 矢量化性能测试 #\r实例化两个全为1的10000维向量，采取两种处理方式，Python的for循环和对+的调用 ###初始化两个Tensor\rn = 10000\ra = torch.ones([n])\rb = torch.ones([n])\r###用for循环完成一次\rc = torch.zeros(n)\rtimer = Timer()\rfor i in range(n):\rc[i] = a[i] + b[i]\rf\u0026#39;{timer.stop():.5f} sec\u0026#39;\r###用线性代数完成矢量化运算\rtimer.start()\rd = a + b\rf\u0026#39;{timer.stop():.5f} sec\u0026#39; 得到的结果为 0.167sec 和0.00042 sec 3.1.3 正态分布与平方损失 #\r接下来，我们通过对噪声分布的假设来解读平方损失目标函数 正态分布 #\r[[Normal Distribution 正态分布]] 均方误差损失函数 #\r均方损失可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布，如下 $$y=w^Tx+b+\\epsilon$$ \\(\\epsilon\\)代表了噪声 Likehood 似然 #\r[[Likehood 似然]] 通过给定的x观测到特定y的似然（likelihood） $$p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)$$ 对于似然函数\\(L(\\theta|data)=P(data|\\theta)= \\Pi^N_{i=1}P(x_i|\\theta)\\) 已知x的正态分布密度函数，也就是x（\\(\\theta\\)取每个值的概率） 要求得给定x（\\(\\theta\\)）（其不固定，但遵循正态分布）观测到特点y的似然，得到公式 \\(L(x|y)=P(y|x)\\) 已知\\(p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)\\) 由于\\(P(y|x)=\\Pi^N_{i=1}P(y^{(i)}|x^{(i)})\\)（由x的参数条件下观测到y的可能性为独立的N个子事件的乘积） 根据[[Maximum Likehood Estimation 极大似然估计]]，参数w和b的最优值是使整个数据集的[[Likehood 似然]]最大的值 但又因为乘积最大化问题十分复杂，并且由于历史遗留问题，优化常说的不是最大化，而是最小化 所以我们需要通过最小化对数似然\\(-logP(y|x)\\)，由此可以得到的数学公式为 $$-logP(y|x)=\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(y^{(i)}-w^Tx^{(i)}-b)^2$$ 推导如下 \\(-logP(y|x)=-log\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)\\) \\(=-log\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)}\\) \\(=-log(\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}})+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) \\(=-log\\sum\\limits^n_{i=1}(2\\pi\\sigma^2)^{-\\frac{1}{2}}+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) = \\(\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) 现在我们只需要假设�是某个固定常数就可以忽略第一项\\(\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)\\)，因为第一项不依赖于w和b 对于第二项，除了常数\\(\\frac{1}{\\sigma^2}\\)外，其余部分与[[#平方误差函数]]是一样的 平方误差函数 #\r$$l^{(i)}(w,b)=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$\n幸运的是，上面式子的解并不依赖于\\(\\sigma\\) 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计 3.1.4 从线性回归到深度网络 #\r到目前，我只谈论了线性模型，而神经网络涵盖了更为丰富的模型，并且我们也可以用描述神经网络的方式来描述线性模型，从而把线性模型看作一个神经网络。可以用“层”符号来重写这个模型 3.1.4.1 神经网络图 #\r制图表可以可视化模型中正在发生的事情，但该图只显示了链接模式，而不包含权重和偏置的值 在上图中，输入为\\(x_1,\\dots x_d\\)，可知输入层的Feature Dimensionality 输入数（或称为特征维度）为d\n网络的输出层为\\(o_1\\)，因此输出层的输出数为1\n需要注意的是，输入值都是已经给定的，并且只有一个_计算_神经元。 由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。\nFully-Connected Layer 全连接层 #\r对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（Fully-connected layer）或称为稠密层（dense layer）。 3.1.4.2 生物学 #\r即使观察真实的神经元，但当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感\n我们援引斯图尔特·罗素和彼得·诺维格在他们的经典人工智能教科书 Artificial Intelligence:A Modern Approach (Russell and Norvig, 2016) 中所说的：虽然飞机可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。 同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。\n","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 4/15/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eRegression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式\u003c/p\u003e","title":"D2L 3.1 Linear Regression","type":"docs"},{"content":"从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。\n3.2.1 生成数据集 #\r为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集 使用线性模型参数\\(w=[2,-3.4]^T,b=4.2\\)和噪声项\\(\\epsilon\\)生成数据集及其标签 $$y=Xw+b+\\epsilon$$ \\(\\epsilon\\)可以视为模型预测和标签时的潜在观测误差 3.2.2 读取数据集 #\r3.2.3 初始化模型参数 #\r通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。 w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\rb = torch.zeros(1, requires_grad=True) 在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据 并运用[[2.5 自动微分]]来计算梯度 3.2.4 定义模型 #\r这里我们用的还是线性模型，即$$\\hat y=w^Tx+b$$ def linreg(X, w, b): #@save\r\u0026#34;\u0026#34;\u0026#34;线性回归模型\u0026#34;\u0026#34;\u0026#34;\rreturn torch.matmul(X, w) + b 3.2.5 定义损失函数 #\r模型建立后，开始使用对原函数的损失函数进行梯度下降 这里我们使用[[3.1_LinearRegression#平方误差函数]] 3.2.6 定义优化算法 #\r使用[[3.1_LinearRegression#Minibatch Stochastic Gradient Descent 小批量随机梯度下降]] 3.2.7 训练 #\r本质为执行一下循环 初始化参数 更新梯度，更新参数 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/","section":"Docs","summary":"\u003cp\u003e从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。\u003c/p\u003e","title":"D2L 3.2 Object-Oriented Design for Implementation","type":"docs"},{"content":"本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型\n3.3.1 生成数据集 #\rimport numpy as np\rimport torch\rfrom torch.utils import data\rfrom d2l import torch as d2l\rtrue_w = torch.tensor([2, -3.4])\rtrue_b = 4.2\rfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000) d2l.synthetic_data(true_w, true_b, 1000) 是 d2l 库中的一个函数调用。这个函数用于生成合成数据，其中包括特征数据和对应的标签数据。 具体地，这个函数接受三个参数： true_w：真实的权重，用于生成特征数据。 true_b：真实的偏置，用于生成特征数据。 1000：生成数据的数量，这里是指生成1000个样本。 3.3.2 读取数据集 #\rdef load_array(data_arrays, batch_size, is_train=True): #@save\r\u0026#34;\u0026#34;\u0026#34;构造一个PyTorch数据迭代器\u0026#34;\u0026#34;\u0026#34;\rdataset = data.TensorDataset(*data_arrays)\rreturn data.DataLoader(dataset, batch_size, shuffle=is_train)\rbatch_size = 10\rdata_iter = load_array((features, labels), batch_size) 3.3.3 定义模型 #\r对于标准深度学习模型，我们可以使用框架的预定义好的层 ## nn是神经网络的缩写\rfrom torch import nn\rnet = nn.Sequential(nn.Linear(2, 1)) nn.Sequential：这是 PyTorch 中用于构建顺序神经网络模型的类。它允许用户按顺序堆叠多个层或模块，构建神经网络模型。 nn.Linear(2, 1)：这里创建了一个全连接层，其中 nn.Linear 是 PyTorch 中用于定义全连接层的类。构造函数 nn.Linear(in_features, out_features) 接受两个参数： in_features：输入特征的数量。在这个例子中，输入特征的数量为 2。 out_features：输出特征的数量。在这个例子中，输出特征的数量为 1。 因此，net 这个模型包含一个具有 2 个输入特征和 1 个输出特征的全连接层。 这样的模型可以用于简单的二分类问题，其中输入特征有 2 个，输出特征有 1 个，代表着模型对样本的分类结果。 3.3.4 初始化模型参数 #\rnet[0].weight.data.normal_(0, 0.01)\rnet[0].bias.data.fill_(0) 在网络的第一层输入参数 3.3.5 定义损失函数 #\r计算均方误差使用的是MSELoss类，也称为平方�2范数。 默认情况下，它返回所有样本损失的平均值。 loss = nn.MSELoss() 3.3.6 定义优化算法 #\rtrainer = torch.optim.SGD(net.parameters(), lr=0.03) 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。 3.3.7 训练 #\r通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。\nnum_epochs = 3\rfor epoch in range(num_epochs):\rfor X, y in data_iter:\rl = loss(net(X) ,y)\rtrainer.zero_grad()\rl.backward()\rtrainer.step()\rl = loss(net(features), labels)\rprint(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) num_epochs = 3：定义了训练的轮数，这里设置为 3 for epoch in range(num_epochs):：使用 for 循环迭代每个训练轮数 for X, y in data_iter:：使用 data_iter 迭代器遍历训练数据集，其中 X 是特征，y 是对应的标签 l = loss(net(X) ,y)：计算模型对当前批次数据的预测值，并计算与真实标签之间的损失 trainer.zero_grad()：梯度清零，以避免梯度累积 l.backward()：反向传播，计算损失函数相对于模型参数的梯度 trainer.step()：更新模型参数，采用优化算法更新参数 l = loss(net(features), labels)：计算当前训练轮数结束后整个训练集上的损失 print(f'epoch {epoch + 1}, loss {l:f}')：打印当前训练轮数和对应的损失值 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/","section":"Docs","summary":"\u003cp\u003e本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型\u003c/p\u003e","title":"D2L 3.3 A concise implementation of linear regression","type":"docs"},{"content":"回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。\n事实上，我们也对_分类_问题感兴趣：不是问“多少”，而是问“哪一个”\n3.4.1 分类问题 #\r[[One-hot encoding 独热编码]] 3.4.2 网格架构 #\r为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的[[Affine Function 仿射函数]] E.X.\n假设现在有3个未规范化的预测(Logit)：\\(o_1,o_2和o_3\\) \\(o_1=x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1\\) \\(o_2=x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2\\) \\(o_1=x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3\\) 3.4.3 全连接层的参数开销 #\r对于任何具有d个输入和q个输出的全连接层[[3.1_LinearRegression#Fully-Connected Layer 全连接层]]，其参数开销为\\(O(dq)\\)，但可以通过超参数减少到\\(O(\\frac{dq}{n})\\) 3.4.4 softmax 运算 #\r我们希望模型的输出\\(\\hat y_j\\)可以视为属于类\\(j\\)的概率，然后选择具有最大输出值的类别\\(argmaxx_jy_j\\)作为我们的预测，例如\\(\\hat y_1,\\hat y_2\\)和\\(\\hat y_3\\)分别为\\(\\hat y={0.1,0.8,0.1}\\)那么我们的预测变为独热编码的\\(y={0,1,0}\\)，即为鸡 能否将未规范化的预测o直接视作我们感兴趣的输出呢 #\r不行 因为将线性层的输出直接视为概率时存在一些问题 我们没有限制这些输出数字的总和为1 根据输入的不同，它们可以为负值 其违反了[[概率论公理]] 概率论 #\r要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率 Calibration 校准 #\r例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别 Softmax 函数 #\r社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上发明的softmax函数 softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式$$\\hat y=softmax(o)，其中\\hat y_j=\\frac{exp(o_j)}{\\sum_kexp(o_k)}=\\frac{e^j}{\\sum_ke^k}$$ 这里，对于所有的j总有\\(0\\leq\\hat y_j\\leq1\\)，因此\\(\\hat y\\)可以视为一个正确的概率分布 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。 3.4.5 小批量样本的矢量化 #\r为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算 3.4.6 损失函数 #\r使用[[Maximum Likehood Estimation 极大似然估计]] 3.4.6.1 对数似然 #\rsoftmax函数给出了一个向量\\(\\hat y\\)， 我们可以将其视为“对给定任意输入x的每个类的条件概率\n通过计算softmax的对数似然，可以推导出他的损失函数\n假设现在有一个数据集 \\({X,Y}\\)，其具有n个样本，其中索引i的样本由特征向量\\(x^{(i)}\\)和独热标签向量\\(y^{(i)}\\)组成，可以将估计值与实际值进行比较$$P(Y|X)=\\prod^n_{i=1}P(y^{(i)}|x^{(i)})$$\n根据[[3.1_LinearRegression#Likehood 似然]]，已知最大化$P(Y|X)，相当于最小化负对数似然 $$P(Y|X)=\\sum^n_{i=1}-logP(y^{(i)}|x^{(i)})=\\sum^n_{i=1}l(y^{(i)},\\hat y^{(i)})$$\n其中对于任何标签y和预测模型\\(\\hat y\\)，损失函数为$$l(y,\\hat y)=-\\sum^{q}_{j=1}y_j\\log \\hat y_j$$\n这个[[3.1_LinearRegression#Loss Function 损失函数]]并没有介绍过，他的名字为Cross-entropy Loss交叉熵损失，将在后面介绍到\n为什么要加入对数，而不是直接取负数 #\r数值稳定性： 在概率模型中，可能会有大量的乘法运算，这可能导致数值下溢或溢出问题，尤其是当概率很小的时候。通过取对数，可以将乘法运算转换为加法运算，从而提高计算的稳定性。 对数函数的导数相对于原函数来说更简单，这使得梯度的计算更加高效。特别是在梯度下降等优化算法中，简化的导数计算可以显著减少计算量。 对数函数的特性使得推导和分析变得更加简单，因为它可以将乘法转换为加法，并且有很多性质，例如对数函数的导数比原函数更容易处理 3.4.6.2 softmax及其导数 #\r由于softmax和相关的损失函数很常见， 因此我们需要更好地理解它的计算方式 将3.4.3带入Cross-entropy Loss Function中，得到 $$\\begin{align}l(y,\\hat y)=-\\sum^{q}{j=1}y_j\\log \\frac{e^{o_j}}{{\\sum^{q}{k=1}e^{o_k}}} \\=-\\sum_{j=1}^{q}y_j[\\ln e^{o_j}-\\ln \\sum^q_{k=1}e^{o_k}] \\=\\sum^q_{j=1}y_j\\log\\sum^q_{k=1}e^{o_k}-\\sum^q_{j=1}y_jo_j \\=\\log \\sum^q_{k=1}e^{o_k}-\\sum^q_{j=1}y_jo_j\\end{align}$$ Softmax结合Cross Entropy的求导过程 #\r已知Cross Entropy Function$$H(y_i,p_i)=-\\sum_iy_i\\log pi$$\n\\(y_i\\)为预测事件，\\(\\log p_i\\)为一个分布的最优编码\n得到[[Home Page]] 3.4.6.3 交叉熵损失 #\r[[Cross-Entropy 交叉熵]] 3.4.7.1 熵 #\r[[Cross-Entropy 交叉熵#2. 熵]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/","section":"Docs","summary":"\u003cp\u003e回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。\u003c/p\u003e","title":"D2L 3.4 Softmax Regression","type":"docs"},{"content":"MNIST数据集 (LeCun et al., 1998) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (Xiao et al., 2017)。\n在此引入这个数据集是因为之后对于算法的评估均给予这一数据集\n%matplotlib inline\rimport sys\rfrom mxnet import gluon\rfrom d2l import mxnet as d2l\rd2l.use_svg_display() 3.5.1 读取数据集 #\r## 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\r## 并除以255使得所有像素的数值均在0～1之间\rtrans = transforms.ToTensor()\rmnist_train = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True)\rmnist_test = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True) Fshion-MNIST中包含的10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。 def get_fashion_mnist_labels(labels): #@save\r\u0026#34;\u0026#34;\u0026#34;返回Fashion-MNIST数据集的文本标签\u0026#34;\u0026#34;\u0026#34;\rtext_labels = [\u0026#39;t-shirt\u0026#39;, \u0026#39;trouser\u0026#39;, \u0026#39;pullover\u0026#39;, \u0026#39;dress\u0026#39;, \u0026#39;coat\u0026#39;,\r\u0026#39;sandal\u0026#39;, \u0026#39;shirt\u0026#39;, \u0026#39;sneaker\u0026#39;, \u0026#39;bag\u0026#39;, \u0026#39;ankle boot\u0026#39;]\rreturn [text_labels[int(i)] for i in labels] Plt 可视化样本 #\rdef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save\r\u0026#34;\u0026#34;\u0026#34;绘制图像列表\u0026#34;\u0026#34;\u0026#34;\rfigsize = (num_cols * scale, num_rows * scale)\r_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\raxes = axes.flatten()\rfor i, (ax, img) in enumerate(zip(axes, imgs)):\rif torch.is_tensor(img):\r# 图片张量\rax.imshow(img.numpy())\relse:\r# PIL图片\rax.imshow(img)\rax.axes.get_xaxis().set_visible(False)\rax.axes.get_yaxis().set_visible(False)\rif titles:\rax.set_title(titles[i])\rreturn axes\rX, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\rshow_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y)); ![[Pasted image 20240331164614.png]]\n3.5.2 读取小批量 #\r为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。 batch_size = 256\rdef get_dataloader_workers(): #@save\r\u0026#34;\u0026#34;\u0026#34;使用4个进程来读取数据\u0026#34;\u0026#34;\u0026#34;\rreturn 4\rtrain_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\rnum_workers=get_dataloader_workers()) 3.5.3. 整合所有组件 #\r现在我们定义load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状 def load_data_fashion_mnist(batch_size, resize=None): #@save\r\u0026#34;\u0026#34;\u0026#34;下载Fashion-MNIST数据集，然后将其加载到内存中\u0026#34;\u0026#34;\u0026#34;\rtrans = [transforms.ToTensor()]\rif resize:\rtrans.insert(0, transforms.Resize(resize))\rtrans = transforms.Compose(trans)\rmnist_train = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True)\rmnist_test = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True)\rreturn (data.DataLoader(mnist_train, batch_size, shuffle=True,\rnum_workers=get_dataloader_workers()),\rdata.DataLoader(mnist_test, batch_size, shuffle=False,\rnum_workers=get_dataloader_workers())) 下面，我们通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能。 train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\rfor X, y in train_iter:\rprint(X.shape, X.dtype, y.shape, y.dtype)\rbreak 我们现在已经准备好使用Fashion-MNIST数据集，便于下面的章节调用来评估各种分类算法 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/","section":"Docs","summary":"\u003cp\u003eMNIST数据集 (\u003ca href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90\"title=\"LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., \u0026amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\" target=\"_blank\"\u003eLeCun \u003cem\u003eet al.\u003c/em\u003e, 1998\u003c/a\u003e) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (\u003ca href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189\"title=\"Xiao, H., Rasul, K., \u0026amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.\" target=\"_blank\"\u003eXiao \u003cem\u003eet al.\u003c/em\u003e, 2017\u003c/a\u003e)。\u003c/p\u003e","title":"D2L 3.5 Image classification datasets","type":"docs"},{"content":"\r3.6.1 初始化模型参数 #\r和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量 在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10 初始化权重w #\r与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0 num_inputs = 784\rnum_outputs = 10\rW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\rb = torch.zeros(num_outputs, requires_grad=True) 3.6.2 定义softmax操作 #\r实现softmax操作由三个步骤组成 对每个项求幂 对每一行求和，得到其规范化常数 每一行除以其规范化常数，保持结果的和为1 $$softmax(X){ij}=\\frac{exp(X{ij})}{\\sum_kexp(X_{ik})}$$ 分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自统计物理学中一个模拟粒子群分布的方程 def softmax(X):\rX_exp = torch.exp(X)\rpartition = X_exp.sum(1, keepdim=True)\rreturn X_exp / partition # 这里应用了广播机制 keepdim=True: 在进行张量操作时，保持原始张量的维度\ntorch.normal(0, 1, (2, 5)) 是用 PyTorch 生成一个服从均值为 0，标准差为 1 的正态分布的张量。\n其中的 (2, 5) 是指生成的张量的形状为 2 行 5 列的矩阵\n3.6.3 定义模型 #\r定义softmax操作后，我们可以实现softmax回归模型 def net(X):\rreturn softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) 3.6.4 定义损失函数 #\r引入[[Cross-Entropy 交叉熵]]损失函数 深度学习中，交叉熵函数最为常见，因为分类问题的数量远远超过了回归问题 y = torch.tensor([0, 2])\ry_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ry_hat[[0, 1], y] y_hat： 包含2个样本在3个类别的预测概率 y：真实类，0代表第一类，1代表第二类，2代表第三类 [[0,1],y]：一种tensor的高级引索功能，其选择了y_hat中的第一行和第二行 而y给予了列的位置，所以输出分别为第一行第0位和第二行第2位 3.6.5 分类精度 #\r给定预测概率分布\\(\\hat y\\)，当我们必须输出Hard Prediciton 硬预测时，我们通常选择概率最高的类 当预测和标签分类y一致时，即是正确的 分类精度指的就是正确预测数量与总预测数量之比 def accuracy(y_hat, y): #@save\r\u0026#34;\u0026#34;\u0026#34;计算预测正确的数量\u0026#34;\u0026#34;\u0026#34;\rif len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1:\ry_hat = y_hat.argmax(axis=1)\rcmp = y_hat.type(y.dtype) == y\rreturn float(cmp.type(y.dtype).sum()) 扩展到任意数据迭代器data_iter可访问的数据集 #\rdef evaluate_accuracy(net, data_iter): #@save\r\u0026#34;\u0026#34;\u0026#34;计算在指定数据集上模型的精度\u0026#34;\u0026#34;\u0026#34;\rif isinstance(net, torch.nn.Module):\rnet.eval() # 将模型设置为评估模式\rmetric = Accumulator(2) # 正确预测数、预测总数, Accmulator在下面定义\rwith torch.no_grad():\rfor X, y in data_iter:\rmetric.add(accuracy(net(X), y), y.numel())\rreturn metric[0] / metric[1] 首先，如果 net 是 torch.nn.Module 的子类，就将模型设置为评估模式，即调用 net.eval()。在评估模式下，模型的行为可能会略有不同，比如 Dropout 层在评估模式下会关闭，以避免随机丢弃部分节点 创建了一个名为 metric 的累加器（Accumulator）。这个累加器用于记录正确预测数和总预测数，初始化为两个元素的列表 [0, 0] Accumulator：这个类在下面定义 使用 torch.no_grad() 上下文管理器，禁用梯度计算 最后就是将评估结果添加至metric中 Accumulator类 #\r这里定义一个实用程序类Accumulator，用于对多个变量进行累加。 在上面的evaluate_accuracy函数中， 我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。 class Accumulator: #@save\r\u0026#34;\u0026#34;\u0026#34;在n个变量上累加\u0026#34;\u0026#34;\u0026#34;\rdef __init__(self, n):\rself.data = [0.0] * n\rdef add(self, *args):\rself.data = [a + float(b) for a, b in zip(self.data, args)]\rdef reset(self):\rself.data = [0.0] * len(self.data)\rdef __getitem__(self, idx):\rreturn self.data[idx] __init__(self, n): 这是类的构造函数，用于初始化累加器。它接受一个参数 n，表示要累加的变量的数量。在初始化时，创建了一个包含 n 个元素的列表，每个元素初始化为 0.0 add(self, *args): 这个方法用于将参数 args 中的值与累加器中的值相加。参数 args 是一个可变参数，可以接受任意数量的参数。通过 zip 函数，将 args 中的值逐个与累加器中对应位置的值相加，并更新累加器中的值 reset(self): 这个方法用于重置累加器的值 __getitem__(self, idx): 这个方法允许通过索引访问累加器中的值。给定一个索引 idx，它返回累加器中对应位置的值 3.6.6 训练 #\r首先，我们定义一个函数来训练一个迭代周期 updater是更新模型参数的常用函数，它接受批量大小作为参数 def train_epoch_ch3(net, train_iter, loss, updater): #@save\r\u0026#34;\u0026#34;\u0026#34;训练模型一个迭代周期（定义见第3章）\u0026#34;\u0026#34;\u0026#34;\r# 将模型设置为训练模式\rif isinstance(net, torch.nn.Module):\rnet.train()\r# 训练损失总和、训练准确度总和、样本数\rmetric = Accumulator(3)\rfor X, y in train_iter:\r# 计算梯度并更新参数\ry_hat = net(X)\rl = loss(y_hat, y)\rif isinstance(updater, torch.optim.Optimizer):\r# 使用PyTorch内置的优化器和损失函数\rupdater.zero_grad()\rl.mean().backward()\rupdater.step()\relse:\r# 使用定制的优化器和损失函数\rl.sum().backward()\rupdater(X.shape[0])\rmetric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\r# 返回训练损失和训练精度\rreturn metric[0] / metric[2], metric[1] / metric[2] if isinstance(net, torch.nn.Module):：检查变量 net 是否是 torch.nn.Module 类的实例 net.train(): 这一行将模型（net）设置为训练模式 metric = Accumulator(3): 创建一个长度为3的累加器 在计算梯度后，根据数据类型，如pytorch类或者自定义类累加处理结果 训练函数 #\rdef train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save\r\u0026#34;\u0026#34;\u0026#34;训练模型（定义见第3章）\u0026#34;\u0026#34;\u0026#34;\ranimator = Animator(xlabel=\u0026#39;epoch\u0026#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9],\rlegend=[\u0026#39;train loss\u0026#39;, \u0026#39;train acc\u0026#39;, \u0026#39;test acc\u0026#39;])\rfor epoch in range(num_epochs):\rtrain_metrics = train_epoch_ch3(net, train_iter, loss, updater)\rtest_acc = evaluate_accuracy(net, test_iter)\ranimator.add(epoch + 1, train_metrics + (test_acc,))\rtrain_loss, train_acc = train_metrics\rassert train_loss \u0026lt; 0.5, train_loss\rassert train_acc \u0026lt;= 1 and train_acc \u0026gt; 0.7, train_acc\rassert test_acc \u0026lt;= 1 and test_acc \u0026gt; 0.7, test_acc ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/","section":"Docs","summary":"\u003ch2 class=\"relative group\"\u003e3.6.1 初始化模型参数 \r\n    \u003cdiv id=\"361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量\u003c/li\u003e\n\u003cli\u003e在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003e初始化权重w \r\n    \u003cdiv id=\"%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enum_inputs = 784\r\nnum_outputs = 10\r\n\r\nW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\r\nb = torch.zeros(num_outputs, requires_grad=True)\n\u003c/code\u003e\u003c/pre\u003e\r\n\r\n\u003ch2 class=\"relative group\"\u003e3.6.2 定义softmax操作 \r\n    \u003cdiv id=\"362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e实现softmax操作由三个步骤组成\u003c/li\u003e\n\u003cli\u003e对每个项求幂\u003c/li\u003e\n\u003cli\u003e对每一行求和，得到其规范化常数\u003c/li\u003e\n\u003cli\u003e每一行除以其规范化常数，保持结果的和为1\n$$softmax(X)\u003cem\u003e{ij}=\\frac{exp(X\u003c/em\u003e{ij})}{\\sum_kexp(X_{ik})}$$\u003c/li\u003e\n\u003cli\u003e分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自\u003ca href=\"https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29\" target=\"_blank\"\u003e统计物理学\u003c/a\u003e中一个模拟粒子群分布的方程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edef softmax(X):\r\n    X_exp = torch.exp(X)\r\n    partition = X_exp.sum(1, keepdim=True)\r\n    return X_exp / partition  # 这里应用了广播机制\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ekeepdim=True: 在进行张量操作时，保持原始张量的维度\u003c/p\u003e","title":"D2L 3.6 Implementation of softmax regression from scratch","type":"docs"},{"content":"在了解多层感知机前，需要先了解[[Perceptron 感知机]]\n4.1.1 隐藏层 #\r在[[3.1_LinearRegression#3.1.1.1 线性模型]]中我们描述了[[Affine Transformation 仿射变换]]，如一次函数一般是一种带有偏置项的线性变换 如果预测值在仿射变换后确实与输入数据有线性关系，那么这种方式确实够用 可是大部分情况下，仿射变换中的线性是一个很强的假设 4.1.1.1 线性模型可能会出错 #\r线性意味着单调假设，权重w在正的情况下，任何特征的增大都会导致模型输出的增大 E.X.\n如果我们试图预测一个人是否会偿还贷款，我们可以认为收入较高的申请人比收入较低的申请人更有可能偿还贷款 但上述例子只阐明了单调性而非线性 收入从0到5万会带来比100万到105万更大的还款可能性 在上例中，我们任然可以通过[[2.2 数据预处理]]的方式使线性更加合理，如对数化处理 但一个违反单调性的例子比如体温和死亡率的关系 对于体温高于37度的人来说，温度越高风险越高 而对于体温低于37度的人来说，温度越低风险就越低 这种情况也可以使用理37度的距离作为特征 分类问题，如对于猫狗分类问题，在位置（13，17）处像素强度进行添加，是否整个图像描绘狗的[[Likehood 似然]]会增加？ 这一评估标准注定会失败，如倒置图像后，类别依然保留 对于上面两个例子来说，猫狗的分类问题无法通过简单的预处理解决 对于[[深度神经网络]]，我们将使用隐藏层 4.1.1.2 在网络中加入隐藏层 #\r我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型 有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡\n4.1.1.3 从线性到非线性 #\r同之前的章节一样，我们通过矩阵\\(X\\in R^{n\\times d}\\)来表示n个样本的小批量，其中每个样本具有d个输入特征\n对于具有h个隐藏单元的单隐藏层多层感知机，用\\(H\\in R^{n\\times h}\\)表示隐藏层的输出，称为Hidden Representatiosn 隐藏表示\n因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重\\(W^{(1)}\\in R^{R\\times h}\\)和隐藏层偏置\\(b^{(1)}\\in R^{1\\times h}\\)以及输出层\\(W^{(2)}\\in R^{h\\times q}\\)和输出层偏置\\(b^{(2)}\\in R^{1\\times q}\\)\n所以形式上，对于单隐藏层的多层感知机的输出\\(O\\in R^{n\\times q}\\)，有 $$\\begin{align} \\ H=WX^{(1)}+b^{(1)} \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n现阶段，隐藏层为输入层的放射函数，而输出层为隐藏层的放射函数，即$$O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW+b$$\n注意到现在在多层感知机的单隐藏层下，模型依然只做到了线性的放射函数\n所以为了发挥多层架构的潜力，我们需要添加一个额外的关键要素：[[Activation Function 激活函数]]，激活函数的输出则称为Activations 活性值\n一般来说，有了激活函数，模型就不会退化成线性模型 $$\\begin{align} \\ H=\\sigma(XW^{(1)}+b^{(1)}) \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，从而产生更有表达能力的模型\n4.1.1.4 通用近似定理 #\r多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的 而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论 4.1.2 激活函数 Activation Function #\r[[Activation Function 激活函数]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/","section":"Docs","summary":"\u003cp\u003e在了解多层感知机前，需要先了解[[Perceptron 感知机]]\u003c/p\u003e","title":"D2L 4.1 MultilayerPerceptron","type":"docs"},{"content":"在了解多层感知机前，需要先了解[[Perceptron 感知机]]\n4.1.1 隐藏层 #\r在[[3.1_LinearRegression#3.1.1.1 线性模型]]中我们描述了[[Affine Transformation 仿射变换]]，如一次函数一般是一种带有偏置项的线性变换 如果预测值在仿射变换后确实与输入数据有线性关系，那么这种方式确实够用 可是大部分情况下，仿射变换中的线性是一个很强的假设 4.1.1.1 线性模型可能会出错 #\r线性意味着单调假设，权重w在正的情况下，任何特征的增大都会导致模型输出的增大 E.X.\n如果我们试图预测一个人是否会偿还贷款，我们可以认为收入较高的申请人比收入较低的申请人更有可能偿还贷款 但上述例子只阐明了单调性而非线性 收入从0到5万会带来比100万到105万更大的还款可能性 在上例中，我们任然可以通过[[2.2 数据预处理]]的方式使线性更加合理，如对数化处理 但一个违反单调性的例子比如体温和死亡率的关系 对于体温高于37度的人来说，温度越高风险越高 而对于体温低于37度的人来说，温度越低风险就越低 这种情况也可以使用理37度的距离作为特征 分类问题，如对于猫狗分类问题，在位置（13，17）处像素强度进行添加，是否整个图像描绘狗的[[Likehood 似然]]会增加？ 这一评估标准注定会失败，如倒置图像后，类别依然保留 对于上面两个例子来说，猫狗的分类问题无法通过简单的预处理解决 对于[[深度神经网络]]，我们将使用隐藏层 4.1.1.2 在网络中加入隐藏层 #\r我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型 有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡\n4.1.1.3 从线性到非线性 #\r同之前的章节一样，我们通过矩阵\\(X\\in R^{n\\times d}\\)来表示n个样本的小批量，其中每个样本具有d个输入特征\n对于具有h个隐藏单元的单隐藏层多层感知机，用\\(H\\in R^{n\\times h}\\)表示隐藏层的输出，称为Hidden Representatiosn 隐藏表示\n因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重\\(W^{(1)}\\in R^{R\\times h}\\)和隐藏层偏置\\(b^{(1)}\\in R^{1\\times h}\\)以及输出层\\(W^{(2)}\\in R^{h\\times q}\\)和输出层偏置\\(b^{(2)}\\in R^{1\\times q}\\)\n所以形式上，对于单隐藏层的多层感知机的输出\\(O\\in R^{n\\times q}\\)，有 $$\\begin{align} \\ H=WX^{(1)}+b^{(1)} \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n现阶段，隐藏层为输入层的放射函数，而输出层为隐藏层的放射函数，即$$O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW+b$$\n注意到现在在多层感知机的单隐藏层下，模型依然只做到了线性的放射函数\n所以为了发挥多层架构的潜力，我们需要添加一个额外的关键要素：[[Activation Function 激活函数]]，激活函数的输出则称为Activations 活性值\n一般来说，有了激活函数，模型就不会退化成线性模型 $$\\begin{align} \\ H=\\sigma(XW^{(1)}+b^{(1)}) \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，从而产生更有表达能力的模型\n4.1.1.4 通用近似定理 #\r多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的 而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论 4.1.2 激活函数 Activation Function #\r[[Activation Function 激活函数]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/","section":"Docs","summary":"\u003cp\u003e在了解多层感知机前，需要先了解[[Perceptron 感知机]]\u003c/p\u003e","title":"D2L 4.1 MultilayerPerceptron","type":"docs"},{"content":"通过更高级的API进一步简洁训练过程\n4.3.1 模型 #\rnet = nn.Sequential(nn.Flatten(),\rnn.Linear(784, 256),\rnn.ReLU(),\rnn.Linear(256, 10))\rdef init_weights(m):\rif type(m) == nn.Linear:\rnn.init.normal_(m.weight, std=0.01)\rnet.apply(init_weights); 初始化神经网络 batch_size, lr, num_epochs = 256, 0.1, 10\rloss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;)\rtrainer = torch.optim.SGD(net.parameters(), lr=lr)\rtrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\rd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/","section":"Docs","summary":"\u003cp\u003e通过更高级的API进一步简洁训练过程\u003c/p\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e4.3.1 模型 \r\n    \u003cdiv id=\"431-%E6%A8%A1%E5%9E%8B\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#431-%E6%A8%A1%E5%9E%8B\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enet = nn.Sequential(nn.Flatten(),\r\n                    nn.Linear(784, 256),\r\n                    nn.ReLU(),\r\n                    nn.Linear(256, 10))\r\n\r\ndef init_weights(m):\r\n    if type(m) == nn.Linear:\r\n        nn.init.normal_(m.weight, std=0.01)\r\n\r\nnet.apply(init_weights);\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e初始化神经网络\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ebatch_size, lr, num_epochs = 256, 0.1, 10\r\nloss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;)\r\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\r\n\r\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\r\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n\u003c/code\u003e\u003c/pre\u003e","title":"D2L 4.3 Simple Implementation of Multilayer Perceptron","type":"docs"},{"content":"深度学习的目的是发现Pattern，即做到模型的Generalization 泛化\n[[Overfitting Problem]] 原因很简单：当我们将来部署该模型时，模型需要判断从未见过的患者。 只有当模型真正发现了一种泛化模式时，才会作出有效的预测\n困难在于，当我们训练模型时，我们只能访问数据中的小部分样本。 最大的公开图像数据集包含大约一百万张图像。 而在大部分时候，我们只能从数千或数万个数据样本中学习。 在大型医院系统中，我们可能会访问数十万份医疗记录。 当我们使用有限的样本时，可能会遇到这样的问题： 当收集到更多的数据时，会发现之前找到的明显关系并不成立。\nOverfitting 过拟合 #\r模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合 左： Underfitting 欠拟合 中：拟合 右：Overfitting 过拟合 Regularization 正则化 #\r对抗过拟合的技术称为正则化 [[Regularization 正则化]] 4.4.1 训练误差和泛化误差 #\rTraining Error 训练误差 #\r模型在训练数据集上计算得到的误差 Generalization Error 泛化误差 #\r同样分布样本的无限多个数据的模型误差期望 但问题是对于无限多个数据，我们不可能准确的计算出Genrelization Errorz 4.4.1.1 统计学习理论 #\r我们假设训练数据和测试数据都是从相同的分布中独立提取的。 这通常被称为_独立同分布假设_（i.i.d. assumption） 4.4.1.2 模型复杂性 #\r一个模型的复杂性取决于很多因素 如模型参数，取值范围 Early Stopping 早停 #\r早停（Early Stopping）：这是一种防止过拟合的技术，其中训练过程在验证集上的性能开始恶化时停止。这意味着，如果模型在验证集上的误差开始增加，表明模型可能开始过拟合训练数据，此时停止进一步训练可以避免这种情况。 4.4.2 模型选择 #\r在一个训练中，我们会选择几个候选模型对他们进行评估 4.4.2.1 验证集 #\r训练集，验证集，测试集分别是什么_训练集 验证集 测试集-CSDN博客\n总的来说，对于Superivised Training，一般讲整体划为3个区块 Training Set 训练集 #\r训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数 训练集中的参数直接参与到梯度下降中 Validation Set 验证集 #\rz\n而验证集用于模型的选择，更具体地来说，验证集并不参与学习参数的确定，也就是验证集并没有参与梯度下降的过程 验证集只是为了选择超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数 Test Set 测试集 #\r测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型的评价 4.4.2.2 K-Fold Cross-Validation K折交叉验证 #\r训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集 过程描述 #\r数据分组：首先，整个数据集被随机分成K个大小大致相同的子集。 迭代训练与验证：每次迭代中，选择其中一个子集作为验证集，而其余的K-1个子集合并作为训练集。 性能评估：模型在训练集上训练，并在验证集上进行评估。这个过程重复K次，每次选择不同的子集作为验证集。 平均性能：最终模型的性能是所有K次迭代中验证性能的平均值。这样可以更全面地评估模型的性能。 4.4.3 欠拟合还是过拟合？ #\rGenerlization Error高的模型叫做Underfitting Train Error远低于Validation Error的模型叫做Overfitting 4.4.3.1 模型复杂性 #\r![[Pasted image 20240615153938.png]]\n简单来说，从左到右模型经历了从欠拟合到过拟合的一个过程，也是从高损失到高方差的过程 其是因为模型从没学习过参数到对于微小参数（甚至是随机噪声）严重敏感的一个过程 Lost 损失 #\r定义：偏差是指模型在预测中的系统误差，即模型对学习数据的一般性质的理解程度。 高偏差：通常表示模型过于简单（欠拟合），未能捕捉到数据的关键结构，通常会导致在训练集和测试集上都表现不佳。 Variance 方差 #\r定义：方差是指模型对于训练数据的微小变化的敏感度。 高方差：表示模型过于复杂（过拟合），对训练数据中的随机噪声也进行了学习，这可能使得模型在新的、未见过的数据上表现不佳。 4.4.3.2 数据集大小 #\r训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合 而样本更过通常会减小Gerneralization Error 一般来说，更多的数据不会有什么坏处 4.4.4 多项式回归 #\r拟合一个多项式\n[[4.4 Overfitting Normal \u0026amp; Underfitting - Pytorch]]\n4.4.4.1 生成数据集 #\r![[Pasted image 20240616094316.png]]\n噪声值位均值0到标准差0.1的正态分布 在优化的过程中，我们通常希望避免非常大的梯度值或损失值。 这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因 max_degree = 20 # 多项式的最大阶数\rn_train, n_test = 100, 100 # 训练和测试数据集大小\rtrue_w = np.zeros(max_degree) # 分配大量的空间\rtrue_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\rfeatures = np.random.normal(size=(n_train + n_test, 1))\rnp.random.shuffle(features)\rpoly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\rfor i in range(max_degree):\rpoly_features[:, i] /= math.gamma(i + 1) # gamma(n)=(n-1)!\r# labels的维度:(n_train+n_test,)\rlabels = np.dot(poly_features, true_w)\rlabels += np.random.normal(scale=0.1, size=labels.shape) max_degree = 20 : 即使多项式仅为三阶，但我们需要用一个20纬的多项式去拟合它，这是复杂模型中的一种 features = np.random.normal(size=(n_train + n_test, 1)): 分配200个一维的特征 np.random.shuffle(features): 随机打乱数据 poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))：分配每个特征的高阶数据 使用伽马正则化防止特征的迅速增大 最后点乘特征和真实权重得到Label，并将Label加上合适的噪声 # NumPy ndarray转换为tensor\rtrue_w, features, poly_features, labels = [torch.tensor(x, dtype=\rtorch.float32) for x in [true_w, features, poly_features, labels]]\rfeatures[:2], poly_features[:2, :], labels[:2] 转化为tensor def evaluate_loss(net, data_iter, loss): #@save\r\u0026#34;\u0026#34;\u0026#34;评估给定数据集上模型的损失\u0026#34;\u0026#34;\u0026#34;\rmetric = d2l.Accumulator(2) # 损失的总和,样本数量\rfor X, y in data_iter:\rout = net(X)\ry = y.reshape(out.shape)\rl = loss(out, y)\rmetric.add(l.sum(), l.numel())\rreturn metric[0] / metric[1] def train(train_features, test_features, train_labels, test_labels,\rnum_epochs=400):\rloss = nn.MSELoss(reduction=\u0026#39;none\u0026#39;)\rinput_shape = train_features.shape[-1]\r# 不设置偏置，因为我们已经在多项式中实现了它\rnet = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\rbatch_size = min(10, train_labels.shape[0])\rtrain_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\rbatch_size)\rtest_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\rbatch_size, is_train=False)\rtrainer = torch.optim.SGD(net.parameters(), lr=0.01)\ranimator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, yscale=\u0026#39;log\u0026#39;,\rxlim=[1, num_epochs], ylim=[1e-3, 1e2],\rlegend=[\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;])\rfor epoch in range(num_epochs):\rd2l.train_epoch_ch3(net, train_iter, loss, trainer)\rif epoch == 0 or (epoch + 1) % 20 == 0:\ranimator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\revaluate_loss(net, test_iter, loss)))\rprint(\u0026#39;weight:\u0026#39;, net[0].weight.data.numpy() 欠拟合 #\r# 从多项式特征中选择前2个维度，即1和x\rtrain(poly_features[:n_train, :2], poly_features[n_train:, :2],\rlabels[:n_train], labels[n_train:]) 只给予了前两个特征值 ![[Pasted image 20240704160340.png]] 过拟合 #\r# 从多项式特征中选取所有维度\rtrain(poly_features[:n_train, :], poly_features[n_train:, :],\rlabels[:n_train], labels[n_train:], num_epochs=1500) 将w中的20列全部给到了模型导致了过拟合 ![[Pasted image 20240704160346.png]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/","section":"Docs","summary":"\u003cp\u003e深度学习的目的是发现Pattern，即做到模型的Generalization 泛化\u003c/p\u003e","title":"D2L 4.4 Model Selection, Underfitting, and Overfitting","type":"docs"},{"content":" ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/","section":"Docs","summary":"\u003chr\u003e","title":"Chapter 3. Linear Neural Network","type":"docs"},{"content":" ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/","section":"Docs","summary":"\u003chr\u003e","title":"Chapter 4. Multilayer Perceptron","type":"docs"},{"content":"《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. https://zh-v2.d2l.ai/index.html\n‌ #\r","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/","section":"Docs","summary":"\u003cp\u003e《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. \u003ca href=\"https://zh-v2.d2l.ai/index.html\" target=\"_blank\"\u003ehttps://zh-v2.d2l.ai/index.html\u003c/a\u003e\u003c/p\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e‌ \r\n    \u003cdiv id=\"heading\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#heading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e","title":"Dive Into Deep Learning","type":"docs"},{"content":"","date":"Dec 27 2021","externalUrl":null,"permalink":"/tags/wangyiyun/","section":"Tags","summary":"","title":"Wangyiyun","type":"tags"},{"content":"\r","date":"Dec 27 2021","externalUrl":null,"permalink":"/blogs/wangyiyun/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/wangyiyun/1_hu17702828048897762413.jpg 330w,\r\n        /blogs/wangyiyun/1_hu360113685819379410.jpg 660w,\r\n        /blogs/wangyiyun/1_hu13608319589208325737.jpg 1024w,\r\n        /blogs/wangyiyun/1_hu6717855714386122734.jpg 2x\"\r\n        src=\"/blogs/wangyiyun/1_hu360113685819379410.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"网易云年度总结","type":"blogs"},{"content":"","date":"Dec 26 2021","externalUrl":null,"permalink":"/tags/shanghai/","section":"Tags","summary":"","title":"Shanghai","type":"tags"},{"content":"\r","date":"Dec 26 2021","externalUrl":null,"permalink":"/blogs/theband/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/theband/1_hu12727092779758697027.jpg 330w,\r\n        /blogs/theband/1_hu6728190700059376434.jpg 660w,\r\n        /blogs/theband/1_hu5333335250471856356.jpg 1024w,\r\n        /blogs/theband/1_hu8413573821294788432.jpg 2x\"\r\n        src=\"/blogs/theband/1_hu6728190700059376434.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"外滩2021","type":"blogs"},{"content":"\r","date":"Nov 1 2021","externalUrl":null,"permalink":"/blogs/desk2021/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FoxCsgo/FoxCsgo-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"2021 桌搭","type":"blogs"},{"content":"","date":"Nov 1 2021","externalUrl":null,"permalink":"/tags/csgp/","section":"Tags","summary":"","title":"Csgp","type":"tags"},{"content":"\r","date":"Oct 12 2021","externalUrl":null,"permalink":"/blogs/5e600/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/5e600/1_hu9666354151374759163.jpg 330w,\r\n        /blogs/5e600/1_hu16441751272499849929.jpg 660w,\r\n        /blogs/5e600/1_hu13471086248097805267.jpg 1024w,\r\n        /blogs/5e600/1_hu2577903920060232791.jpg 2x\"\r\n        src=\"/blogs/5e600/1_hu16441751272499849929.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"5e 600分对决","type":"blogs"},{"content":"","date":"Oct 12 2021","externalUrl":null,"permalink":"/tags/desk/","section":"Tags","summary":"","title":"Desk","type":"tags"},{"content":"\r","date":"Sep 3 2021","externalUrl":null,"permalink":"/blogs/foxcsgo/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FoxCsgo/FoxCsgo-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"沙漠之狐","type":"blogs"},{"content":"","date":"Apr 28 2020","externalUrl":null,"permalink":"/tags/qiuqiu/","section":"Tags","summary":"","title":"Qiuqiu","type":"tags"},{"content":"","date":"Apr 28 2020","externalUrl":null,"permalink":"/tags/zhuzi/","section":"Tags","summary":"","title":"Zhuzi","type":"tags"},{"content":"\r","date":"Apr 28 2020","externalUrl":null,"permalink":"/blogs/qiuqiu/young/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Qiuqiu/Young/Young-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"小时候","type":"blogs"},{"content":"\r","date":"Apr 28 2020","externalUrl":null,"permalink":"/blogs/zhuzi/young/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-2.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-3.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-4.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-5.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"小时候","type":"blogs"},{"content":"","date":"Jan 25 2020","externalUrl":null,"permalink":"/blogs/qiuqiu/","section":"Blogs","summary":"","title":"球球","type":"blogs"},{"content":"\r","date":"Dec 21 2018","externalUrl":null,"permalink":"/blogs/fundationchristams/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FundationChristams/FundationChristams-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Fundation Christams","type":"blogs"},{"content":"","date":"Dec 21 2018","externalUrl":null,"permalink":"/blogs/zhuzi/","section":"Blogs","summary":"","title":"竹子","type":"blogs"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Special thanks to those for thier invaluable contributions\n","externalUrl":null,"permalink":"/credits/","section":"Credits","summary":"\u003cp\u003eSpecial thanks to those for thier invaluable contributions\u003c/p\u003e","title":"Credits","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/faker/","section":"Credits","summary":"","title":"Faker","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/ss/","section":"Credits","summary":"","title":"SS","type":"credits"},{"content":"123\nRecord some thinking\n","externalUrl":null,"permalink":"/thoughts/thethreebodyproblem/","section":"Thoughts","summary":"\u003cp\u003e123\u003c/p\u003e\n\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"The Three Body Problem","type":"thoughts"},{"content":"","externalUrl":null,"permalink":"/thoughts/","section":"Thoughts","summary":"","title":"Thoughts","type":"thoughts"},{"content":"","externalUrl":null,"permalink":"/credits/%E6%B3%95%E8%80%81/","section":"Credits","summary":"","title":"法老","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/%E9%82%93%E7%B4%AB%E6%A3%8B/","section":"Credits","summary":"","title":"邓紫棋","type":"credits"}]