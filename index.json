


[{"content":" Last Edit 9/24/24\nHooke’s Law #\r$$F=kx$$\nSpring constant (k): Spring Constant Material properties（材料性能） #\r主要指材料的基本机械性能，如弹性模量、屈服强度、抗拉强度等 它们的计算方式应当剔除几何尺寸的影响 Stress 应力 #\r物体在外力作用下，单位面积上承受的内力 $$Stress(\\sigma)=\\frac{F}{A_0}$$ \\(A_0\\): Initial Cross-Sectional Area 应力的单位通常是帕斯卡（Pa） Strain 应变 #\r应变是材料在外力作用下发生的变形程度 $$Strain(\\epsilon)=\\frac{\\Delta l}{l_0}$$\n\\(\\Delta l\\)：change of material\u0026rsquo;s length \\(l_0\\): Initial length of material Young\u0026rsquo;s Modulus 杨氏模量 #\r是固体在载荷下的刚度或对弹性变形的抵抗力的量度\n材料在压缩或拉伸时会发生Elastic Deformation，而在Unloaded之后则会回到之前的Equilibrium $$E=\\frac{\\sigma}{\\epsilon}=\\frac{\\frac{F}{A_0}}{\\frac{\\Delta l}{l_0}}=\\frac{F\\cdot l_0}{A_0\\cdot \\Delta l}$$\nYoung\u0026rsquo;s Modulus (E)\n单位是Pa\nStructure Independent #\r当说\u0026quot;XXX is xxx independent\u0026quot; 时，代表了某个事物不依赖于某个特定因素 这里则是Young\u0026rsquo;s Modulus是不依赖于Structure Young\u0026rsquo;s Modulus其只与材料有关，与形状无关 具体来说是取决与材料的原子级别的相互作用，而不是材料的宏观或微观结构 Micro Perspective of Stress and Strain #\r从微观角度来看，物体由Atoms组成，其中存在Inter Atomic Forces 当Applied External Force的时候，物体将处于Loaded状态，其Shape将会发生改变 具体来说，Shape发生的改变是由于Atoms之间的间距发生了改变 不过只要整个Stress小于Yield Strength，所有的Deformation都将是Elastic的 代表了，当External Force被撤去的时候，既Unloaded之后，Atoms将会回到他们原来的Equilibrium position 需要注意的是，Atom之间的间距将会回到一开始的\\(r=r_0\\) 所以可以得出一个结论：Elastic Strain is Reversible Stress-Strain Curve 应力-应变图 #\r本图实际上为F-r图，即拉力-原子间半径图，但与Stress-Strain图相似，便用SS图讲解\n对于一个材料，在对其施加Stress的时候，其Strain会出现如此的固定趋势 在Stress等于0的时候，物体处于Equilibrium状态，具体来说其Attractive Force = Repulsive Force 在持续施加Stress后，Atoms最终将到达一个Yield Strength（不可逆点）后将会产生Plastic Deformation（将在下一章提到） 将Stress-Strain图放大到\\(r_0\\)两边后观察 可以发现Stress随Strain(Atomic Spacing)基本呈现Linear Trend，便可以说 $$E\\propto \\frac{dF}{dr}|_r=r_0$$ Young\u0026rsquo;s Modulus is directly propotional to slope of interatomic force speration curve at equilibrium spacing 简单的理解便为：杨氏模量等于与Stress对于Strain的变化率 更具图可以看出Stress对Strain的变化速率越大，其Young\u0026rsquo;s Modulus越大 即在Plastic Deformation前，Stress（External Force）越大材料的Young\u0026rsquo;s Modulus越大 既抵抗Elastic Deformation的作用越大 The way to determine material properties #\rTensile(Tension) Test\n图中展示了拉伸测试的基本原理，即通过对材料施加拉力（Tension），使其伸长（elongating），从而获得应力-应变曲线等相关数据 Grip Region（夹持区）：这是样品被测试机夹持的地方，两端施加拉力 Reduced Section（缩小部分）：这是样品的中间区域，它的截面积被减少，以确保样品在这个区域发生断裂或变形。在该区域内，材料会受到均匀的拉伸应力。 通过拉伸测试，可以获得材料的关键参数，如杨氏模量 (Young\u0026rsquo;s Modulus)、屈服强度 (Yield Strength)、极限抗拉强度 (Ultimate Tensile Strength, UTS) 和断裂延伸率 (Fracture Elongation) Disadvantage of Tensile Test #\r这种测试方法仍然存在局限性，例如Ceramics \u0026amp; Glasses等都不能利用这种方式测试Material Properties，具体来说 Low strain to fracture, almost no deformation before breaking：在断裂之间，几乎不发生Elastic Deformation hard to grip：对于Tensile Test所必须的Grip Region，由于一些Material的特殊性，他们无法在Grip Region内被夹住，继而无法进一步测试 Hard to load on Axis: 当Grip Region滑动的时候，无法使力沿轴拉伸 关于为什么这些会发生，如为什么Ceramics会直接断裂等，参考[[ECMS 3. Plastic Deformations]] ","date":"Sep 24 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms2.elasticbehavior/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 9/24/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eHooke’s Law \r\n    \u003cdiv id=\"hookes-law\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#hookes-law\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e$$F=kx$$\u003c/p\u003e","title":"ECMS 2. Elastic Behavior","type":"docs"},{"content":"\rLast Edit: 9/23/24\nPlastice Deformation (Permanent Deformation) #\rwe use the term plastic to describe permanent deformation 之所以是Plastic，是因为它derives from the Greek plastikos meaning to sculpt Changes After Plastic Deformation #\r在Plastic Deformation后，Atomic Spacing将保持\\(r=r_0\\) 但是Sequence of atoms将进入一个New Equilibrium 即在Marcro Perspective上发生Shape的Deform Tensile Strain将会保持一定非零大小 Micro Perspective of Plastic Deformation #\rDifferent between Elastic and Plastic Deformation #\rElastic #\r对于Elastic Deformation，开始前物理Atom之间间距应为\\(r_0\\) 泄力后仍应该是\\(r_0\\)，并且Atom将会到他们原有的Equilibrium 并且物体从Marco Perspective上并不发生Deformation Plastic #\r结束后Atom之间间距仍应该是\\(r_0\\) 泄力后Atom将进入一个新的Equilibrium 物体在泄力后，他的Tensile Strain将不会便为0而是保持在一定数 即Shape已经发生了Perminant Change Beyond Elastic Region #\r在Elastic Region外，便是完整的[[ECMS 2. Elastic Behavior#Young\u0026rsquo;s Modulus 杨氏模量]]的模型 Yield Strength: 屈服强度是指材料在发生永久变形之前，能够承受的最大应力。 当Strain到达Yield Strength之后，材料会从Elastic Deformation转变为Plastic Deformation，即Material发生Permanent Deformation 在过了Yield Strength之后Strain再增加后到了一定程度之后便会产生Fracture(Broken into pieces) Stress-Strain Curve for different materials #\rMetals #\r图中的绿色曲线 其特点有在一定位置之后开始产生Permanent Deformation 之后在持续的施加Stress之后其Strain变化率降低最后产生Fracture 相比于Ceramic和Polymer，其Young\u0026rsquo;s Modulus处于中间位置，高于Polymer但小于没有Permanent Deformation阶段的Ceramic Polymer #\r相对来说没有什么特点 具有较低的Young\u0026rsquo;s Modulus和Permanent Deformation区间 Ceramic #\r对于陶瓷类的物质，其没有Permanent Deformation的区间 对于他来说也存在Elastic Region 但可以看出整体Young\u0026rsquo;s Modulus非常高，并且呈现线性 在施加了一定的Stress后会直接Load enough and fracture Three-Point-Blending Test #\r底部两个点用作支撑，上方一个力将物体往下压 $$Stress(\\sigma)=\\frac{3FL}{2wh^2}$$\nTempered Glass 钢化玻璃 #\r![[ECMS 3. Plastic Deformations-5.png]]\n再高温下迅速向表面喷冷凝液将其降温 冷却将使玻璃表面收缩的比内部更快，产生了向心的Compress Stress 而内部由于受力将产生反作用力，向外产生张应力 整体的结构处于一个向内部收缩的趋势，导致当其受到了外力的时候，尝试使其Fracture的导致分子之间结构被破坏的力将被抵消 并且由于其内部存在Residual Stress（残余应力）导致了整体结构破坏的时候其Residual Stress将破坏结构至非常小的结构 ","date":"Sep 23 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms3.plasticdeformation/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 9/23/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch4 class=\"relative group\"\u003ePlastice Deformation (Permanent Deformation) \r\n    \u003cdiv id=\"plastice-deformation-permanent-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#plastice-deformation-permanent-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h4\u003e\r\n\u003cul\u003e\n\u003cli\u003ewe use the term \u003cstrong\u003eplastic\u003c/strong\u003e to describe permanent deformation\u003c/li\u003e\n\u003cli\u003e之所以是Plastic，是因为它derives from the Greek \u003cstrong\u003eplastikos\u003c/strong\u003e meaning to sculpt\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch5 class=\"relative group\"\u003eChanges After Plastic Deformation \r\n    \u003cdiv id=\"changes-after-plastic-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#changes-after-plastic-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h5\u003e\r\n\u003cul\u003e\n\u003cli\u003e在Plastic Deformation后，Atomic Spacing将保持\\(r=r_0\\)\u003c/li\u003e\n\u003cli\u003e但是Sequence of atoms将进入一个New Equilibrium\u003c/li\u003e\n\u003cli\u003e即在Marcro Perspective上发生Shape的Deform\u003c/li\u003e\n\u003cli\u003eTensile Strain将会保持一定非零大小\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch5 class=\"relative group\"\u003eMicro Perspective of Plastic Deformation \r\n    \u003cdiv id=\"micro-perspective-of-plastic-deformation\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#micro-perspective-of-plastic-deformation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h5\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations-1.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS3.PlasticDeformations/ECMS3.PlasticDeformations-2.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 3. Plastic Deformation","type":"docs"},{"content":" Last Edit: 9/25/24\nYoung’s modulus change with Density #\rOrdered Solids #\r大多数固体材料都是Polycrystaline 多晶体的 尤其是Metal在原子尺度上是按照Crystal Structure排列的 Atomic scale尺度一般在\\(10^{-10}\\)的级别 Unit Cell #\r最小的Convenient Building Block Grains #\r多晶材料的这些晶体被称为晶粒（grains）。每个晶粒内部的原子排列是有序的，但不同晶粒之间的原子排列方向各不相同，这种微观结构对材料的机械性能和物理性质有重要影响 Short Range Order #\r指的是在局部区域内，原子或分子的排列是有规律的 玻璃、液体这样的材料，它们通常是Short Range Order的 Long Range Order #\r在整个材料中，原子或分子的排列是有规律的，并且这种规律性会一直延续到较大的尺度（远远超过单个原子的范围） 晶体材料，如金属和矿物，通常具有长程有序 Simple Cubic #\r简单的一个立方体上的八个角为Atoms的结构 其正方体的边长被定义为a n：1 Atom CN：6 Side Dimension: \\(a=2r\\) Hard Sphere Model #\r所有的Atoms都被当作一个球来模拟，主要是方便描述原子在晶体结构中的排列方式 Reduced Sphere Model #\rReal Graph #\r可以发现这样的Hard Shpere Model看着实在是Messy 所以就将所有Atoms简化为Reduced Sphere Model The Atomic Packing Factor 填充系数 #\r对于一个Unit Cell，APF描述了Atom对于体积的占有率，从100%完全占据到0%一点没有 $$APF=\\frac{Volume_{Spheres}}{Volumn_{Unit~Cell}}$$ 本质上就是一个百分比或者说分数 现在分别计算两个体积 对于Spheres来说，单个的体积为\\(\\frac{4}{3}\\pi R^3\\)，而将他乘上Number of Atoms后便可以得到所有的体积 对于Unit Cell来说，以a为边长，其体积自然为\\(a^3\\) Face Centred Cubic (FCC) Structure #\r结构特点：每个晶胞的八个顶点各有一个原子，此外每个面中心还有一个原子。 n：每个晶胞含有 4 个原子（8 个顶点原子各占 1/8，6 个面心原子各占 1/2）。 CN：12（每个原子有 12 个最近邻原子）。 APF：约 74%（原子占据的体积比例）。 例子：铝、铜、金、银等 在Simple Cubic的基础上加入一些Face Centred Atoms（在每个面中心的Atom） Number of Atoms in FCC #\r对于FCC来说，每一个角上都是\\(\\frac{1}{8}\\)个Atom 每一个面都有\\(\\frac{1}{2}\\)个Atom 所以一共就是\\(\\frac{1}{8}*8+\\frac{1}{2}*6=4\\)个Atom 即\\(n_{FCC}=4\\) Coordination Number of FCC #\r$$CN_{FCC}=12$$\nAtomic Packing Factor of FCC #\r则对于FCC Structure来说，其APF即为 $$APF=\\frac{4\\frac{4}{3}\\pi R^3}{a^3}$$ 现在要将Atom的radius与Unit Cell的边长a做替换好消掉其中一个 有\\(a^2+a^2=(4R)^2\\)，勾股定律 则有\\(2a^2=16R^2\\)，即\\(a_{FCC}=2\\sqrt{2}R\\) 将a带入后便有 $$APF = \\frac{4 \\left( \\frac{4}{3} \\pi R^3 \\right)}{(2\\sqrt{2}R)^3} \\ \\Rightarrow APF_{FCC} = 0.74$$ Avogadro\u0026rsquo;s constant 阿伏伽德罗常数 #\r通常用符号\\(N_A\\)表示，是指在1摩尔物质中包含的微观粒子（如原子、分子、离子等）的数量。其数值大约为： $$N_A=6.022×10^{23 }g\\cdot mol^{−1}$$\nTheoretical Density 理论密度 #\r$$\\text{Mass}{\\text{Atoms in Unit Cell}} = \\text{Number}{\\text{Atoms in Unit Cell}} \\cdot \\frac{\\text{Molar Mass of Atom}}{\\text{Avogadro\u0026rsquo;s Number}}$$\n$$m = n \\cdot \\frac{A}{N_A}$$ $$\\rho = \\frac{nA}{V_C N_A}$$\nDensity密度，即质量和体积的比值 \\(n\\)：Number of Atoms，一个Crystal里的完整分子个数 A: Molar Mass(g/mol)，原子相对质量 \\(V_c\\)：Unit Cell的体积 \\(N_A\\)：阿伏伽德罗常数\\(mol^{-1}\\) \\(\\frac{A}{N_A}\\)：\\(\\frac{g/mol}{mol^{-1}}=g\\)，等于一个Atom有几g，再乘上Atom Nuber得到全部的Mass Rock Salt Structure #\r是一个Common Ceramic的Crystal Structure 结构特点：这是离子晶体结构，通常由两种不同大小的离子（如Na⁺和Cl⁻）组成。大的离子（Cl⁻）形成一个面心立方（FCC）结构，小的离子（Na⁺）填充在八面体间隙中。 A：每个晶胞含有 4 个阳离子和阴离子。 CN：阳离子和阴离子的CN都为6。 APF：约 67%。 例子：氯化钠（NaCl）、氧化镁（MgO）等。 对于Ceramic Structure来说，其拥有多余一种的Atom类型，具体来说有Cation和Anion两种 其是Ionic Compound的一种特有的Crystal Structure 对于Rosk Salt Structure来说，其包含了两种Ion，即Anion（蓝色）与Cation（红色） Anions在Unit Cell的角上（Cation也可以在，他们描述的将是同一种结构，但一般来说体积大的Anion会先占据Unit Cell的角落，将小的Cation挤到中间去） 这个结构看起来像面心立方（FCC），但实际上不是纯粹的FCC，因为阳离子和阴离子之间有相互作用，会推开阴离子，使得阴离子不会像真正的FCC结构那样直接通过面对角线相互接触。 Number of Atoms in Rock Salt (Stoichiometry) #\r对于如NaCl这样的material，其Anion：Cation比都是1：1的，但上图中明显缺少了一个Cation 其正确的位置应该是整个Unit Cell的正中央 Coordination Number for Cations in Rock Salt #\r对于一个Rock Salt来说，拿最中间的Cation举例，可以发现与其接触的Atom有6个 于是就可以说\\(CationCoordinationNumber_{Rock~Salt} = 6\\) Density of Rock Salt #\r对于Rock Slat Structure来说，由于其Atom种类变为了Anion与Cation两个，其[[#Theoretical Density]]的分子也要对应的便为两个的和，即 $$\\rho=\\frac{n_CA_C+n_AA_A}{V_CN_A}$$ \\(n_CA_C\\)：Cation的Number和Moalr Mass，nA同理 对于Vc来说，其a变为了两个Atoms的Radus*2，有\\(V_C=(2R_A+2R_C)^3\\) Theoretical Density of Rock Salt #\r$$\\rho = \\frac{n_C A_C + n_A A_A}{V_C N_A}$$\nAnions #\r在Rock Salt Structure中的Anion看似处于FCC的位置中 但实际上由于附近的Cation和不同Anion之间的相互作用力导致Anion实际上不在精确的FCC位置上 Cations #\rthe cations will always touch their nearest neighbour anions The Body Centred Cubic Crystal (BCC) Structure #\r结构特点：每个晶胞的八个顶点各有一个原子，且晶胞中心还有一个原子。 n：每个晶胞含有 2 个原子（8 个顶点原子各占 1/8，中心原子占 1 个）。 CN：8（每个原子有 8 个最近邻原子）。 APF：约 68%。 例子：铁、钨、铬等。 Number of Atoms in BCC #\r$$\\frac{1}{2}*2+\\frac{1}{8}*8=2$$\nCoordination Number of BCC #\r$$CoordinationNumberBCC​=8$$\nAtomic Packing Factor for BCC #\r$$APF=\\frac{Volume_{Spheres}}{Volumn_{Unit~Cell}}$$\n对于BCC，要取其Unit Cell边长与Atom radius关系得用Cubic Diagonal，即 $$3a^2=16R^2，\\Rightarrow a_{BCC}=\\frac{4}{\\sqrt3}R$$ $$APF = \\frac{2 \\left( \\frac{4}{3} \\pi R^3 \\right)}{(\\frac{4}{\\sqrt3})^3} \\ \\Rightarrow APF_{BCC} = 0.68$$\nInterstitial Sites #\rSpace between other atoms 其体积就代表了Crystal Structure中的间隙的部分 By convention, we name interstitial sites according to the solid geometry that they create Octanhedron Interstitial Site #\r对于Rock Salt中的Anion的Interstitial Sites，将他们命名为Octanhedron Interstitial Site Coordination Number of a Interstitial Site #\r对于Interstitial Site来说，其CN代表了Interstitial Site中心点位置的Atom与最近Atom接触的个数 对于Rock Salt来说，Intersitital Site CN = 6 Simple Cubic Interstitial Site #\r结构特点：每个晶胞的八个顶点各有一个原子，顶点原子通过边连接，但面心和体心没有原子。 n：1Atom（8 个顶点原子各占 1/8）。 CN：6（每个原子有 6 个最近邻原子）。 APF：约 52%。 例子：钋（唯一的自然存在的例子）。 纯的简单立方晶格中，中心位置是空的（这个红点代表的就是Intersititial Site间隙位的大小 但在体心立方或其他某些间隙结构中，这一位置可以被其他原子或离子占据 Coordination Number of Simple Cubic #\r$$CoordinationNumberSimpleCubic​=8$$\nThe Size of Interstitial Sites #\r就Interstitial Site来说，其具有实际大小 前面Rock Salt中提到过Cations will always touch their nearest neighbour anions 阳离子总是会接触它们最近的阴离子，因此阳离子只有在足够大时才会占据晶体结构中的间隙位。如果阳离子太小，它将无法与最近的阴离子接触，因此不会稳定地占据间隙位 $$\\sin 45 = \\frac{2R_A}{2R_A + 2R_C} \\\n(2R_A + 2R_C)\\sin 45 = 2R_A \\\n2R_A\\sin 45 + 2R_C\\sin 45 = 2R_A \\\nR_A\\sin 45 + R_C\\sin 45 = R_A \\\n\\frac{R_A}{R_A}\\sin 45 + \\frac{R_C}{R_A}\\sin 45 = 1 \\\n\\sin 45 + \\frac{R_C}{R_A}\\sin 45 = 1 \\\n\\frac{R_C}{R_A}\\sin 45 = 1 - \\sin 45 \\\n\\frac{R_C}{R_A} = \\frac{1 - \\sin 45}{\\sin 45}=0.414$$\nHexagonal Close Packed (HCP) Structure #\r结构特点：原子以六边形排列，沿c轴有堆叠的结构。原子层是按照ABAB\u0026hellip;的顺序堆叠。 n：每个晶胞含有 6 个原子（从整体堆叠考虑）。 CN：12（类似于FCC，每个原子有12个最近邻原子）。 APF：约 74%。 ex.：镁、钛、锌等 Number of Atoms in HCP #\rCoordination Number of HCP #\rCN = 12 Atomic Packing Factor in HCP #\rHCP有着和FCC一样的APF（都为0.74）所以FCC有时会被称为CCP Different to FCC #\rFCC和HCP的主要区别在于原子的堆积顺序 FCC中的原子堆积顺序是ABCABC 而HCP中的堆积顺序是ABAB 尽管它们的堆积顺序不同，但由于原子排列非常紧密，它们的APF都是0.74 Close Packed #\r要想让Atom排列的更加紧密，需要有Close Packed的结构 当Atom在这种排列下，APF才能更高 在Not Close Packed下，这种排列的方式更加的松散 Closed Packed Plane #\rNot Closed Packed Plane in FCC #\r对于FCC的侧面上的Atom来说，其并不处于Closed Packed状态下 ","date":"Sep 25 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms4.thestructureproperty/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 9/25/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eYoung’s modulus change with Density \r\n    \u003cdiv id=\"youngs-modulus-change-with-density\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#youngs-modulus-change-with-density\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS4.TheStructureProperty/ECMS4.TheStructure-Property-24.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"ECMS 4. The Structure Property","type":"docs"},{"content":" Last Edit: 10/16/24\nStress-Strain Curve #\r当回头重新看Stress-Strain Curve的时候可以发现一些特殊的点 如Proportion Limit，Ultimate Tensile Strength两个点 Proportional Limit #\r在曲线的最初阶段，存在一个接近于Linear的区域，其代表了Linear Elastic的Region 而也必会存在一个点象征着Linear Elastic Region的结束 但犹豫一些测量的误差或者精度上的问题，将导致最终的这一个Linear Elastic结束的点无法被确定，所以需要一个约定俗成的方法 0.2% Offset Yield Strength #\r在\\(\\epsilon=0.002\\)的位置画一个平行于Linear Elastic Region的直线，其交于SS Curve的位置即为Yield Strength ex. #\rA hypothetical metal has a 0.2% offset yield strength of 358 MPa, an ultimate tensile strength of 522 MPa, and a fracture strength of 460 MPa. A sample of this metal, originally 1 m in length with a cross section of 2 mm × 2 mm is loaded along its long axis. Just before fracture, while the load is still applied, the length is 1.3 m and when the load is released, the length is 1.18 m. Calculate the modulus of elasticity in GPa.（这我做集贸啊）\n之前有公式\\(\\sigma=E\\epsilon\\) 现在需要将其推广到\\(E=\\frac{\\Delta \\sigma}{\\epsilon}\\)上 \\(\\Delta \\sigma\\)：有460MPa-0MPa=460MPa \\(\\epsilon\\)：有0.3-0.18=0.12 Uniform deformation #\r在经历了Linear Elastic Region后，出现的便是Uniform Platic区间 这个阶段内，材料的塑性变形均匀地分布在整个试样或构件中 其具体的Unifrom体现在了变形过程中结构的完整性上 在Non-Unifrom Region中，材料已经发生了局部的Fracture，即产生了Neck 这也解释了为什么在过了Ultimate Tensile strength后Stress开始下降，即当金属样品承受的应力值逐渐增大时，最终会开始失效。 在拉伸过程中，当金属原子间的一些键断裂时，就会出现这种情况 The Dislocation #\rDisloaction广泛存在于各种晶体材料中，不仅限于金属。 它们在不同类型材料中的运动机制和对材料性能的影响各不相同。 例如，在金属中位错运动相对容易，而在陶瓷和半导体中则较为困难，且其作用更为复杂 具体来说存在有4种不同的Imperfections，来自不同的维度 Dislocation Density #\rDislocation Density: 材料中单位体积内存在的位错数量 Dislocation Density越高，材料的强度和硬度可能会有所增加，但延展性会降低 Metal #\rDislocations are always present in metals. 可以通过加热的方式更改Metal的dislocation density Zero-Dimensional Imperfections or Point Defects #\r点缺陷（Point Defect）是指材料的晶体结构中，由于原子或离子位置上的异常，导致的局部晶体结构缺陷 Interstitial Impurities #\rInterstitial Impurities指的是一些较小的Atom（比如碳、氮等）进入了Crystal中本来空着的间隙位置 Substitutional Impurities #\r当一个外来原子取代了晶体中正常位置上的原子时，形成Substitutional Impurities。 这种缺陷在合金中常见，例如铜和锌形成的黄铜 Vacancies #\r在晶体中，一个本应有原子的位置上缺少了一个原子 它会导致周围的原子重新调整位置，影响材料的物理性质 但Vacancies的产生并不会导致Material的Strength发生改变s Zero-Dimension\u0026rsquo;s Influence on Higher Dimension #\r当Zero Dimension Impurity发生的时候，其会对周围的Crystal Structure产生一个Strain Fields 这个Strain Field将会Repel其他的Atom进入Dislocation 其通常会造成一种One-Dimensional Imperfection Ratio of the Number of Vacancies #\r$$\\frac{N_v}{N} = e^{\\frac{-Q_v}{kT}} \\tag{1}$$\nNv​：这是Crystal中Number of Vancancies，即晶体结构中缺失原子的位置数。\nN：Number of Atoms\nQv​：表示生成一个Vacancies所需的Energy，通常以电子伏特（eV）为单位。\nk：这是Boltzmann Constant，数值约为 \\(1.38\\times 10^{-23}J/K\\)，用于将温度与能量联系起来。\nT：这是Thermodynamic temperature，以开尔文（K）为单位，是热力学温度是根据热力学原理来衡量系统绝对温度的一个度量，其零点对应理论上的绝对零度，即系统的分子运动几乎完全停止、能量达到最低的状态。\n从本质上讲，原子在它们的晶格位点上拼命振动，试图跳出它们的位点。 它们真的很努力。 我的意思是说，每秒大约有 1013 次！ 在固态中，由于结合能强于热能，大多数情况下它们都不会成功（实际上，原子形成有序固体有很大的节能作用，但我们稍后会详细介绍）。 但偶尔也会有原子从其晶格位置成功跃迁，并移动到晶格的其他位置。 这就会留下一个缺失的原子或空位。 因此，我们可以将空位的形成看作是将原子固定在晶格位点上的结合能与将原子从晶格位点上挤出的热能之间的持续斗争\nThermodynamics 热力学 #\rThermodynamics说明了在一个物体中，能量是分布在Atoms上而不是对于所有Atom都具有相同能量的 因此，单个原子可能有足够的能量跳出其晶格位置，而其余大多数原子则没有，这是有道理的 如图 10 所示，随着温度的升高，我们发现有更多的原子进入了高能态。 同时，随着温度的降低，高能态原子的数量也在减少。 Boltzmann Distribution #\r在绝对零度（0 K）：所有原子都会处于最低能量状态，因为此时系统没有足够的能量让原子占据更高的能量状态。 在无限温度（∞ K）：系统的温度极高，粒子的能量足以占据任何能量状态。因此所有能量状态的粒子数都会趋于相等，也就是所有能量状态均等分布 在Boltzmann Distribution中，不会出现所有粒子只占据最高能量状态的情况，即使是在极高温度下 One-Dimensional Imperfections or Dislocations #\rCold Work #\r当我们对金属进行Plastic Deformation时，会产生新的Dislocation。 这增加了Dislocation Density，这将在未来中增加Dislocation的难度 当我们观察金属的Stress-Strain Curve，发现应力在Yield Strength之后继续增加时，我们首次观察到金属通过Plastic Deformation而得到强化。 事实上，如果我们Unload一个sample并重新load，其在stress水平达到我们在前一个循环中留下的Stress之前不会开始Plastic Deformation 这是一种通过塑性变形进行的强化，不过在工业上，我们通常是通过轧制或拉动金属零件，或将金属零件压入模具来实现塑性变形，而不是通过简单的拉伸来拉动零件 Hot Work #\r材料被加热到再结晶温度以上，这意味着减少了Dislocation Density Two-Dimensional Imperfections #\rFree Surfaces #\r自由表面（Free Surfaces） 是指材料的外部表面, 这些表面与外界环境直接接触 原子排列不规则：在材料的自由表面，原子周围的配位数（与其他原子结合的数量）比材料内部的原子要少 Grain Boundaries #\r是指多晶材料中不同Crystal Structure交界的地方 当Dislocation在Crystal中移动，其必定要穿过Grain Boundary，然而这对位错来说是一个挑战 如果我们减小金属的晶粒尺寸，就会产生更多的晶界和更多的位错运动障碍，从而有望提高金属的强度 This strengthening mechanism has a pretty self-explanatory name: grain size reduction. Three-Dimensional Imperfections or Second Phase Particles #\rThree-dimensional imperfections occur any time we have a second phase within a solid 从本质上讲，如果固体中存在晶体结构不同的区域，就会出现第二相或三维缺陷 ","date":"Oct 16 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms5.furtheronstressstrain/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 10/16/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003eStress-Strain Curve \r\n    \u003cdiv id=\"stress-strain-curve\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#stress-strain-curve\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS5.FurtherOnStressStrain/MCMS5.FurtherOnStressStrain.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e当回头重新看Stress-Strain Curve的时候可以发现一些特殊的点\u003c/li\u003e\n\u003cli\u003e如Proportion Limit，Ultimate Tensile Strength两个点\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eProportional Limit \r\n    \u003cdiv id=\"proportional-limit\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#proportional-limit\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e在曲线的最初阶段，存在一个接近于Linear的区域，其代表了Linear Elastic的Region\u003c/li\u003e\n\u003cli\u003e而也必会存在一个点象征着Linear Elastic Region的结束\u003c/li\u003e\n\u003cli\u003e但犹豫一些测量的误差或者精度上的问题，将导致最终的这一个Linear Elastic结束的点无法被确定，所以需要一个约定俗成的方法\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003e0.2% Offset Yield Strength \r\n    \u003cdiv id=\"02-offset-yield-strength\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#02-offset-yield-strength\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS5.FurtherOnStressStrain/MCMS5.FurtherOnStressStrain-1.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"MCMS 5. Further On Stress Strain","type":"docs"},{"content":" Last Edit: 11/4/24\nPolymer 聚合物 #\r前缀“poly-”意指“很多”，暗示了这些聚合物分子结构中有许多重复的部分。 而“mer”指的是“重复单元”或“基元”，这是一种分子的基本单元 Basic Structure #\r与Crystal Structure不同，他们的基本结构是Cubics。并且其会在三维空间（即沿着三个方向）重复排列，形成一个规则的晶体结构 Polymer的结构则不同，它的基元通常只在One Dimnesion上重复，这种重复形成了一条长链 可以看到Polymer的内部结构为长线，而线其实是由很小的分子所组成的 Polyethylene and Polymer #\rPolymer #\r聚合物是一个广义术语，指的是由许多重复单元（基元）组成的长链分子。 聚合物的基元可以是各种各样的分子结构，不限于某一种类型。 Polyethylene #\r聚乙烯是一种具体的聚合物，由重复的乙烯单元（C₂H₄）构成。乙烯单元经过聚合反应形成长链，从而产生聚乙烯。 它是最常见的合成聚合物之一，但只是聚合物的一种类型。 放大上图可以发现 Polyethylene由String结构组成，而String则由分子组成 分子内部，如上图的Polyethylene则是由Hydrogen和Carbon Atoms组成，而Atom之间的作用力则为Intramolecular Bonds（作用在分子内部的相互作用力） Polymer\u0026rsquo;s Deformation #\r在观察超市购买了重物，并将它们放入一个塑料袋中后，可能会发现袋子的把手开始拉伸，甚至感到不适，因为它开始压入手中。随着重量的增加，把手继续拉伸，但在某个点，它似乎停止了延展。有时你甚至能在被拉伸的地方看到颜色的轻微变化 这一现象也可以通过String Model解释 Initial Stage #\r拉伸初期，由于Polymer内部结构仍是Randomly Oriented的Polymer chains没有按照拉伸的方向对齐 于是就可以像拉开松散的线一般将他们分开 Bonds #\rPrimary Bond #\rPrimary bond是 intramolecular bond，存在于一个分子内部的原子之间，例如Covelent Bond或Ionic Bond 这种键非常强，是构成Polymer Chians的基本骨架。 Secondary Bond #\rSecondary bond 是 intermolecular bond，即分子之间的作用力，比如范德华力或氢键 这种键相对较弱，存在于不同分子链之间，允许它们在一定条件下滑动或重新排列 Plastic Deformation Stage #\r随着拉伸的继续，Polymer chains开始发生滑动 滑动可以类比为面条互相滑动的情形 这就是分子间的“Friction”——在Polymer中我们称之为“Intermolecular Force”，也叫“secondary bond” （次级键） Alignment Stage #\r在Polymer Chain到位了之后，Secondary Bond（Inter Molecular Bond）的作用逐渐减小，变为了Primary Bond（Intra Molecular Bond） 所以Polymer的Plastic Deformation所需要的Stress反而会上升 Yield Strength #\r由于对于Polymer来说在发生了Plastic Deformation后其Polymer Chain将会变为Secondary Bond受力，其Stress反而上升了，所以将Curve在Plastic Deformation时候的峰值作为Yield Strength是合理的 The change of mer Units #\r已知Polymer由Polymer Chain组成，而Polymer Chain则是由很多Molecular (Mer)组成的 所以改变Molecular的元素便直接改变了Polymer Polypropylene 聚丙烯 #\r聚丙烯 (PP) 是一种非常常见且有用的聚合物。星巴克® 的那些漂亮的可重复使用杯子就是用 PP 制成的 在塑料瓶的底下可以看到其Recycling Code 为5 通常来说Polypropylene会比Polythylene更加坚固，其来自于额外的\\(CH_3\\) Polyvinylchloride (PVC) #\r出于某些原因，PVC的名字里就出现了Cl，所以其分子当然也包含Cl 而对于PVC中的V来说，其代表了Vinyl，是这种结构 Periodic Table 元素周期表 #\r对于周期表来说，其具有以下特性 周期（横向）趋势：当从左到右观察周期表时，原子半径逐渐减小，电子更接近原子核，因此核对外层电子的吸引力增强，Electronegativity增大 族（纵向）趋势：从周期表的顶端向底部移动时，原子半径增大，外层电子距离原子核更远，核对电子的吸引力减弱，因此Electronegativity减小 Electronegativity 电负性 #\r反应了Atom吸引电子的强弱程度 Bonding 分子键 #\rCovalent Bond 共价键 #\r十分熟悉的一种Bonding Type 一个Molecule中的两个Atom共享Electron Nonpolar Covalent Bond 非极性共价键 #\r当两个原子的Electronegativity几乎相等的时候，形成Nonpolar Covalent Bond 其特性为Electorn将均匀的分布于Atom之间 Polar Covalent Bond 极性共价键 #\r两个Atom的Electronegativity具有明显差异的时候 其仍然是Covalent Bond，但是Sharing的Atom会明显的偏向于其中一个Electronegative更大的Atom 可以发现Cl带有了部分的负电荷（Electron），所以其是具有更高Electronegativity的那个 并且Electrons在Covalent Bond中将会偏向于Cl Ionic Bond 离子键 #\r两个原子间的Electronegativity差极大时，一个Atom将会抢走另外一个的Electron形成Ionic Bond Polytetrafluoroethylene 聚四氟乙烯 #\r一种非反应性的Polymer 每一个Carbon Atom上都结合了大量的Floride，它们很大可以防止PTFE内部不被波坏 虽然Floride的electronegativity很高，但由于其Molecule中的对称性结构，所以形成了一个NonPolar Covalent Bond Hydrophobic 疏水性 #\rPTFE的特殊点在于其为一种Hydrophobic Polymer，具体来说Liquid无法通过其缝隙进入材料，而Gas却可以自由的通过 这就是户外服装在雨中保持干爽的愿意，一般称其为“Breathing\u0026quot; Polymethylmethacrylate 聚甲基丙烯酸甲酯 PMMA #\r一种透明的Polymer 每个合并单元上的大侧基团（通常称为 \u0026ldquo;笨重 \u0026ldquo;侧基）会阻止分子相互靠近组织。 这就确保了聚合物是完全无组织的，或者说是无定形的。 当聚合物结晶时，其折射率与无定形时不同。 如果聚合物中既有无定形的部分，也有结晶的部分（即所谓的半结晶），那么穿过聚合物的光线就不会沿着直接的路径传播，聚合物就会呈现半透明或不透明的状态。 因此，PMMA 之所以是透明的，部分原因是合并单元确保其保持 100% 透明 Length of Polymer Chain #\r前面提到过Polymer Chain是很长的，但却没有给出一个量化的办法 在合成Polymer的时候，控制其分子长度是很困难的 但可以得到一个其长度的分布图 假定一个理想的Polymer Sample，其Polymer Chain的存在需要通过一种分布来描述 描述这个分布的方式并不是通过长度而是重量 具体来说，假设有如下盒子，其里面包含了绳子 木盒子，里面有一段绳子。 你不能打开盒子，你需要确定盒子里绳子的长度。 给你一个空盒子的质量、一根一米长的绳子和一个天平。 你可以用质量来确定盒子里绳子的长度 发现可以通过绳子单位长度的质量计算盒子里绳子的长度 Number Average Molecular Weight 数均分子量 #\r$$\\overline{M}{\\text{number}} = \\frac{\\sum{n=1}^{i} M_n x_n}{\\sum x_n}$$\n所有分子的分子量加总后除以分子总数得到的平均分子量 \\(M_{number}\\)​：数均分子量 \\(M_n​\\)：第n类分子的分子量 \\(x_n\\)：第n类分子的数量比例（即该类别分子数占总分子数的比例） Analogy Candy Box #\rWeight Average Molecular Weight 重均分子量 #\r$$\\overline{M}{\\text{weight}} = \\sum{n=1}^{i} M_n w_n $$\n\\(M_{weight}\\)​：重均分子量 \\(M_n​\\)：第n类分子的分子量 \\(x_n\\)：第n类分子的质量分数（即该类别分子总质量占总所有分子总质量的比例） Analogy #\r用同样的模型说明 Dispersity 分散性 #\r$$\\mathcal{D} = \\frac{\\overline{M}{\\text{weight}}}{\\overline{M}{\\text{number}}} $$\n\\(\\mathcal{D}\\): 分散性 Dispersity \\(M_{weight}\\)​：重均分子量 \\(M_{number}\\)​：数均分子量 当\\(\\mathcal{D} \u0026gt;1\\)时：分子量分布较宽，即多分散（Polydisperse）。随着\\(\\mathcal{D}\\)值增大，分子量的差异越大，分布越宽 Why Molecular Weight Anyway? #\r当面条较长时，很难将一根面条从其他面条中分离出来。 聚合物也是如此，当然，这也是了解分子量如此重要的原因。 随着聚合物分子量的增加，强度也会增加，而且由于长分子的缠结增加，断裂应变通常也会增加。 Ways of molecules stack up #\r聚合物分子虽然通常是线性的，但并非直线。 也就是说，它们是曲线形的 但这并不意味着它们总是完全无序的。 我们还看到，当聚合物发生塑性变形( chain Orientation) 时，会产生一些有序性 Crystalline Polymer 半结晶聚合物 #\r在某些情况下，它们可以在没有任何外部负载的情况下自行有序化 聚乙烯等简单聚合物中的分子通常会在自身上来回折叠 但是由于分子非常长，聚合物永远不可能 100% 结晶 并且由于Crystal Region比Amorphous通过Secondary Bond更牢固地结合在一起，因此这些区域的强度更高 Change of Crystal Density of Polymer 改变聚合物的结晶度 #\r对于Polymer Chain来说，几乎总是有一些所谓的分支从主分子上延伸出来，其仍然是分子的一部分 事实上，我们经常会专门设计一种聚合物，使其具有分支。 低密度聚乙烯LDPE就是这种情况 低密度聚乙烯LDPE中更多和更长的分支降低了分子相互靠近排列的能力，降低了结晶度，从而降低了密度、强度甚至弹性模量 Change of the Intramolecular Bonds #\r想要通过mer unit 改变Polymer整体强度，则可以use elements that are very electronegative We could also ensure that they are bonded to something that is very easy to make positive 于是就可以想到Hydrogen Hydrogen #\r对于Hydrogen来说其有很低的Electronegativity，其Electron很容易被抢走，剩下其Proton 只需要一个有点电负性的元素，氢就会变成正元素 Hydrongen Bond #\r犹豫Hydrogen具有的特小的Electronegativity，导致了其极易产生一个High Strengh Polar Covalent Bond（强偶极子键），所以一种特殊的Bond则尤其命名：Hydrogen Bond Introducing Hydrogen Bond between Molecules #\r将分子之间引入氢键是一种增强分子间相互作用的方式 Cross Link 交联 #\r交联则是通过强的主键（即分子内的共价键）将聚合物分子永久地连接起来，从而显著增强聚合物的强度和弹性。交联聚合物的一个经典例子是天然乳胶橡胶。 在制作弹性体时，交联程度需要适中。如果交联过多，聚合物会变得硬且脆，失去弹性，不再适合作为弹性体 Temperature #\r聚合物的许多特性都是由分子间的弱键造成的 这些键（有时也称为相互作用）更容易被热能破坏，即使温度相对较低：接近室温 在金属或陶瓷中，大部分特性都是由将它们连接在一起的强键、主键的性质决定的（稍后将详细介绍），这些键的键能远远高于接近室温时的热能。 Melting Temperature 熔点温度（Tm​） #\r熔点温度指的是聚合物从固态转变为液态的温度 超过这个温度后，聚合物会像厚液体一样流动 Glass Transition Temperature 玻璃化转变温度 (Tg) #\r通常适用于非晶态或半晶态聚合物 表示的是聚合物从硬脆的“玻璃态”转变为柔软、易变形的“橡胶态”的温度 低于Tg的温度下，聚合物链段的运动受到限制，材料表现出类似玻璃的刚性 高于Tg的温度下，链段有更多的活动空间，材料变得柔软。 Melting Process #\r在加热过程中，热能将首先在Amorphous Region开始破坏分子间的键能。 随着持续加热，热能最终将足以破坏Crystal Region的分子间键 因此，Amorphous Region被破坏时的较低温度被称为Glass Transition Temperature 当Polymer低于Tg时，其像普通窗玻璃一样又硬又脆 Loading Time 施加负载的时间 #\r快速施加负荷：当它被快速拉伸时会像脆性材料一样断裂，且无永久变形。这是因为其分子链较短，在快速拉伸时分子之间没有足够的时间进行重新排列，导致聚合物直接破裂。 长时间施加负荷，聚合物会发生蠕变，即随着时间的延长，材料会逐渐变形 弹性变形（Elastic Deformation） #\r特点：弹性变形是瞬时的，即加载后立即产生变形，卸载后立即恢复原状。 变形性质：弹性变形是可逆的，即材料可以恢复到原始形状，不会有永久的变形残留。 应用场景：在桌子短暂放置在地毯上的情况下，地毯纤维受到压力后会产生弹性变形，但桌子移开后，地毯几乎立即恢复原状。 分子运动：在弹性变形中，聚合物分子链段的变形非常有限，分子间没有发生显著的滑动或重新排列。 粘性变形（Viscous Deformation） #\r特点：粘性变形是随时间累积的，即需要长时间加载才能显现。 变形性质：粘性变形是不可逆的，即变形在卸载后不完全恢复，会留下永久变形。 应用场景：当桌子长时间放置在地毯上（例如一年），地毯纤维会缓慢移动或滑动，导致永久变形，即使桌子移开后，地毯也无法完全恢复原状。 分子运动：在粘性变形中，聚合物分子链段逐渐滑动，重新排列，表现为类似液体流动的行为，这个过程不可逆。 粘弹性变形（Viscoelastic Deformation） #\r聚合物材料通常表现出粘弹性变形，即兼具弹性和粘性变形的特性。它们在短时间内表现为弹性变形，但在长时间加载下逐渐表现出粘性变形。不同聚合物在粘弹性方面有所差异：\nLimits of the noodle model #\r一个模型几乎总是有缺点的。 如果不是这样，我们就会称之为定律 再次考虑前面提到的Transparent Glass， 我们说过，部分原因是由于这种聚合物是完全无定形的，这是事实 但是，这并不能解释为什么Amorphous Polymer本身就应该是透明的 要真正理解这一点，我们需要进一步了解可见光的本质以及光与材料中电子的相互作用 这是因为材料的透明性主要取决于光在其中的传播方式 当可见光照射到材料上时，光会与材料中的电子发生相互作用，而这种相互作用的方式决定了光是被吸收、反射还是通过材料 在透明的非晶态聚合物（如Plexiglas®）中，分子的电子结构允许可见光穿过，而不会显著散射或吸收光，因此表现出透明性。 相比之下，在非晶态金属中，电子结构密集且自由度较高，能够大量吸收和反射光，从而使材料表现为不透明和反光 ","date":"Nov 4 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/ecms6.plastics/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit: 11/4/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003ePolymer 聚合物 \r\n    \u003cdiv id=\"polymer-%E8%81%9A%E5%90%88%E7%89%A9\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#polymer-%E8%81%9A%E5%90%88%E7%89%A9\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e前缀“poly-”意指“很多”，暗示了这些聚合物分子结构中有许多重复的部分。\u003c/li\u003e\n\u003cli\u003e而“mer”指的是“重复单元”或“基元”，这是一种分子的基本单元\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003eBasic Structure \r\n    \u003cdiv id=\"basic-structure\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#basic-structure\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e与Crystal Structure不同，他们的基本结构是Cubics。并且其会在三维空间（即沿着三个方向）重复排列，形成一个规则的晶体结构\u003c/li\u003e\n\u003cli\u003ePolymer的结构则不同，它的基元通常只在One Dimnesion上重复，这种重复形成了一条长链\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/docs/EMCS_Static/ECMS6.Plastics/ECMS6.Plastics.png\" alt=\"\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"MCMS 6. Plastics","type":"docs"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/","section":"Buezwqwg","summary":"","title":"Buezwqwg","type":"page"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/tags/chemistry/","section":"Tags","summary":"","title":"Chemistry","type":"tags"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/tags/docs/","section":"Tags","summary":"","title":"Docs","type":"tags"},{"content":"Simple, yet powerful. Learn how to use Blowfish and its features.\n","date":"Nov 4 2024","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"\u003cp\u003eSimple, yet powerful. Learn how to use Blowfish and its features.\u003c/p\u003e","title":"Docs","type":"docs"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/series/ecms/","section":"Series","summary":"","title":"ECMS","type":"series"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/tags/ecms/","section":"Tags","summary":"","title":"ECMS","type":"tags"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"Nov 4 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"\rYour browser does not support the video tag.\r","date":"Oct 22 2024","externalUrl":null,"permalink":"/docs/projectilemotion/","section":"Docs","summary":"\u003cvideo width=\"640\" height=\"360\" controls\u003e\r\n  \u003csource src=\"Projectile.mp4\" type=\"video/mp4\"\u003e\r\n  Your browser does not support the video tag.\r\n\u003c/video\u003e","title":"Projectile Motion when air resisitance is propftional to velocity","type":"docs"},{"content":" “这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想” - 三体\nYour browser does not support the video tag.\r","date":"Oct 17 2024","externalUrl":null,"permalink":"/docs/montecarlomethod/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003e“这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想” - 三体\u003c/p\u003e","title":"Monte Carlo Approach to Calculate π","type":"docs"},{"content":"\r元数据 #\r[!abstract] 三体（全集）\n书名： 三体（全集） 作者： 刘慈欣 简介： 每个人的书架上都该有套《三体》！关于宇宙最狂野的想象！就是它！征服世界的中国科幻神作！包揽九项世界顶级科幻大奖！出版16个语种，横扫30国读者！奥巴马、雷军、马化腾、周鸿祎、潘石屹、扎克伯格……强推！刘慈欣获得2018年度克拉克想象力贡献社会奖！刘慈欣是中国科幻小说的最主要代表作家，亚洲首位世界科幻大奖“雨果奖”得主，被誉为中国科幻的领军人物。 出版时间： 2018-12-01 00:00:00 ISBN： 9785214589626 分类： 精品小说-科幻小说 出版社： 海南省电子音像出版社 PC地址：https://weread.qq.com/web/reader/ce032b305a9bc1ce0b0dd2a 高亮划线 #\r第十三章 红岸之二 #\r📌 雷政委说完站起来，迈着军人的稳健步伐离去。叶文洁的双眼盈满了泪水，透过眼泪，屏幕上的代码变成了一团团跳动的火焰。自父亲死后，这是她第一次流泪。 叶文洁开始熟悉监听部的工作，她很快发现，自己在这里远不如在发射部顺利，她已有的计算机知识早已落后，大部分软件技术都得从头学起。虽然有雷政委的信任，但对她的限制还是很严的，她可以看程序源代码，但不许接触数据库。 在日常工作中，叶文洁更多是接受杨卫宁的领导，他对她更加粗暴了，动不动就发火。雷政委多次劝他也没用，好像一见到叶文洁，他就充满了一种无名的焦虑。 ⏱ 2024-08-30 09:52:10 ^695233-17-5851\n第十六章 三体、哥白尼、宇宙橄榄球、三日凌空 #\r📌 很好，哥白尼，很好，你这种现实的、符合实验科学思想的想法是大多数学者不具备的，就凭这一点，你的理论也值得听一听。” 教皇对汪淼点点头，“说说看吧。” 汪淼走到长桌的另一端，让自己镇定了一下，说：“其实很简单：太阳的运行之所以没有规律，是因为我们的世界中有三颗太阳，它们在相互引力的作用下，做着无法预测的三体运动。当我们的行星围绕着其中的一颗太阳做稳定运行时，就是恒纪元；当另外一颗或两颗太阳运行到一定距离内，其引力会将行星从它围绕的太阳边夺走，使其在三颗太阳的引力范围内游移不定时，就是乱纪元；一段不确定的时间后，我们的行星再次被某一颗太阳捕获，暂时建立稳定的轨道，恒纪元就又开始了。这是一场宇宙橄榄球赛，运动员是三颗太阳，我们的世界就是球！” ⏱ 2024-08-30 10:05:15 ^695233-20-2574\n第十七章 三体问题 #\r📌 这时，我就像一个半生寻花问柳的放荡者突然感受到了爱情。 “你不知道庞加莱[插图]吗？”汪淼打断魏成问。 当时不知道，学数学的不知道庞加莱是不对，但我不敬仰大师，自己也不想成大师，所以不知道。但就算当时知道庞加莱，我也会继续对三体问题的研究。全世界都认为这人证明了三体问题不可解，可我觉得可能是个误解，他只是证明了初始条件的敏感性，证明了三体系统是一个不可积分的系统，但敏感性不等于彻底的不确定，只是这种确定性包含着数量更加巨大的不同形态。现在要做的是找到一种新的算法。当时我立刻想到了一样东西：你听说过“蒙特卡洛法”吗？哦，那是一种计算不规则图形面积的计算机程序算法，具体做法是在软件中用大量的小球随机击打那块不规则图形，被击中的地方不再重复打击，这样，达到一定的数量后，图形的所有部分就会都被击中一次，这时统计图形区域内小球的数量，就得到了图形的面积，当然，球越小结果越精确。 ⏱ 2024-08-30 10:16:26 ^695233-21-4188\n📌 这种方法虽然简单，却展示了数学中的一种用随机的蛮力对抗精确逻辑的思想方法，一种用数量得到质量的计算思想。这就是我解决三体问题的策略。我研究三体运动的任何一个时间断面，在这个断面上，各个球的运动矢量有无限的组合，我将每一种组合看做一种类似于生物的东西，关键是要确定一个规则：哪种组合的运行趋势是“健康的”和“有利的”，哪种是“不利的”和“有害的”，让前者获得生存的优势，后者则产生生存困难，在计算中就这样优胜劣汰，最后生存下来的就是对三体下一断面运动状态的正确预测。 ⏱ 2024-08-30 10:16:27 ^695233-21-4775\n第三十二章 古筝行动 #\r📌 叶文洁：如果他们能够跨越星际来到我们的世界，说明他们的科学已经发展到相当的高度，一个科学如此昌明的社会，必然拥有更高的文明和道德水准。 审问者：你认为这个结论，本身科学吗？ 叶文洁：…… ⏱ 2024-09-02 13:19:12 ^695233-36-12335-12543\n危机纪年第20年，三体舰队距太阳系4.15光年 #\r📌 人类大脑的进化需要两万至二十万年才能实现明显的改变，而人类文明只有五千年历史，所以我们目前拥有的仍然是原始人的大脑 ⏱ 2024-10-12 11:38:59 ^695233-47-9290-9347\n📌 政治思想工作是通过科学的理性思维来建立信念。 ⏱ 2024-10-12 11:39:43 ^695233-47-10801-10823\n下部 黑暗森林 #\r📌 普通人的目光，是他们所在地区和时代的文明程度的最好反映。他曾经看到过一组由欧洲摄影师拍摄的清朝末年的照片，最深的印象就是照片上的人呆滞的目光，在那些照片上，不论是官员还是百姓，眼睛中所透出的只有麻木和愚钝，看不到一点生气。 ⏱ 2024-10-12 12:13:19 ^695233-48-2540-2651\n📌 给岁月以文明，而不是给文明以岁月。 ⏱ 2024-10-12 13:19:45 ^695233-48-32277-32294\n📌 章北海停下手中的笔，抬头看着舱外的东方延绪，他的目光平静如水，“同为军人，知道我们之间最大的区别在哪里吗？你们按照可能的结果来决定自己的行动；而我们，不管结果如何，必须尽责任，这是唯一的机会，所以我就做了。 ⏱ 2024-10-12 13:59:12 ^695233-48-84364-84467\n📌 两个多世纪前，阿瑟·克拉克在他的科幻小说《2001：太空奥德赛》中描述了一个外星超级文明留在月球上的黑色方碑，考察者用普通尺子量方碑的三道边，其长度比例是1∶3∶9，以后，不管用何种更精确的方式测量，穷尽了地球上测量技术的最高精度，方碑三边的比例仍是精确的1∶3∶9，没有任何误差。克拉克写道：那个文明以这种方式，狂妄地显示了自己的力量。 ⏱ 2024-10-12 14:10:45 ^695233-48-100308-100477\n危机纪年第208年，三体舰队距太阳系2.07光年 #\r📌 因为在昨天晚上的演讲中，你说人类迟迟未能看清宇宙的黑暗森林状态，并不是由于文明进化不成熟而缺少宇宙意识，而是因为人类有爱。 “这不对吗？” 对，虽然“爱”这个词用在科学论述中涵义有些模糊，但你后面的一句话就不对了，你说很可能人类是宇宙中唯一拥有爱的种族，正是这个想法，支撑着你走完了自己面壁者使命中最艰难的一段。 “当然，这只是一种表达方式，一种不严格的……比喻而已。” 至少我知道三体世界也是有爱的，但因其不利于文明的整体生存而被抑制在萌芽状态，但这种萌芽的生命力很顽强，会在某些个体身上成长起来。 “请问您是……” 我们以前不认识，我是两个半世纪前曾向地球发出警告的监听员。 “天啊，您还活着？”庄颜惊叫道。 也活不了多长时间了，我一直处于脱水状态，但这么长的岁月，脱水的机体也会老化。不过我真的看到了自己想看的未来，我感到很幸福。 “请接受我们的敬意。”罗辑说。 我只是想和您讨论一种可能：也许爱的萌芽在宇宙的其他地方也存在，我们应该鼓励她的萌发和成长。 “为此我们可以冒险。” 对，可以冒险。 “我有一个梦，也许有一天，灿烂的阳光能照进黑暗森林。” 这时，这里的太阳却在落下去，现在只在远山露出顶端的一点，像山顶上镶嵌着的一块光灿灿的宝石。孩子已经跑远，同草地一起沐浴在金色的晚霞之中。 太阳快落下去了，你们的孩子居然不害怕？ “当然不害怕，她知道明天太阳还会升起来的。” ⏱ 2024-10-12 15:08:48 ^695233-49-13253-14291\n危机纪元4年，云天明 #\r📌 程心似乎听到了他心中的话，她慢慢抬起头来，他们的目光第一次这么近地相遇，比他梦中的还近，她那双因泪水而格外晶莹的美丽眼睛让他心碎。 但接着，程心说出一句完全意外的话：“天明，知道吗？安乐死法是为你通过的。” ⏱ 2024-10-14 12:31:36 ^695233-55-21409-21540\n危机纪元1-4年，程心 #\r📌 “你会把你妈卖给妓院吗？”维德问。 ⏱ 2024-10-14 12:31:06 ^695233-56-2751-2768\n威慑纪元61年，执剑人 #\r📌 因为杀的人太少了。杀一个人是要被判死刑的，杀几个几十个更是如此，如果杀了几千几万人，那就罪该万死；但如果再多些，杀了几十万人呢？当然也该判死刑，但对于有些历史知识的人，这个回答就不是太确定了；再进一步，如果杀了几百万人呢？那可以肯定这人不会被判死刑，甚至不会受到法律的惩处，不信看看历史就知道了，那些杀人超过百万的人，好像都被称为伟人和英雄；更进一步，如果这人毁灭了一个世界，杀死了其中的所有生命，那他就成了救世主！ ⏱ 2024-10-15 01:16:23 ^695233-60-4506-4714\n📌 “很复杂，直接原因是：那个恒星系，就是他向宇宙广播了坐标导致其被摧毁的那个，不知道其中有没有生命，但肯定存在有的可能，所以他被指控有世界灭绝罪的嫌疑。这是现代法律中最重的罪了。” ⏱ 2024-10-16 14:17:58 ^695233-60-4831-4920\n📌 如果说面壁计划是人类历史上首次出现的怪物，那黑暗森林威慑和执剑人在历史上却有过先例。公元20世纪华约和北约两大军事集团的冷战就是一个准终极威慑。冷战中的1974年，苏联启动Perimeter计划，建立了一个后来被称为末日系统的预警系统，其目的是在北约核突袭中，当政府决策层和军队高级指挥层均被消灭、国家已失去大脑的情况下，仍具备启动核反击的能力。它利用核爆监测系统监控苏联境内的核爆迹象，所有的数据会汇整到中央计算机，经过逻辑判读决定是否要启动核反击。这个系统的核心是一个绝密的位于地层深处的控制室，当系统做出反击的判断时，将由控制室内的一名值班人员启动核反击。公元2009年，一位曾参加过Perimeter战略值班的军官对记者披露，他当时竟然只是一名刚从伏龙芝军事学院毕业的二十五岁的少尉！当系统做出反击判断时，他是毁灭的最后一道屏障。这时，苏联全境和东欧已在火海之中，他在地面的亲人和朋友都已经死亡，如果他按下启动反击的按钮，北美大陆在半个小时后也将同样成为生命的地狱，随之而来的覆盖全球的辐射尘和核冬天将是整个人类的末日。那一时刻，人类文明的命运就掌握在他手中。后来，人们问他最多的话就是：如果那一时刻真的到来，你会按下按钮吗？ 这位历史上最早的执剑人说：我不知道。 ⏱ 2024-10-16 14:26:20 ^695233-60-9487-10051\n📌 人们发现威慑纪元是一个很奇怪的时代，一方面，人类社会达到空前的文明程度，民主和人权得到前所未有的尊重；另一方面，整个社会却笼罩在一个独裁者的阴影下。有学者认为，科学技术一度是消灭极权的力量之一，但当威胁文明生存的危机出现时，科技却可能成为催生新极权的土壤。在传统的极权中，独裁者只能通过其他人来实现统治，这就面临着低效率和无数的不确定因素，所以，在人类历史上，百分之百的独裁体制从来没有出现过。但技术却为这种超级独裁的实现提供了可能，面壁者和持剑者都是令人忧虑的例子。超级技术和超级危机结合，有可能使人类社会退回黑暗时代。 ⏱ 2024-10-16 14:28:31 ^695233-60-10907-11168\n📌 “看，她是圣母玛丽亚，她真的是！”年轻母亲对人群喊道，然后转向程心，热泪盈眶地双手合十，“美丽善良的圣母，保护这个世界吧，不要让那些野蛮的嗜血的男人毁掉这美好的一切。” ⏱ 2024-10-16 23:16:52 ^695233-60-20469-20553\n威慑纪元62年，奥尔特星云外，“万有引力”号 #\r📌 “三维，在弦理论中，不算时间维，宇宙有十个维度，可只有三个维度释放到宏观，形成我们的世界，其余的都卷曲在微观中。” ⏱ 2024-10-16 23:24:46 ^695233-61-13721-13778\n威慑纪元最后十分钟，62年11月28日16：17：34至16：27：58，威慑控制中心 #\r📌 在程心的潜意识中，她是一个守护者，不是毁灭者；她是一个女人，不是战士。她将用自己的一生守护两个世界的平衡，让来自三体的科技使地球越来越强大，让来自地球的文化使三体越来越文明，直到有一天，一个声音对她说：放下红色开关，到地面上来吧，世界不再需要黑暗森林威慑，不再需要执剑人了。 ⏱ 2024-10-17 01:26:30 ^695233-63-1797-1934\n威慑后一小时，失落的世界 #\r📌 “这都是为什么？”程心喃喃地问，更像是问自己。 “因为宇宙不是童话。” ⏱ 2024-10-17 01:30:38 ^695233-64-4901-4964\n第三部 #\r📌 安逸的美梦彻底破灭，黑暗森林理论得到了最后的证实，三体世界被摧毁了。 ⏱ 2024-10-17 15:58:32 ^695233-70-5401-5435\n广播纪元7年，云天明的童话 #\r📌 AA拿过程心叠好的带篷的小纸船，称赞很漂亮，然后示意程心也进浴室。在盥洗台上，她用小刀片从香皂上切下了小小的一片，然后把小纸船的尾部扎了一个小孔，把那一小片香皂插入小孔中，抬头对程心神秘地一笑，轻轻地把纸船放进已灌满水并且水面已经平静下来的浴缸中。 小船向前移动了，在这片小小的水面上，从此岸航向彼岸。 程心立刻明白了原理：香皂在水中溶解后，降低了小船后方水面的张力，但船前方水面的张力不变，小船就被前方水面的张力拉过去了￼。但这个想法转瞬即逝，程心的思想随即被一道闪电照亮！在她的眼中，浴缸中平静的水面变成了漆黑的太空，白色的小纸船在这无际的虚空中以光速航行… ⏱ 2024-10-17 22:30:43 ^695233-73-48751-49192\n📌 每秒十六点七千米，太阳系的第三宇宙速度，如果达不到这个速度就不可能飞出太阳系。 光也一样。 如果太阳系的真空光速降到每秒十六点七千米以下，光将无法逃脱太阳的引力，太阳系将变成一个黑洞￼。 ⏱ 2024-10-17 22:40:01 ^695233-73-63863-64231\n广播纪元8年，命运的抉择 #\r📌 这让我想起了那天夜里峨眉山的云海，”瓦西里说，“那是中国的一座山，在那山的顶上看月亮是最美的景致。那天夜里，山下全是云海，望不到边，被上空的满月照着，一片银色，很像现在看到的样子。” ⏱ 2024-10-17 22:55:27 ^695233-74-19329-19420\n📌 “其实吧，从科学角度讲，毁灭一词并不准确，没有真正毁掉什么，更没有灭掉什么，物质总量一点不少都还在，角动量也还在，只是物质的组合方式变了变，像一副扑克牌，仅仅重洗而已……可生命是一手同花顺，一洗什么都没了。” ⏱ 2024-10-17 22:55:16 ^695233-74-19474-19578\n第五部 #\r📌 剩下的事就是清理了，歌者再次从仓库中取出那个质量点。他突然想到清理弹星者是不能用质量点的，这个星系的结构与前面已死的那个星系不同，有死角，用质量点可能清理不干净，甚至白费力气，这要用二向箔才行。可是歌者没有从仓库里取二向箔的权限，要向长老申请。 ⏱ 2024-10-18 13:45:16 ^695233-79-5867-5989\n掩体纪元66年，太阳系外围 #\r📌 白Ice笑了起来，“再简单不过的事，你忘记《古兰经》中的故事了？如果大山不会走向穆罕默德，穆罕默德可以走向大山。” ⏱ 2024-10-18 14:26:27 ^695233-81-7378-7435\n📌 丁仪接着说：“在危机初期，当智子首次扰乱加速器时，有几个人自杀。我当时觉得他们不可理喻，对于搞理论的，看到那样的实验数据应该兴奋才对。但现在我明白了，这些人知道的比我多，比如杨冬，她知道的肯定比我多，想得也比我远，她可能知道一些我们现在都不知道的事。难道制造假象的只有智子？难道假象只存在于加速器末端？难道宇宙的其他部分都像处女一样纯真，等着我们去探索？可惜，她把她知道的都带走了。” ⏱ 2024-10-18 14:45:18 ^695233-81-10117-10309\n📌 “我说别傲慢，弱小和无知不是生存的障碍，傲慢才是，想想水滴吧！” ⏱ 2024-10-18 15:50:30 ^695233-81-12293-12325\n📌 “现在逃离，就像在瀑布顶端附近的河面上划船，除非超过一个逃逸速度，否则不论怎样划，迟早都会坠入瀑布，就像在地面向上扔石头，不管扔多高总会落回来。整个太阳系都在跌落区，从中逃离必须达到逃逸速度。” “逃逸速度是多少？” “我反复计算过四遍，应该没错。” “逃逸速度是多少？！” “启示”号和“阿拉斯加”号上的人们屏息凝神，替全人类倾听末日判决，白Ice把这判决平静地说出来： “光速。” ⏱ 2024-10-18 15:55:33 ^695233-81-15419-15751\n掩体纪元67年，二维太阳系 #\r📌 程心现在回想起两次看到《星空》时奇怪的感觉：画面中星空之外的部分，那火焰般的树，暗夜中的村庄和山脉，都呈现出明显的透视和纵深；但上方的星空却丝毫没有立体感，像挂在夜空中的一幅巨画。 因为星空是二维的。 他是怎么画出来的？1889年的凡·高，精神第二次崩溃的凡·高，难道真的用分裂和谵妄的意识，跨越五个多世纪的时空，看到了现在？！或者反过来，他早就看到了未来，这最后审判日的景象才是他精神崩溃和自杀的真正原因？！ ⏱ 2024-10-18 16:17:09 ^695233-83-16582-16843\n📌 隧洞前的罗辑笑了笑，“我要是想走，刚才就跟你们走了，我这样岁数的人，不适合远航了。孩子们，不要为我操心了，我说过的，我什么都没有失去。准备启动空间曲率驱动。” 罗辑的最后一句话是对飞船A.I.说的。 “航线参数？”A.I.问。 “目前航线的延长线吧，我也不知道你们要去哪儿，我想现在你们自己也不知道，要是想起了目的地，在星图上指出来就行了，半径五万光年内的大部分恒星，飞船都可以自动导航到达。” “指令执行中，空间曲率驱动引擎三十秒后启动。”A.I.说。 ⏱ 2024-10-18 16:19:44 ^695233-83-19371-19739\n📌 在宇宙中，曲率驱动航迹既可以成为危险标志，也能成为安全声明。如果航迹在一个世界旁边，是前者；如果把这个世界包裹在其中，则是后者。就像一个手拿绞索的人，他是危险的；但如果他把绞索套到自己的脖子上，他就变成安全的了。 ⏱ 2024-10-18 16:25:06 ^695233-83-22164-22270\n📌 “你们有个约会！”AA说。 “是的，我们有个约会。”程心机械地回答，感情的激荡使她处于呆滞状态。 “那就去你们的星星！” “好的，去我们的星星。”程心激动地对AA说。然后她问飞船A.I.，“能够定位DX3906恒星吗，这是危机纪元初的编号？” “可以，这颗恒星现在的编号是S74390E2，请确认。” ⏱ 2024-10-18 16:27:08 ^695233-83-26179-26441\n第六部 #\r📌 大气成分：氧35%，氮63%，二氧化碳2%，还有微量惰性气体，可以呼吸，但大气压只有0.53个地球标准气压，出舱后不要剧烈活动。”飞船A.I.说。 “站在飞船附近的那个生物是什么？”AA问。 “正常人类。”A.I.简单地回答。 ⏱ 2024-10-18 16:29:13 ^695233-84-4555-4724\n📌 也有人叫它末日飞船。那些光速飞船没有目的地，只是把曲率引擎开到最大功率疯狂加速，无限接近光速，目的就是用相对论效应跨越时间，直达宇宙末日。据他们计算，十年内就可以跨越五百亿年，那他们现在已经到了，哦，当然是以他们的参照系。其实，并不需要有意识地做这事，比如在飞船加速到光速后，曲率引擎出现无法修复的故障，使飞船不能减速，你也可能在有生之年到达宇宙末日。” ⏱ 2024-10-18 16:43:21 ^695233-84-13148-13325\n读书笔记 #\r本书评论 #\r","date":"Oct 16 2024","externalUrl":null,"permalink":"/notes/thethreebodyproblem/","section":"Thoughts","summary":"\u003ch1 class=\"relative group\"\u003e元数据 \r\n    \u003cdiv id=\"%E5%85%83%E6%95%B0%E6%8D%AE\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%85%83%E6%95%B0%E6%8D%AE\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cblockquote\u003e\n\u003cp\u003e[!abstract] 三体（全集）\u003c/p\u003e","title":"The Three Body Problem","type":"notes"},{"content":"Record some thinking\n","date":"Oct 16 2024","externalUrl":null,"permalink":"/notes/","section":"Thoughts","summary":"\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"Thoughts","type":"notes"},{"content":"Record some thinking\n","date":"Oct 14 2024","externalUrl":null,"permalink":"/blogs/","section":"Blogs","summary":"\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"Blogs","type":"blogs"},{"content":"","date":"Oct 14 2024","externalUrl":null,"permalink":"/tags/ontario/","section":"Tags","summary":"","title":"Ontario","type":"tags"},{"content":"\r","date":"Oct 14 2024","externalUrl":null,"permalink":"/blogs/ontariolake/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/OntarioLake/Ontario%20Lake-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/OntarioLake/Ontario%20Lake-2.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Ontario Lake","type":"blogs"},{"content":"","date":"Oct 14 2024","externalUrl":null,"permalink":"/tags/pic/","section":"Tags","summary":"","title":"Pic","type":"tags"},{"content":"","date":"Sep 23 2024","externalUrl":null,"permalink":"/tags/cover/","section":"Tags","summary":"","title":"Cover","type":"tags"},{"content":" Ramsay, S. (2024). Engineering Chemistry \u0026amp; Materials Science. Top Hat. https://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b. ","date":"Sep 23 2024","externalUrl":null,"permalink":"/docs/emcs_engineering-chemistry--materials-science/","section":"Docs","summary":"\u003cul\u003e\n\u003cli\u003eRamsay, S. (2024). Engineering Chemistry \u0026amp; Materials Science. Top Hat. \u003ca href=\"https://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b\" target=\"_blank\"\u003ehttps://app.tophat.com/e/797389/content/course-work/item/1213609::72131f51-0d59-4379-85f9-30182e840f9b\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e","title":"Engineering Chemistry \u0026 Materials Science","type":"docs"},{"content":"","date":"Apr 15 2024","externalUrl":null,"permalink":"/tags/computer-science/","section":"Tags","summary":"","title":"Computer Science","type":"tags"},{"content":"","date":"Apr 15 2024","externalUrl":null,"permalink":"/series/d2l/","section":"Series","summary":"","title":"D2L","type":"series"},{"content":"","date":"Apr 15 2024","externalUrl":null,"permalink":"/tags/d2l/","section":"Tags","summary":"","title":"D2L","type":"tags"},{"content":" Last Edit 4/15/24\nRegression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式\n[[Regression 回归]] 3.1.1 线性回归的基本元素 #\rLinear Regression 线性回归可以追溯到19世纪，其基于几个基本的假设 假设自变量与因变量之间为线性关系 假设噪声正常，如遵循正态分布 3.1.1.1 线性模型 #\r[[线性模型]] 线性假设是指目标可以表示为特征的加权和，如下例子 E.X. $$Price=w_{area}\\cdot area+w_{age}\\cdot age+b$$\nw称为Weight权重 b称为Bias，Offset或者Intercept偏置，即特征为0时的预测值 严格来说，上式为输入特征的一个[[Affine Transformation 仿射变换]]\n给定一个数据集，我们的目标即为寻找模型的Weight和Offset 高维数据集 #\r在Deep Learning 领域，我们通常使用的是高纬数据集，建模时采用[[2.3 线性代数]]的表示方法会比较方便。 当我们的输入包含多个特征时，我们将预测结果表示为\\(\\hat{y}\\) 点积形式 #\r可以用点积形式来简洁的表达模型\\(x\\in R^d,w\\in R^d\\) $$\\hat{y}=w^Tx+b$$ Model Parameters 模型参数 #\r在开始寻找最好的模型参数前，我们还需要两个东西 一种模型质量的度量方式 #\r一种能更新模型以提高预测质量的方式 #\r3.1.1.2 损失函数 #\r在开始考虑如何用模型Fit 拟合数据之前，我们需要一个拟合程度的度量 Loss Function 损失函数 #\r量话目标的实际值和预测值之间的差距 通常选用非负数作为Cost，并且数值越小损失越小 平方误差函数 #\r回归问题中最常用的Cost Function是平方误差函数 $$l^{(i)}(w,b)=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$ 常数\\(\\frac{1}{2}\\)的存在不会带来本质的差别，但当我们对这一方程求导后由于\\(\\frac{1}{2}\\)的存在会使常数等于1 由于平方误差函数中的二次方项，会导致估计值和观测值之间较大的差异造成更大的损失。 为了度量模型在整个数据集上的质量，我们需要计算训练集上的样本损失均值 $$L(w,b)= \\frac{1}{n}\\Sigma^n_{i=1}l^{(i)}(w,b)=\\frac{1}{n}\\Sigma^n_{i=1}\\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2$$ 总的来说训练模型就是为了找到一组参数\\(w^,b^\\)，其 $$w^,b^=argmin~L(w,b)$$ 3.1.1.3 解析式 #\r线性回归是一个很简单的优化问题，与大部分模型不同，其解可以用一个公式简单的表达出来，这类解便称为Analytical Solution 解析解 3.1.1.4 随机梯度下降 #\r即使在无法得到解析解的情况下，我们可以有效的训练模型 Gradient Descent 梯度下降 #\r最简单的方法就是计算Cost Function关于模型参数的导数（梯度） 但由于每次操作前都需要遍历整个数据集，导致执行速度非常之慢 所以通常会在每次更新时候抽取一小批样本，即为Minibatch Stochastic Gradient Descent 小批量随机梯度下降 SGD #\rMinibatch Stochastic Gradient Descent 小批量随机梯度下降 每次迭代中，我们首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数，并从当前参数的值中减掉 $$(w,b)\\leftarrow(w,b)-\\frac{\\eta}{|B|}\\Sigma_{i\\in B}\\partial_{w,b}l^{(i)}(w,b)$$ 初始化模型参数的值 从数据集中随机抽取小批量样本在负梯度方向上更新参数，并一直迭代 \\(\\eta\\)表示Learning Rate 学习率 B表示Batch Size 批量大小 Hyperparameter 超参数 #\r这些可以调整但不在训练过程中更新的参数称为超参数 Hyperparameter Tuning为调整Hyperparameter的过程 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的 收敛 #\rLinear Regression只会让预测值无限接近于实际值而却不能在有限的步数内非常精确地达到最小值 Generalization 泛化 #\r寻找到一组合适的Hyperparameter纵然困难，但更加困难的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为Generalization 泛化 3.1.1.5 用模型进行预测 #\r需要指出的是，Deep Learning对于实际值的接近更多是一种Prediction预测而非Inference推断 3.1.2 矢量化加速 #\r为了同时处理整个小批量的样本，同时防止在python中编写开销高昂的for循环 矢量化性能测试 #\r实例化两个全为1的10000维向量，采取两种处理方式，Python的for循环和对+的调用 ###初始化两个Tensor\rn = 10000\ra = torch.ones([n])\rb = torch.ones([n])\r###用for循环完成一次\rc = torch.zeros(n)\rtimer = Timer()\rfor i in range(n):\rc[i] = a[i] + b[i]\rf\u0026#39;{timer.stop():.5f} sec\u0026#39;\r###用线性代数完成矢量化运算\rtimer.start()\rd = a + b\rf\u0026#39;{timer.stop():.5f} sec\u0026#39; 得到的结果为 0.167sec 和0.00042 sec 3.1.3 正态分布与平方损失 #\r接下来，我们通过对噪声分布的假设来解读平方损失目标函数 正态分布 #\r[[Normal Distribution 正态分布]] 均方误差损失函数 #\r均方损失可以用于线性回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布，如下 $$y=w^Tx+b+\\epsilon$$ \\(\\epsilon\\)代表了噪声 Likehood 似然 #\r[[Likehood 似然]] 通过给定的x观测到特定y的似然（likelihood） $$p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)$$ 对于似然函数\\(L(\\theta|data)=P(data|\\theta)= \\Pi^N_{i=1}P(x_i|\\theta)\\) 已知x的正态分布密度函数，也就是x（\\(\\theta\\)取每个值的概率） 要求得给定x（\\(\\theta\\)）（其不固定，但遵循正态分布）观测到特点y的似然，得到公式 \\(L(x|y)=P(y|x)\\) 已知\\(p(y|x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)\\) 由于\\(P(y|x)=\\Pi^N_{i=1}P(y^{(i)}|x^{(i)})\\)（由x的参数条件下观测到y的可能性为独立的N个子事件的乘积） 根据[[Maximum Likehood Estimation 极大似然估计]]，参数w和b的最优值是使整个数据集的[[Likehood 似然]]最大的值 但又因为乘积最大化问题十分复杂，并且由于历史遗留问题，优化常说的不是最大化，而是最小化 所以我们需要通过最小化对数似然\\(-logP(y|x)\\)，由此可以得到的数学公式为 $$-logP(y|x)=\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(y^{(i)}-w^Tx^{(i)}-b)^2$$ 推导如下 \\(-logP(y|x)=-log\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)\\) \\(=-log\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{(-\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2)}\\) \\(=-log(\\sum\\limits^n_{i=1}\\frac{1}{\\sqrt{2\\pi \\sigma^2}})+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) \\(=-log\\sum\\limits^n_{i=1}(2\\pi\\sigma^2)^{-\\frac{1}{2}}+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) = \\(\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}(y-w^Tx-b)^2\\) 现在我们只需要假设�是某个固定常数就可以忽略第一项\\(\\sum\\limits^n_{i=1}\\frac{1}{2}log(2\\pi\\sigma^2)\\)，因为第一项不依赖于w和b 对于第二项，除了常数\\(\\frac{1}{\\sigma^2}\\)外，其余部分与[[#平方误差函数]]是一样的 平方误差函数 #\r$$l^{(i)}(w,b)=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$\n幸运的是，上面式子的解并不依赖于\\(\\sigma\\) 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计 3.1.4 从线性回归到深度网络 #\r到目前，我只谈论了线性模型，而神经网络涵盖了更为丰富的模型，并且我们也可以用描述神经网络的方式来描述线性模型，从而把线性模型看作一个神经网络。可以用“层”符号来重写这个模型 3.1.4.1 神经网络图 #\r制图表可以可视化模型中正在发生的事情，但该图只显示了链接模式，而不包含权重和偏置的值 在上图中，输入为\\(x_1,\\dots x_d\\)，可知输入层的Feature Dimensionality 输入数（或称为特征维度）为d\n网络的输出层为\\(o_1\\)，因此输出层的输出数为1\n需要注意的是，输入值都是已经给定的，并且只有一个_计算_神经元。 由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。\nFully-Connected Layer 全连接层 #\r对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换称为全连接层（Fully-connected layer）或称为稠密层（dense layer）。 3.1.4.2 生物学 #\r即使观察真实的神经元，但当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感\n我们援引斯图尔特·罗素和彼得·诺维格在他们的经典人工智能教科书 Artificial Intelligence:A Modern Approach (Russell and Norvig, 2016) 中所说的：虽然飞机可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。 同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。\n","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/","section":"Docs","summary":"\u003cblockquote\u003e\n\u003cp\u003eLast Edit 4/15/24\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eRegression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式\u003c/p\u003e","title":"D2L 3.1 Linear Regression","type":"docs"},{"content":"从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。\n3.2.1 生成数据集 #\r为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集 使用线性模型参数\\(w=[2,-3.4]^T,b=4.2\\)和噪声项\\(\\epsilon\\)生成数据集及其标签 $$y=Xw+b+\\epsilon$$ \\(\\epsilon\\)可以视为模型预测和标签时的潜在观测误差 3.2.2 读取数据集 #\r3.2.3 初始化模型参数 #\r通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。 w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\rb = torch.zeros(1, requires_grad=True) 在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据 并运用[[2.5 自动微分]]来计算梯度 3.2.4 定义模型 #\r这里我们用的还是线性模型，即$$\\hat y=w^Tx+b$$ def linreg(X, w, b): #@save\r\u0026#34;\u0026#34;\u0026#34;线性回归模型\u0026#34;\u0026#34;\u0026#34;\rreturn torch.matmul(X, w) + b 3.2.5 定义损失函数 #\r模型建立后，开始使用对原函数的损失函数进行梯度下降 这里我们使用[[3.1_LinearRegression#平方误差函数]] 3.2.6 定义优化算法 #\r使用[[3.1_LinearRegression#Minibatch Stochastic Gradient Descent 小批量随机梯度下降]] 3.2.7 训练 #\r本质为执行一下循环 初始化参数 更新梯度，更新参数 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/","section":"Docs","summary":"\u003cp\u003e从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。\u003c/p\u003e","title":"D2L 3.2 Object-Oriented Design for Implementation","type":"docs"},{"content":"本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型\n3.3.1 生成数据集 #\rimport numpy as np\rimport torch\rfrom torch.utils import data\rfrom d2l import torch as d2l\rtrue_w = torch.tensor([2, -3.4])\rtrue_b = 4.2\rfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000) d2l.synthetic_data(true_w, true_b, 1000) 是 d2l 库中的一个函数调用。这个函数用于生成合成数据，其中包括特征数据和对应的标签数据。 具体地，这个函数接受三个参数： true_w：真实的权重，用于生成特征数据。 true_b：真实的偏置，用于生成特征数据。 1000：生成数据的数量，这里是指生成1000个样本。 3.3.2 读取数据集 #\rdef load_array(data_arrays, batch_size, is_train=True): #@save\r\u0026#34;\u0026#34;\u0026#34;构造一个PyTorch数据迭代器\u0026#34;\u0026#34;\u0026#34;\rdataset = data.TensorDataset(*data_arrays)\rreturn data.DataLoader(dataset, batch_size, shuffle=is_train)\rbatch_size = 10\rdata_iter = load_array((features, labels), batch_size) 3.3.3 定义模型 #\r对于标准深度学习模型，我们可以使用框架的预定义好的层 ## nn是神经网络的缩写\rfrom torch import nn\rnet = nn.Sequential(nn.Linear(2, 1)) nn.Sequential：这是 PyTorch 中用于构建顺序神经网络模型的类。它允许用户按顺序堆叠多个层或模块，构建神经网络模型。 nn.Linear(2, 1)：这里创建了一个全连接层，其中 nn.Linear 是 PyTorch 中用于定义全连接层的类。构造函数 nn.Linear(in_features, out_features) 接受两个参数： in_features：输入特征的数量。在这个例子中，输入特征的数量为 2。 out_features：输出特征的数量。在这个例子中，输出特征的数量为 1。 因此，net 这个模型包含一个具有 2 个输入特征和 1 个输出特征的全连接层。 这样的模型可以用于简单的二分类问题，其中输入特征有 2 个，输出特征有 1 个，代表着模型对样本的分类结果。 3.3.4 初始化模型参数 #\rnet[0].weight.data.normal_(0, 0.01)\rnet[0].bias.data.fill_(0) 在网络的第一层输入参数 3.3.5 定义损失函数 #\r计算均方误差使用的是MSELoss类，也称为平方�2范数。 默认情况下，它返回所有样本损失的平均值。 loss = nn.MSELoss() 3.3.6 定义优化算法 #\rtrainer = torch.optim.SGD(net.parameters(), lr=0.03) 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。 3.3.7 训练 #\r通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。\nnum_epochs = 3\rfor epoch in range(num_epochs):\rfor X, y in data_iter:\rl = loss(net(X) ,y)\rtrainer.zero_grad()\rl.backward()\rtrainer.step()\rl = loss(net(features), labels)\rprint(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) num_epochs = 3：定义了训练的轮数，这里设置为 3 for epoch in range(num_epochs):：使用 for 循环迭代每个训练轮数 for X, y in data_iter:：使用 data_iter 迭代器遍历训练数据集，其中 X 是特征，y 是对应的标签 l = loss(net(X) ,y)：计算模型对当前批次数据的预测值，并计算与真实标签之间的损失 trainer.zero_grad()：梯度清零，以避免梯度累积 l.backward()：反向传播，计算损失函数相对于模型参数的梯度 trainer.step()：更新模型参数，采用优化算法更新参数 l = loss(net(features), labels)：计算当前训练轮数结束后整个训练集上的损失 print(f'epoch {epoch + 1}, loss {l:f}')：打印当前训练轮数和对应的损失值 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/","section":"Docs","summary":"\u003cp\u003e本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型\u003c/p\u003e","title":"D2L 3.3 A concise implementation of linear regression","type":"docs"},{"content":"回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。\n事实上，我们也对_分类_问题感兴趣：不是问“多少”，而是问“哪一个”\n3.4.1 分类问题 #\r[[One-hot encoding 独热编码]] 3.4.2 网格架构 #\r为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的[[Affine Function 仿射函数]] E.X.\n假设现在有3个未规范化的预测(Logit)：\\(o_1,o_2和o_3\\) \\(o_1=x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1\\) \\(o_2=x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2\\) \\(o_1=x_1w_{31}+x_2w_{32}+x_3w_{33}+x_4w_{34}+b_3\\) 3.4.3 全连接层的参数开销 #\r对于任何具有d个输入和q个输出的全连接层[[3.1_LinearRegression#Fully-Connected Layer 全连接层]]，其参数开销为\\(O(dq)\\)，但可以通过超参数减少到\\(O(\\frac{dq}{n})\\) 3.4.4 softmax 运算 #\r我们希望模型的输出\\(\\hat y_j\\)可以视为属于类\\(j\\)的概率，然后选择具有最大输出值的类别\\(argmaxx_jy_j\\)作为我们的预测，例如\\(\\hat y_1,\\hat y_2\\)和\\(\\hat y_3\\)分别为\\(\\hat y={0.1,0.8,0.1}\\)那么我们的预测变为独热编码的\\(y={0,1,0}\\)，即为鸡 能否将未规范化的预测o直接视作我们感兴趣的输出呢 #\r不行 因为将线性层的输出直接视为概率时存在一些问题 我们没有限制这些输出数字的总和为1 根据输入的不同，它们可以为负值 其违反了[[概率论公理]] 概率论 #\r要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率 Calibration 校准 #\r例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别 Softmax 函数 #\r社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上发明的softmax函数 softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式$$\\hat y=softmax(o)，其中\\hat y_j=\\frac{exp(o_j)}{\\sum_kexp(o_k)}=\\frac{e^j}{\\sum_ke^k}$$ 这里，对于所有的j总有\\(0\\leq\\hat y_j\\leq1\\)，因此\\(\\hat y\\)可以视为一个正确的概率分布 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。 3.4.5 小批量样本的矢量化 #\r为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算 3.4.6 损失函数 #\r使用[[Maximum Likehood Estimation 极大似然估计]] 3.4.6.1 对数似然 #\rsoftmax函数给出了一个向量\\(\\hat y\\)， 我们可以将其视为“对给定任意输入x的每个类的条件概率\n通过计算softmax的对数似然，可以推导出他的损失函数\n假设现在有一个数据集 \\({X,Y}\\)，其具有n个样本，其中索引i的样本由特征向量\\(x^{(i)}\\)和独热标签向量\\(y^{(i)}\\)组成，可以将估计值与实际值进行比较$$P(Y|X)=\\prod^n_{i=1}P(y^{(i)}|x^{(i)})$$\n根据[[3.1_LinearRegression#Likehood 似然]]，已知最大化$P(Y|X)，相当于最小化负对数似然 $$P(Y|X)=\\sum^n_{i=1}-logP(y^{(i)}|x^{(i)})=\\sum^n_{i=1}l(y^{(i)},\\hat y^{(i)})$$\n其中对于任何标签y和预测模型\\(\\hat y\\)，损失函数为$$l(y,\\hat y)=-\\sum^{q}_{j=1}y_j\\log \\hat y_j$$\n这个[[3.1_LinearRegression#Loss Function 损失函数]]并没有介绍过，他的名字为Cross-entropy Loss交叉熵损失，将在后面介绍到\n为什么要加入对数，而不是直接取负数 #\r数值稳定性： 在概率模型中，可能会有大量的乘法运算，这可能导致数值下溢或溢出问题，尤其是当概率很小的时候。通过取对数，可以将乘法运算转换为加法运算，从而提高计算的稳定性。 对数函数的导数相对于原函数来说更简单，这使得梯度的计算更加高效。特别是在梯度下降等优化算法中，简化的导数计算可以显著减少计算量。 对数函数的特性使得推导和分析变得更加简单，因为它可以将乘法转换为加法，并且有很多性质，例如对数函数的导数比原函数更容易处理 3.4.6.2 softmax及其导数 #\r由于softmax和相关的损失函数很常见， 因此我们需要更好地理解它的计算方式 将3.4.3带入Cross-entropy Loss Function中，得到 $$\\begin{align}l(y,\\hat y)=-\\sum^{q}{j=1}y_j\\log \\frac{e^{o_j}}{{\\sum^{q}{k=1}e^{o_k}}} \\=-\\sum_{j=1}^{q}y_j[\\ln e^{o_j}-\\ln \\sum^q_{k=1}e^{o_k}] \\=\\sum^q_{j=1}y_j\\log\\sum^q_{k=1}e^{o_k}-\\sum^q_{j=1}y_jo_j \\=\\log \\sum^q_{k=1}e^{o_k}-\\sum^q_{j=1}y_jo_j\\end{align}$$ Softmax结合Cross Entropy的求导过程 #\r已知Cross Entropy Function$$H(y_i,p_i)=-\\sum_iy_i\\log pi$$\n\\(y_i\\)为预测事件，\\(\\log p_i\\)为一个分布的最优编码\n得到[[Home Page]] 3.4.6.3 交叉熵损失 #\r[[Cross-Entropy 交叉熵]] 3.4.7.1 熵 #\r[[Cross-Entropy 交叉熵#2. 熵]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/","section":"Docs","summary":"\u003cp\u003e回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。\u003c/p\u003e","title":"D2L 3.4 Softmax Regression","type":"docs"},{"content":"MNIST数据集 (LeCun et al., 1998) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (Xiao et al., 2017)。\n在此引入这个数据集是因为之后对于算法的评估均给予这一数据集\n%matplotlib inline\rimport sys\rfrom mxnet import gluon\rfrom d2l import mxnet as d2l\rd2l.use_svg_display() 3.5.1 读取数据集 #\r## 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，\r## 并除以255使得所有像素的数值均在0～1之间\rtrans = transforms.ToTensor()\rmnist_train = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True)\rmnist_test = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True) Fshion-MNIST中包含的10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。 def get_fashion_mnist_labels(labels): #@save\r\u0026#34;\u0026#34;\u0026#34;返回Fashion-MNIST数据集的文本标签\u0026#34;\u0026#34;\u0026#34;\rtext_labels = [\u0026#39;t-shirt\u0026#39;, \u0026#39;trouser\u0026#39;, \u0026#39;pullover\u0026#39;, \u0026#39;dress\u0026#39;, \u0026#39;coat\u0026#39;,\r\u0026#39;sandal\u0026#39;, \u0026#39;shirt\u0026#39;, \u0026#39;sneaker\u0026#39;, \u0026#39;bag\u0026#39;, \u0026#39;ankle boot\u0026#39;]\rreturn [text_labels[int(i)] for i in labels] Plt 可视化样本 #\rdef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save\r\u0026#34;\u0026#34;\u0026#34;绘制图像列表\u0026#34;\u0026#34;\u0026#34;\rfigsize = (num_cols * scale, num_rows * scale)\r_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\raxes = axes.flatten()\rfor i, (ax, img) in enumerate(zip(axes, imgs)):\rif torch.is_tensor(img):\r# 图片张量\rax.imshow(img.numpy())\relse:\r# PIL图片\rax.imshow(img)\rax.axes.get_xaxis().set_visible(False)\rax.axes.get_yaxis().set_visible(False)\rif titles:\rax.set_title(titles[i])\rreturn axes\rX, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\rshow_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y)); ![[Pasted image 20240331164614.png]]\n3.5.2 读取小批量 #\r为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取小批量。 batch_size = 256\rdef get_dataloader_workers(): #@save\r\u0026#34;\u0026#34;\u0026#34;使用4个进程来读取数据\u0026#34;\u0026#34;\u0026#34;\rreturn 4\rtrain_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\rnum_workers=get_dataloader_workers()) 3.5.3. 整合所有组件 #\r现在我们定义load_data_fashion_mnist函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数resize，用来将图像大小调整为另一种形状 def load_data_fashion_mnist(batch_size, resize=None): #@save\r\u0026#34;\u0026#34;\u0026#34;下载Fashion-MNIST数据集，然后将其加载到内存中\u0026#34;\u0026#34;\u0026#34;\rtrans = [transforms.ToTensor()]\rif resize:\rtrans.insert(0, transforms.Resize(resize))\rtrans = transforms.Compose(trans)\rmnist_train = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=True, transform=trans, download=True)\rmnist_test = torchvision.datasets.FashionMNIST(\rroot=\u0026#34;../data\u0026#34;, train=False, transform=trans, download=True)\rreturn (data.DataLoader(mnist_train, batch_size, shuffle=True,\rnum_workers=get_dataloader_workers()),\rdata.DataLoader(mnist_test, batch_size, shuffle=False,\rnum_workers=get_dataloader_workers())) 下面，我们通过指定resize参数来测试load_data_fashion_mnist函数的图像大小调整功能。 train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\rfor X, y in train_iter:\rprint(X.shape, X.dtype, y.shape, y.dtype)\rbreak 我们现在已经准备好使用Fashion-MNIST数据集，便于下面的章节调用来评估各种分类算法 ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/","section":"Docs","summary":"\u003cp\u003eMNIST数据集 (\u003ca href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90\"title=\"LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., \u0026amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\" target=\"_blank\"\u003eLeCun \u003cem\u003eet al.\u003c/em\u003e, 1998\u003c/a\u003e) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (\u003ca href=\"https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189\"title=\"Xiao, H., Rasul, K., \u0026amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.\" target=\"_blank\"\u003eXiao \u003cem\u003eet al.\u003c/em\u003e, 2017\u003c/a\u003e)。\u003c/p\u003e","title":"D2L 3.5 Image classification datasets","type":"docs"},{"content":"\r3.6.1 初始化模型参数 #\r和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量 在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10 初始化权重w #\r与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0 num_inputs = 784\rnum_outputs = 10\rW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\rb = torch.zeros(num_outputs, requires_grad=True) 3.6.2 定义softmax操作 #\r实现softmax操作由三个步骤组成 对每个项求幂 对每一行求和，得到其规范化常数 每一行除以其规范化常数，保持结果的和为1 $$softmax(X){ij}=\\frac{exp(X{ij})}{\\sum_kexp(X_{ik})}$$ 分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自统计物理学中一个模拟粒子群分布的方程 def softmax(X):\rX_exp = torch.exp(X)\rpartition = X_exp.sum(1, keepdim=True)\rreturn X_exp / partition # 这里应用了广播机制 keepdim=True: 在进行张量操作时，保持原始张量的维度\ntorch.normal(0, 1, (2, 5)) 是用 PyTorch 生成一个服从均值为 0，标准差为 1 的正态分布的张量。\n其中的 (2, 5) 是指生成的张量的形状为 2 行 5 列的矩阵\n3.6.3 定义模型 #\r定义softmax操作后，我们可以实现softmax回归模型 def net(X):\rreturn softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) 3.6.4 定义损失函数 #\r引入[[Cross-Entropy 交叉熵]]损失函数 深度学习中，交叉熵函数最为常见，因为分类问题的数量远远超过了回归问题 y = torch.tensor([0, 2])\ry_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ry_hat[[0, 1], y] y_hat： 包含2个样本在3个类别的预测概率 y：真实类，0代表第一类，1代表第二类，2代表第三类 [[0,1],y]：一种tensor的高级引索功能，其选择了y_hat中的第一行和第二行 而y给予了列的位置，所以输出分别为第一行第0位和第二行第2位 3.6.5 分类精度 #\r给定预测概率分布\\(\\hat y\\)，当我们必须输出Hard Prediciton 硬预测时，我们通常选择概率最高的类 当预测和标签分类y一致时，即是正确的 分类精度指的就是正确预测数量与总预测数量之比 def accuracy(y_hat, y): #@save\r\u0026#34;\u0026#34;\u0026#34;计算预测正确的数量\u0026#34;\u0026#34;\u0026#34;\rif len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1:\ry_hat = y_hat.argmax(axis=1)\rcmp = y_hat.type(y.dtype) == y\rreturn float(cmp.type(y.dtype).sum()) 扩展到任意数据迭代器data_iter可访问的数据集 #\rdef evaluate_accuracy(net, data_iter): #@save\r\u0026#34;\u0026#34;\u0026#34;计算在指定数据集上模型的精度\u0026#34;\u0026#34;\u0026#34;\rif isinstance(net, torch.nn.Module):\rnet.eval() # 将模型设置为评估模式\rmetric = Accumulator(2) # 正确预测数、预测总数, Accmulator在下面定义\rwith torch.no_grad():\rfor X, y in data_iter:\rmetric.add(accuracy(net(X), y), y.numel())\rreturn metric[0] / metric[1] 首先，如果 net 是 torch.nn.Module 的子类，就将模型设置为评估模式，即调用 net.eval()。在评估模式下，模型的行为可能会略有不同，比如 Dropout 层在评估模式下会关闭，以避免随机丢弃部分节点 创建了一个名为 metric 的累加器（Accumulator）。这个累加器用于记录正确预测数和总预测数，初始化为两个元素的列表 [0, 0] Accumulator：这个类在下面定义 使用 torch.no_grad() 上下文管理器，禁用梯度计算 最后就是将评估结果添加至metric中 Accumulator类 #\r这里定义一个实用程序类Accumulator，用于对多个变量进行累加。 在上面的evaluate_accuracy函数中， 我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。 class Accumulator: #@save\r\u0026#34;\u0026#34;\u0026#34;在n个变量上累加\u0026#34;\u0026#34;\u0026#34;\rdef __init__(self, n):\rself.data = [0.0] * n\rdef add(self, *args):\rself.data = [a + float(b) for a, b in zip(self.data, args)]\rdef reset(self):\rself.data = [0.0] * len(self.data)\rdef __getitem__(self, idx):\rreturn self.data[idx] __init__(self, n): 这是类的构造函数，用于初始化累加器。它接受一个参数 n，表示要累加的变量的数量。在初始化时，创建了一个包含 n 个元素的列表，每个元素初始化为 0.0 add(self, *args): 这个方法用于将参数 args 中的值与累加器中的值相加。参数 args 是一个可变参数，可以接受任意数量的参数。通过 zip 函数，将 args 中的值逐个与累加器中对应位置的值相加，并更新累加器中的值 reset(self): 这个方法用于重置累加器的值 __getitem__(self, idx): 这个方法允许通过索引访问累加器中的值。给定一个索引 idx，它返回累加器中对应位置的值 3.6.6 训练 #\r首先，我们定义一个函数来训练一个迭代周期 updater是更新模型参数的常用函数，它接受批量大小作为参数 def train_epoch_ch3(net, train_iter, loss, updater): #@save\r\u0026#34;\u0026#34;\u0026#34;训练模型一个迭代周期（定义见第3章）\u0026#34;\u0026#34;\u0026#34;\r# 将模型设置为训练模式\rif isinstance(net, torch.nn.Module):\rnet.train()\r# 训练损失总和、训练准确度总和、样本数\rmetric = Accumulator(3)\rfor X, y in train_iter:\r# 计算梯度并更新参数\ry_hat = net(X)\rl = loss(y_hat, y)\rif isinstance(updater, torch.optim.Optimizer):\r# 使用PyTorch内置的优化器和损失函数\rupdater.zero_grad()\rl.mean().backward()\rupdater.step()\relse:\r# 使用定制的优化器和损失函数\rl.sum().backward()\rupdater(X.shape[0])\rmetric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\r# 返回训练损失和训练精度\rreturn metric[0] / metric[2], metric[1] / metric[2] if isinstance(net, torch.nn.Module):：检查变量 net 是否是 torch.nn.Module 类的实例 net.train(): 这一行将模型（net）设置为训练模式 metric = Accumulator(3): 创建一个长度为3的累加器 在计算梯度后，根据数据类型，如pytorch类或者自定义类累加处理结果 训练函数 #\rdef train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save\r\u0026#34;\u0026#34;\u0026#34;训练模型（定义见第3章）\u0026#34;\u0026#34;\u0026#34;\ranimator = Animator(xlabel=\u0026#39;epoch\u0026#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9],\rlegend=[\u0026#39;train loss\u0026#39;, \u0026#39;train acc\u0026#39;, \u0026#39;test acc\u0026#39;])\rfor epoch in range(num_epochs):\rtrain_metrics = train_epoch_ch3(net, train_iter, loss, updater)\rtest_acc = evaluate_accuracy(net, test_iter)\ranimator.add(epoch + 1, train_metrics + (test_acc,))\rtrain_loss, train_acc = train_metrics\rassert train_loss \u0026lt; 0.5, train_loss\rassert train_acc \u0026lt;= 1 and train_acc \u0026gt; 0.7, train_acc\rassert test_acc \u0026lt;= 1 and test_acc \u0026gt; 0.7, test_acc ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/","section":"Docs","summary":"\u003ch2 class=\"relative group\"\u003e3.6.1 初始化模型参数 \r\n    \u003cdiv id=\"361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量\u003c/li\u003e\n\u003cli\u003e在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10\u003c/li\u003e\n\u003c/ul\u003e\n\r\n\r\n\u003ch3 class=\"relative group\"\u003e初始化权重w \r\n    \u003cdiv id=\"%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h3\u003e\r\n\u003cul\u003e\n\u003cli\u003e与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enum_inputs = 784\r\nnum_outputs = 10\r\n\r\nW = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\r\nb = torch.zeros(num_outputs, requires_grad=True)\n\u003c/code\u003e\u003c/pre\u003e\r\n\r\n\u003ch2 class=\"relative group\"\u003e3.6.2 定义softmax操作 \r\n    \u003cdiv id=\"362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e实现softmax操作由三个步骤组成\u003c/li\u003e\n\u003cli\u003e对每个项求幂\u003c/li\u003e\n\u003cli\u003e对每一行求和，得到其规范化常数\u003c/li\u003e\n\u003cli\u003e每一行除以其规范化常数，保持结果的和为1\n$$softmax(X)\u003cem\u003e{ij}=\\frac{exp(X\u003c/em\u003e{ij})}{\\sum_kexp(X_{ik})}$$\u003c/li\u003e\n\u003cli\u003e分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自\u003ca href=\"https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29\" target=\"_blank\"\u003e统计物理学\u003c/a\u003e中一个模拟粒子群分布的方程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edef softmax(X):\r\n    X_exp = torch.exp(X)\r\n    partition = X_exp.sum(1, keepdim=True)\r\n    return X_exp / partition  # 这里应用了广播机制\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ekeepdim=True: 在进行张量操作时，保持原始张量的维度\u003c/p\u003e","title":"D2L 3.6 Implementation of softmax regression from scratch","type":"docs"},{"content":"在了解多层感知机前，需要先了解[[Perceptron 感知机]]\n4.1.1 隐藏层 #\r在[[3.1_LinearRegression#3.1.1.1 线性模型]]中我们描述了[[Affine Transformation 仿射变换]]，如一次函数一般是一种带有偏置项的线性变换 如果预测值在仿射变换后确实与输入数据有线性关系，那么这种方式确实够用 可是大部分情况下，仿射变换中的线性是一个很强的假设 4.1.1.1 线性模型可能会出错 #\r线性意味着单调假设，权重w在正的情况下，任何特征的增大都会导致模型输出的增大 E.X.\n如果我们试图预测一个人是否会偿还贷款，我们可以认为收入较高的申请人比收入较低的申请人更有可能偿还贷款 但上述例子只阐明了单调性而非线性 收入从0到5万会带来比100万到105万更大的还款可能性 在上例中，我们任然可以通过[[2.2 数据预处理]]的方式使线性更加合理，如对数化处理 但一个违反单调性的例子比如体温和死亡率的关系 对于体温高于37度的人来说，温度越高风险越高 而对于体温低于37度的人来说，温度越低风险就越低 这种情况也可以使用理37度的距离作为特征 分类问题，如对于猫狗分类问题，在位置（13，17）处像素强度进行添加，是否整个图像描绘狗的[[Likehood 似然]]会增加？ 这一评估标准注定会失败，如倒置图像后，类别依然保留 对于上面两个例子来说，猫狗的分类问题无法通过简单的预处理解决 对于[[深度神经网络]]，我们将使用隐藏层 4.1.1.2 在网络中加入隐藏层 #\r我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型 有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡\n4.1.1.3 从线性到非线性 #\r同之前的章节一样，我们通过矩阵\\(X\\in R^{n\\times d}\\)来表示n个样本的小批量，其中每个样本具有d个输入特征\n对于具有h个隐藏单元的单隐藏层多层感知机，用\\(H\\in R^{n\\times h}\\)表示隐藏层的输出，称为Hidden Representatiosn 隐藏表示\n因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重\\(W^{(1)}\\in R^{R\\times h}\\)和隐藏层偏置\\(b^{(1)}\\in R^{1\\times h}\\)以及输出层\\(W^{(2)}\\in R^{h\\times q}\\)和输出层偏置\\(b^{(2)}\\in R^{1\\times q}\\)\n所以形式上，对于单隐藏层的多层感知机的输出\\(O\\in R^{n\\times q}\\)，有 $$\\begin{align} \\ H=WX^{(1)}+b^{(1)} \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n现阶段，隐藏层为输入层的放射函数，而输出层为隐藏层的放射函数，即$$O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW+b$$\n注意到现在在多层感知机的单隐藏层下，模型依然只做到了线性的放射函数\n所以为了发挥多层架构的潜力，我们需要添加一个额外的关键要素：[[Activation Function 激活函数]]，激活函数的输出则称为Activations 活性值\n一般来说，有了激活函数，模型就不会退化成线性模型 $$\\begin{align} \\ H=\\sigma(XW^{(1)}+b^{(1)}) \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，从而产生更有表达能力的模型\n4.1.1.4 通用近似定理 #\r多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的 而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论 4.1.2 激活函数 Activation Function #\r[[Activation Function 激活函数]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/","section":"Docs","summary":"\u003cp\u003e在了解多层感知机前，需要先了解[[Perceptron 感知机]]\u003c/p\u003e","title":"D2L 4.1 MultilayerPerceptron","type":"docs"},{"content":"在了解多层感知机前，需要先了解[[Perceptron 感知机]]\n4.1.1 隐藏层 #\r在[[3.1_LinearRegression#3.1.1.1 线性模型]]中我们描述了[[Affine Transformation 仿射变换]]，如一次函数一般是一种带有偏置项的线性变换 如果预测值在仿射变换后确实与输入数据有线性关系，那么这种方式确实够用 可是大部分情况下，仿射变换中的线性是一个很强的假设 4.1.1.1 线性模型可能会出错 #\r线性意味着单调假设，权重w在正的情况下，任何特征的增大都会导致模型输出的增大 E.X.\n如果我们试图预测一个人是否会偿还贷款，我们可以认为收入较高的申请人比收入较低的申请人更有可能偿还贷款 但上述例子只阐明了单调性而非线性 收入从0到5万会带来比100万到105万更大的还款可能性 在上例中，我们任然可以通过[[2.2 数据预处理]]的方式使线性更加合理，如对数化处理 但一个违反单调性的例子比如体温和死亡率的关系 对于体温高于37度的人来说，温度越高风险越高 而对于体温低于37度的人来说，温度越低风险就越低 这种情况也可以使用理37度的距离作为特征 分类问题，如对于猫狗分类问题，在位置（13，17）处像素强度进行添加，是否整个图像描绘狗的[[Likehood 似然]]会增加？ 这一评估标准注定会失败，如倒置图像后，类别依然保留 对于上面两个例子来说，猫狗的分类问题无法通过简单的预处理解决 对于[[深度神经网络]]，我们将使用隐藏层 4.1.1.2 在网络中加入隐藏层 #\r我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型 有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡\n4.1.1.3 从线性到非线性 #\r同之前的章节一样，我们通过矩阵\\(X\\in R^{n\\times d}\\)来表示n个样本的小批量，其中每个样本具有d个输入特征\n对于具有h个隐藏单元的单隐藏层多层感知机，用\\(H\\in R^{n\\times h}\\)表示隐藏层的输出，称为Hidden Representatiosn 隐藏表示\n因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重\\(W^{(1)}\\in R^{R\\times h}\\)和隐藏层偏置\\(b^{(1)}\\in R^{1\\times h}\\)以及输出层\\(W^{(2)}\\in R^{h\\times q}\\)和输出层偏置\\(b^{(2)}\\in R^{1\\times q}\\)\n所以形式上，对于单隐藏层的多层感知机的输出\\(O\\in R^{n\\times q}\\)，有 $$\\begin{align} \\ H=WX^{(1)}+b^{(1)} \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n现阶段，隐藏层为输入层的放射函数，而输出层为隐藏层的放射函数，即$$O=(XW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}=XW+b$$\n注意到现在在多层感知机的单隐藏层下，模型依然只做到了线性的放射函数\n所以为了发挥多层架构的潜力，我们需要添加一个额外的关键要素：[[Activation Function 激活函数]]，激活函数的输出则称为Activations 活性值\n一般来说，有了激活函数，模型就不会退化成线性模型 $$\\begin{align} \\ H=\\sigma(XW^{(1)}+b^{(1)}) \\ O=HW^{(2)}+b^{(2)} \\end{align}$$\n为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，从而产生更有表达能力的模型\n4.1.1.4 通用近似定理 #\r多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 我们可以很容易地设计隐藏节点来执行任意计算 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的 而且，虽然一个单隐层网络能学习任何函数， 但并不意味着我们应该尝试使用单隐藏层网络来解决所有问题。 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。 我们将在后面的章节中进行更细致的讨论 4.1.2 激活函数 Activation Function #\r[[Activation Function 激活函数]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/","section":"Docs","summary":"\u003cp\u003e在了解多层感知机前，需要先了解[[Perceptron 感知机]]\u003c/p\u003e","title":"D2L 4.1 MultilayerPerceptron","type":"docs"},{"content":"通过更高级的API进一步简洁训练过程\n4.3.1 模型 #\rnet = nn.Sequential(nn.Flatten(),\rnn.Linear(784, 256),\rnn.ReLU(),\rnn.Linear(256, 10))\rdef init_weights(m):\rif type(m) == nn.Linear:\rnn.init.normal_(m.weight, std=0.01)\rnet.apply(init_weights); 初始化神经网络 batch_size, lr, num_epochs = 256, 0.1, 10\rloss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;)\rtrainer = torch.optim.SGD(net.parameters(), lr=lr)\rtrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\rd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/","section":"Docs","summary":"\u003cp\u003e通过更高级的API进一步简洁训练过程\u003c/p\u003e\n\r\n\r\n\u003ch1 class=\"relative group\"\u003e4.3.1 模型 \r\n    \u003cdiv id=\"431-%E6%A8%A1%E5%9E%8B\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#431-%E6%A8%A1%E5%9E%8B\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h1\u003e\r\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enet = nn.Sequential(nn.Flatten(),\r\n                    nn.Linear(784, 256),\r\n                    nn.ReLU(),\r\n                    nn.Linear(256, 10))\r\n\r\ndef init_weights(m):\r\n    if type(m) == nn.Linear:\r\n        nn.init.normal_(m.weight, std=0.01)\r\n\r\nnet.apply(init_weights);\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e初始化神经网络\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ebatch_size, lr, num_epochs = 256, 0.1, 10\r\nloss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;)\r\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\r\n\r\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\r\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n\u003c/code\u003e\u003c/pre\u003e","title":"D2L 4.3 Simple Implementation of Multilayer Perceptron","type":"docs"},{"content":"深度学习的目的是发现Pattern，即做到模型的Generalization 泛化\n[[Overfitting Problem]] 原因很简单：当我们将来部署该模型时，模型需要判断从未见过的患者。 只有当模型真正发现了一种泛化模式时，才会作出有效的预测\n困难在于，当我们训练模型时，我们只能访问数据中的小部分样本。 最大的公开图像数据集包含大约一百万张图像。 而在大部分时候，我们只能从数千或数万个数据样本中学习。 在大型医院系统中，我们可能会访问数十万份医疗记录。 当我们使用有限的样本时，可能会遇到这样的问题： 当收集到更多的数据时，会发现之前找到的明显关系并不成立。\nOverfitting 过拟合 #\r模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合 左： Underfitting 欠拟合 中：拟合 右：Overfitting 过拟合 Regularization 正则化 #\r对抗过拟合的技术称为正则化 [[Regularization 正则化]] 4.4.1 训练误差和泛化误差 #\rTraining Error 训练误差 #\r模型在训练数据集上计算得到的误差 Generalization Error 泛化误差 #\r同样分布样本的无限多个数据的模型误差期望 但问题是对于无限多个数据，我们不可能准确的计算出Genrelization Errorz 4.4.1.1 统计学习理论 #\r我们假设训练数据和测试数据都是从相同的分布中独立提取的。 这通常被称为_独立同分布假设_（i.i.d. assumption） 4.4.1.2 模型复杂性 #\r一个模型的复杂性取决于很多因素 如模型参数，取值范围 Early Stopping 早停 #\r早停（Early Stopping）：这是一种防止过拟合的技术，其中训练过程在验证集上的性能开始恶化时停止。这意味着，如果模型在验证集上的误差开始增加，表明模型可能开始过拟合训练数据，此时停止进一步训练可以避免这种情况。 4.4.2 模型选择 #\r在一个训练中，我们会选择几个候选模型对他们进行评估 4.4.2.1 验证集 #\r训练集，验证集，测试集分别是什么_训练集 验证集 测试集-CSDN博客\n总的来说，对于Superivised Training，一般讲整体划为3个区块 Training Set 训练集 #\r训练集用来训练模型，即确定模型的权重和偏置这些参数，通常我们称这些参数为学习参数 训练集中的参数直接参与到梯度下降中 Validation Set 验证集 #\rz\n而验证集用于模型的选择，更具体地来说，验证集并不参与学习参数的确定，也就是验证集并没有参与梯度下降的过程 验证集只是为了选择超参数，比如网络层数、网络节点数、迭代次数、学习率这些都叫超参数 Test Set 测试集 #\r测试集只使用一次，即在训练完成后评价最终的模型时使用。它既不参与学习参数过程，也不参数超参数选择过程，而仅仅使用于模型的评价 4.4.2.2 K-Fold Cross-Validation K折交叉验证 #\r训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集 过程描述 #\r数据分组：首先，整个数据集被随机分成K个大小大致相同的子集。 迭代训练与验证：每次迭代中，选择其中一个子集作为验证集，而其余的K-1个子集合并作为训练集。 性能评估：模型在训练集上训练，并在验证集上进行评估。这个过程重复K次，每次选择不同的子集作为验证集。 平均性能：最终模型的性能是所有K次迭代中验证性能的平均值。这样可以更全面地评估模型的性能。 4.4.3 欠拟合还是过拟合？ #\rGenerlization Error高的模型叫做Underfitting Train Error远低于Validation Error的模型叫做Overfitting 4.4.3.1 模型复杂性 #\r![[Pasted image 20240615153938.png]]\n简单来说，从左到右模型经历了从欠拟合到过拟合的一个过程，也是从高损失到高方差的过程 其是因为模型从没学习过参数到对于微小参数（甚至是随机噪声）严重敏感的一个过程 Lost 损失 #\r定义：偏差是指模型在预测中的系统误差，即模型对学习数据的一般性质的理解程度。 高偏差：通常表示模型过于简单（欠拟合），未能捕捉到数据的关键结构，通常会导致在训练集和测试集上都表现不佳。 Variance 方差 #\r定义：方差是指模型对于训练数据的微小变化的敏感度。 高方差：表示模型过于复杂（过拟合），对训练数据中的随机噪声也进行了学习，这可能使得模型在新的、未见过的数据上表现不佳。 4.4.3.2 数据集大小 #\r训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合 而样本更过通常会减小Gerneralization Error 一般来说，更多的数据不会有什么坏处 4.4.4 多项式回归 #\r拟合一个多项式\n[[4.4 Overfitting Normal \u0026amp; Underfitting - Pytorch]]\n4.4.4.1 生成数据集 #\r![[Pasted image 20240616094316.png]]\n噪声值位均值0到标准差0.1的正态分布 在优化的过程中，我们通常希望避免非常大的梯度值或损失值。 这就是我们将特征从$x^i$调整为$\\frac{x^i}{i!}$的原因 max_degree = 20 # 多项式的最大阶数\rn_train, n_test = 100, 100 # 训练和测试数据集大小\rtrue_w = np.zeros(max_degree) # 分配大量的空间\rtrue_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\rfeatures = np.random.normal(size=(n_train + n_test, 1))\rnp.random.shuffle(features)\rpoly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\rfor i in range(max_degree):\rpoly_features[:, i] /= math.gamma(i + 1) # gamma(n)=(n-1)!\r# labels的维度:(n_train+n_test,)\rlabels = np.dot(poly_features, true_w)\rlabels += np.random.normal(scale=0.1, size=labels.shape) max_degree = 20 : 即使多项式仅为三阶，但我们需要用一个20纬的多项式去拟合它，这是复杂模型中的一种 features = np.random.normal(size=(n_train + n_test, 1)): 分配200个一维的特征 np.random.shuffle(features): 随机打乱数据 poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))：分配每个特征的高阶数据 使用伽马正则化防止特征的迅速增大 最后点乘特征和真实权重得到Label，并将Label加上合适的噪声 # NumPy ndarray转换为tensor\rtrue_w, features, poly_features, labels = [torch.tensor(x, dtype=\rtorch.float32) for x in [true_w, features, poly_features, labels]]\rfeatures[:2], poly_features[:2, :], labels[:2] 转化为tensor def evaluate_loss(net, data_iter, loss): #@save\r\u0026#34;\u0026#34;\u0026#34;评估给定数据集上模型的损失\u0026#34;\u0026#34;\u0026#34;\rmetric = d2l.Accumulator(2) # 损失的总和,样本数量\rfor X, y in data_iter:\rout = net(X)\ry = y.reshape(out.shape)\rl = loss(out, y)\rmetric.add(l.sum(), l.numel())\rreturn metric[0] / metric[1] def train(train_features, test_features, train_labels, test_labels,\rnum_epochs=400):\rloss = nn.MSELoss(reduction=\u0026#39;none\u0026#39;)\rinput_shape = train_features.shape[-1]\r# 不设置偏置，因为我们已经在多项式中实现了它\rnet = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\rbatch_size = min(10, train_labels.shape[0])\rtrain_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),\rbatch_size)\rtest_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),\rbatch_size, is_train=False)\rtrainer = torch.optim.SGD(net.parameters(), lr=0.01)\ranimator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;loss\u0026#39;, yscale=\u0026#39;log\u0026#39;,\rxlim=[1, num_epochs], ylim=[1e-3, 1e2],\rlegend=[\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;])\rfor epoch in range(num_epochs):\rd2l.train_epoch_ch3(net, train_iter, loss, trainer)\rif epoch == 0 or (epoch + 1) % 20 == 0:\ranimator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),\revaluate_loss(net, test_iter, loss)))\rprint(\u0026#39;weight:\u0026#39;, net[0].weight.data.numpy() 欠拟合 #\r# 从多项式特征中选择前2个维度，即1和x\rtrain(poly_features[:n_train, :2], poly_features[n_train:, :2],\rlabels[:n_train], labels[n_train:]) 只给予了前两个特征值 ![[Pasted image 20240704160340.png]] 过拟合 #\r# 从多项式特征中选取所有维度\rtrain(poly_features[:n_train, :], poly_features[n_train:, :],\rlabels[:n_train], labels[n_train:], num_epochs=1500) 将w中的20列全部给到了模型导致了过拟合 ![[Pasted image 20240704160346.png]] ","date":"Apr 15 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/","section":"Docs","summary":"\u003cp\u003e深度学习的目的是发现Pattern，即做到模型的Generalization 泛化\u003c/p\u003e","title":"D2L 4.4 Model Selection, Underfitting, and Overfitting","type":"docs"},{"content":" ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/","section":"Docs","summary":"\u003chr\u003e","title":"Chapter 3. Linear Neural Network","type":"docs"},{"content":" ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/","section":"Docs","summary":"\u003chr\u003e","title":"Chapter 4. Multilayer Perceptron","type":"docs"},{"content":"《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. https://zh-v2.d2l.ai/index.html\n‌ #\r","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/","section":"Docs","summary":"\u003cp\u003e《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. \u003ca href=\"https://zh-v2.d2l.ai/index.html\" target=\"_blank\"\u003ehttps://zh-v2.d2l.ai/index.html\u003c/a\u003e\u003c/p\u003e\n\r\n\r\n\u003ch2 class=\"relative group\"\u003e‌ \r\n    \u003cdiv id=\"heading\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\r\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\r\n            style=\"text-decoration-line: none !important;\" href=\"#heading\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e        \r\n    \r\n\u003c/h2\u003e","title":"Dive Into Deep Learning","type":"docs"},{"content":" Your browser does not support the video tag. Full Code is Provided\nimport numpy as np import torch import random class LinearRegression(Scene): def construct(self): def data_generator(w,b,num): X = torch.normal(0, 1, (num, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2,-3.4]) true_b = 4.2 features, labels = data_generator(true_w,true_b,1000) normal_data = features[:,[0]].numpy() #plt.hist(normal_data,bins=100,density=True,color=\u0026#39;lightblue\u0026#39;) head = Text(\u0026#34;Linear Regression - Buezwqwg\u0026#34;) head.set_color(BLUE) self.play(Create(head)) head_0 = Text(\u0026#34;In one process of Linear Regression, there bascially includes 5 steps\u0026#34;,font_size=30) self.play(Uncreate(head),Write(head_0)) self.play(head_0.animate.move_to(UP*3.5)) head_1 = Text(\u0026#34;1. Initial Parameters\u0026#34;,font_size=30) head_2 = Text(\u0026#34;2. Defining Model and Loss Function\u0026#34;,font_size=30) head_3 = Text(\u0026#34;3. Optimization\u0026#34;,font_size=30) head_4 = Text(\u0026#34;4. Loop\u0026#34;,font_size=30) head = VGroup(head_1,head_2,head_3,head_4) head.arrange(DOWN) self.play(Write(head)) # -------------------------------------------------------------------------------------------- head_5 = Text(\u0026#34;In this animate, we start with generating the data\u0026#34;,font_size=30) head_5.move_to(UP*3.5) self.play(Uncreate(head),Uncreate(head_0),Write(head_5)) code_text = \u0026#39;\u0026#39;\u0026#39; def data_generator(w, b, num): X = torch.normal(0, 1, (num, len(w))) y = torch.matmul(X, w) + b y += torch.normal(0, 0.01, y.shape) return X, y.reshape((-1, 1)) true_w = torch.tensor([2,-3.4]) true_b = 4.2 features, labels = data_generator(true_w,true_b,1000) \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) self.play(Write(code),Uncreate(head_5),run_time=3) self.wait(3) self.play(Unwrite(code)) axes = Axes( x_range=[-4, 4, 1], y_range=[0, 0.5, 0.1], axis_config={\u0026#34;color\u0026#34;: BLUE}, ).add_coordinates() # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2) normal_curve = axes.plot( lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2), color=YELLOW ) # 绘制均值为0的竖线 mean_line = DashedLine( start=axes.c2p(0, 0), end=axes.c2p(0, (1 / (2 * PI) ** 0.5)), color=RED ) # 添加图形和标注 self.play(Create(axes)) self.play(Create(normal_curve), Create(mean_line)) # 标注均值和标准差 mean_label = MathTex(r\u0026#34;\\mu=0\u0026#34;).next_to(mean_line, DOWN) std_label = MathTex(r\u0026#34;\\sigma=1\u0026#34;).next_to(normal_curve, UP, buff=0.5) self.play(Write(mean_label), Write(std_label)) # 展示最终效果 self.wait(2) self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line)) # -------------------------------------------------------------------------------------------- head = Text(\u0026#34;Displaying the distribution of features\u0026#34;) feature_one = features[:,[0]].tolist() feature_two = features[:,[1]].tolist() labels = labels.tolist() axes_1 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=5, # x轴的长度 y_length=5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_2 = Axes( x_range=[min(feature_two)[0], max(feature_two)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=5, # x轴的长度 y_length=5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes = VGroup(axes_1,axes_2) axes.arrange(RIGHT,buff=1) self.play(Create(axes)) points_1 = [] for i in range(len(labels)): dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0]) points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_1 = [Create(dot) for dot in points_1] points_2 = [] for i in range(len(labels)): dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0]) points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_2 = [Create(dot) for dot in points_2] self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005)) # 抽取样本-------------------------------------------------------------------------------------------- head = Text(\u0026#39;Shuffle the data and divided into samples(batches)\u0026#39;,font_size=30) self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2))) head.move_to(UP*3.5) code_text = \u0026#39;\u0026#39;\u0026#39; def data_iter(batch_size,features,labels): num = len(features) index = list(range(num)) random.shuffle(index) for i in range(0,num,batch_size): batch_index = torch.tensor(index[i:min(i+batch_size,num)]) yield features[batch_index], labels[batch_index] \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) self.play(Write(code)) self.wait(2) self.play(Uncreate(code),Unwrite(head)) def data_iter(batch_size,features,labels): num = len(features) index = list(range(num)) random.shuffle(index) for i in range(0,num,batch_size): batch_index = torch.tensor(index[i:min(i+batch_size,num)]) return batch_index.tolist() axes_1 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_2 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_3 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_4 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes_5 = Axes( x_range=[min(feature_one)[0], max(feature_one)[0], 1], # x轴范围：从-5到5，步长为1 y_range=[min(labels)[0], max(labels)[0], 5], # y轴范围：从-3到3，步长为1 x_length=2.5, # x轴的长度 y_length=2.5, # y轴的长度 axis_config={\u0026#34;color\u0026#34;: BLUE}, # 坐标轴的颜色 ) axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5) axes.arrange(RIGHT) sample_1 = data_iter(10,features,labels) points_1 = [] for i in sample_1: dot_position = axes_1.coords_to_point(features[i][0],labels[i][0]) points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_1 = [Create(dot) for dot in points_1] sample_2 = data_iter(10,features,labels) points_2 = [] for i in sample_2: dot_position = axes_2.coords_to_point(features[i][0],labels[i][0]) points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_2 = [Create(dot) for dot in points_2] sample_3 = data_iter(10,features,labels) points_3 = [] for i in sample_3: dot_position = axes_3.coords_to_point(features[i][0],labels[i][0]) points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_3 = [Create(dot) for dot in points_3] sample_4 = data_iter(10,features,labels) points_4 = [] for i in sample_4: dot_position = axes_4.coords_to_point(features[i][0],labels[i][0]) points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_4 = [Create(dot) for dot in points_4] sample_5 = data_iter(10,features,labels) points_5 = [] for i in sample_5: dot_position = axes_5.coords_to_point(features[i][0],labels[i][0]) points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW)) animations_5 = [Create(dot) for dot in points_5] head = Text(\u0026#34;Display five of Sample Batches (Batch Size = 10)\u0026#34;,font_size=30) head.set_color(BLUE) head.move_to(UP*2.5) self.play(Write(head)) self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05)) self.wait(3) self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5))) # 定义模型-------------------------------------------------------------------------------------------- head_1 = Text(\u0026#39;Define the Function\u0026#39;) head_1.set_color(BLUE) code_text_1 = \u0026#39;\u0026#39;\u0026#39; def linreg(X, w, b): return torch.matmul(X, w) + b \u0026#39;\u0026#39;\u0026#39; code_1 = Code(code=code_text_1,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head_2 = Text(\u0026#39;Define the Loss Function\u0026#39;) head_2.set_color(BLUE) code_text_2 = \u0026#39;\u0026#39;\u0026#39; def squared_loss(y_hat, y): return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 \u0026#39;\u0026#39;\u0026#39; code_2 = Code(code=code_text_2,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head = VGroup(head_1,code_1,head_2,code_2) head.arrange(DOWN,buff=1) self.play(Write(head)) self.wait(2) self.play(Unwrite(head)) # 展示MSE-------------------------------------------------------------------------------------------- head = MathTex(r\u0026#34;Lose~Function~MSE~:(y_i - \\hat{y}_i)^2\u0026#34;) head.set_color(BLUE) head.move_to(UP*3) self.play(Write(head)) axes = Axes( x_range=[-10, 10, 2.5], y_range=[0, 100, 20], x_length=10, y_length=5, axis_config={\u0026#34;color\u0026#34;: GREEN}, ) # 定义MSE函数 mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10]) mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10]) # 将元素添加到场景中 self.play(Create(axes),Create(mse_curve)) self.wait(2) self.play(Uncreate(head)) head = Text(\u0026#34;The MSE Derivative indicates that loss will be increasing as it increase\u0026#34;,font_size=30) head.set_color(BLUE) head.move_to(UP*3) self.play(Write(head),Create(mse_der)) self.wait(3) self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes)) # 展示SGD-------------------------------------------------------------------------------------------- head = Text(\u0026#34;Now Conduct the Optimization Method\u0026#34;) head.move_to(UP*3) head.set_color(BLUE) code_text = \u0026#39;\u0026#39;\u0026#39; def sgd(params, lr, batch_size): with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_() \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head_2 = Text(\u0026#39;Apply this optimization method for each batch\u0026#39;) head_2.set_color(BLUE) sgd = MathTex(r\u0026#34;(w,b)\\leftarrow (w,b)-\\eta g\u0026#34;) main = VGroup(head,code,head_2,sgd) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(2) self.play(Uncreate(main),run_time=0.1) # 计算梯度-------------------------------------------------------------------------------------------- head = Text(\u0026#34;Now Calculate the Gradient\u0026#34;) head.set_color(BLUE) head.move_to(UP*3) grad = MathTex(r\u0026#34;\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w} \\left( y_i - (w x_i + b) \\right)^2\u0026#34;) grad_1 = MathTex(r\u0026#34;= \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - (wx_i + b)) \\cdot (-x_i)\u0026#34;) grad_2 = MathTex(r\u0026#34;= -\\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#34;) main = VGroup(head,grad,grad_1,grad_2) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(2) self.play(Uncreate(main),run_time=0.01) head = Text(\u0026#34;Then apllies the formula for 1000/10=100 Times\u0026#34;,font_size=45) head.set_color(BLUE) grad = MathTex(r\u0026#39;w := w + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#39;) grad_1 = MathTex(r\u0026#34;b := b + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i)\u0026#34;) main = VGroup(head,grad,grad_1) main.arrange(DOWN,buff=0.7) self.play(Write(main)) self.wait(3) self.play(Uncreate(main),run_time=0.01) # 总结-------------------------------------------------------------------------------------------- code_text = \u0026#39;\u0026#39;\u0026#39; lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) l.sum().backward() sgd([w, b], lr, batch_size) with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;) \u0026#39;\u0026#39;\u0026#39; code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;) head = Text(\u0026#34;Then applies the whole process in epochs and that\u0026#39;s linear regression\u0026#34;,font_size=30) main = VGroup(head,code) main.arrange(DOWN,buff=1) self.play(Write(main)) --- ","date":"Jan 23 2024","externalUrl":null,"permalink":"/docs/d2l_dive-into-deep-learning/linearregression/","section":"Docs","summary":"\u003cvideo width=\"640\" height=\"360\" controls\u003e\n  \u003csource src=\"LinearRegression.mp4\" type=\"video/mp4\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\u003cp\u003eFull Code is Provided\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-from\" data-lang=\"from\"\u003eimport numpy as np\nimport torch\nimport random\n\nclass LinearRegression(Scene):\n    def construct(self):\n        def data_generator(w,b,num):\n            X = torch.normal(0, 1, (num, len(w)))\n            y = torch.matmul(X, w) + b\n            y += torch.normal(0, 0.01, y.shape)\n            return X, y.reshape((-1, 1))\n\n        true_w = torch.tensor([2,-3.4])\n        true_b = 4.2\n        features, labels = data_generator(true_w,true_b,1000)\n        normal_data = features[:,[0]].numpy()\n        #plt.hist(normal_data,bins=100,density=True,color=\u0026#39;lightblue\u0026#39;)  \n         \n\n        head = Text(\u0026#34;Linear Regression - Buezwqwg\u0026#34;)\n        head.set_color(BLUE)\n        self.play(Create(head))\n\n        head_0 = Text(\u0026#34;In one process of Linear Regression, there bascially includes 5 steps\u0026#34;,font_size=30)\n        self.play(Uncreate(head),Write(head_0))\n        self.play(head_0.animate.move_to(UP*3.5))\n        head_1 = Text(\u0026#34;1. Initial Parameters\u0026#34;,font_size=30)\n        head_2 = Text(\u0026#34;2. Defining Model and Loss Function\u0026#34;,font_size=30)\n        head_3 = Text(\u0026#34;3. Optimization\u0026#34;,font_size=30)\n        head_4 = Text(\u0026#34;4. Loop\u0026#34;,font_size=30)\n        head = VGroup(head_1,head_2,head_3,head_4)\n        head.arrange(DOWN)\n        self.play(Write(head))\n\n        # --------------------------------------------------------------------------------------------\n\n        head_5 = Text(\u0026#34;In this animate, we start with generating the data\u0026#34;,font_size=30)\n        head_5.move_to(UP*3.5)\n        self.play(Uncreate(head),Uncreate(head_0),Write(head_5))\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def data_generator(w, b, num):\n            X = torch.normal(0, 1, (num, len(w)))\n            y = torch.matmul(X, w) + b\n            y += torch.normal(0, 0.01, y.shape)\n            return X, y.reshape((-1, 1))\n            \n        true_w = torch.tensor([2,-3.4])\n        true_b = 4.2\n        features, labels = data_generator(true_w,true_b,1000)\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        self.play(Write(code),Uncreate(head_5),run_time=3)\n        self.wait(3)\n        self.play(Unwrite(code))\n        axes = Axes(\n            x_range=[-4, 4, 1],\n            y_range=[0, 0.5, 0.1],\n            axis_config={\u0026#34;color\u0026#34;: BLUE},\n        ).add_coordinates()\n\n        # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2)\n        normal_curve = axes.plot(\n            lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2),\n            color=YELLOW\n        )\n\n        # 绘制均值为0的竖线\n        mean_line = DashedLine(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(0, (1 / (2 * PI) ** 0.5)),\n            color=RED\n        )\n\n        # 添加图形和标注\n        self.play(Create(axes))\n        self.play(Create(normal_curve), Create(mean_line))\n        \n        # 标注均值和标准差\n        mean_label = MathTex(r\u0026#34;\\mu=0\u0026#34;).next_to(mean_line, DOWN)\n        std_label = MathTex(r\u0026#34;\\sigma=1\u0026#34;).next_to(normal_curve, UP, buff=0.5)\n        self.play(Write(mean_label), Write(std_label))\n\n        # 展示最终效果\n        self.wait(2)\n        self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line))\n\n        # --------------------------------------------------------------------------------------------\n\n        head = Text(\u0026#34;Displaying the distribution of features\u0026#34;)\n        feature_one = features[:,[0]].tolist()\n        feature_two = features[:,[1]].tolist()\n        labels = labels.tolist()\n        axes_1 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=5,  # x轴的长度\n            y_length=5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_2 = Axes(\n            x_range=[min(feature_two)[0], max(feature_two)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=5,  # x轴的长度\n            y_length=5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes = VGroup(axes_1,axes_2)\n        axes.arrange(RIGHT,buff=1)\n        self.play(Create(axes))\n\n        points_1 = []\n        for i in range(len(labels)):\n            dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])\n            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_1 = [Create(dot) for dot in points_1]\n        points_2 = []\n        for i in range(len(labels)):\n            dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])\n            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_2 = [Create(dot) for dot in points_2]\n        self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005))\n\n        # 抽取样本-------------------------------------------------------------------------------------------- \n\n        head = Text(\u0026#39;Shuffle the data and divided into samples(batches)\u0026#39;,font_size=30)\n        self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)))\n        head.move_to(UP*3.5)\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def data_iter(batch_size,features,labels):\n            num = len(features)\n            index = list(range(num))\n            random.shuffle(index)\n            for i in range(0,num,batch_size):\n                batch_index = torch.tensor(index[i:min(i+batch_size,num)])\n                yield features[batch_index], labels[batch_index]\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        self.play(Write(code))\n        self.wait(2)\n        self.play(Uncreate(code),Unwrite(head))\n\n        def data_iter(batch_size,features,labels):\n            num = len(features)\n            index = list(range(num))\n            random.shuffle(index)\n            for i in range(0,num,batch_size):\n                batch_index = torch.tensor(index[i:min(i+batch_size,num)])\n                return batch_index.tolist()\n\n\n\n        axes_1 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        \n\n        axes_2 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        \n        axes_3 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_4 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes_5 = Axes(\n            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1\n            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1\n            x_length=2.5,  # x轴的长度\n            y_length=2.5,  # y轴的长度\n            axis_config={\u0026#34;color\u0026#34;: BLUE},  # 坐标轴的颜色\n        )\n        axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5)\n        axes.arrange(RIGHT)\n\n        sample_1 = data_iter(10,features,labels)\n        points_1 = []\n        for i in sample_1:\n            dot_position = axes_1.coords_to_point(features[i][0],labels[i][0])\n            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_1 = [Create(dot) for dot in points_1]\n\n        sample_2 = data_iter(10,features,labels)\n        points_2 = []\n        for i in sample_2:\n            dot_position = axes_2.coords_to_point(features[i][0],labels[i][0])\n            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_2 = [Create(dot) for dot in points_2]\n\n        sample_3 = data_iter(10,features,labels)\n        points_3 = []\n        for i in sample_3:\n            dot_position = axes_3.coords_to_point(features[i][0],labels[i][0])\n            points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_3 = [Create(dot) for dot in points_3]\n\n        sample_4 = data_iter(10,features,labels)\n        points_4 = []\n        for i in sample_4:\n            dot_position = axes_4.coords_to_point(features[i][0],labels[i][0])\n            points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_4 = [Create(dot) for dot in points_4]\n\n        sample_5 = data_iter(10,features,labels)\n        points_5 = []\n        for i in sample_5:\n            dot_position = axes_5.coords_to_point(features[i][0],labels[i][0])\n            points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW))\n        animations_5 = [Create(dot) for dot in points_5]   \n        head = Text(\u0026#34;Display five of Sample Batches (Batch Size = 10)\u0026#34;,font_size=30)\n        head.set_color(BLUE)\n        head.move_to(UP*2.5)\n        self.play(Write(head))\n        self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05))\n        self.wait(3)\n        self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5)))\n\n        # 定义模型-------------------------------------------------------------------------------------------- \n\n        head_1 = Text(\u0026#39;Define the Function\u0026#39;)\n        head_1.set_color(BLUE)\n        code_text_1 = \u0026#39;\u0026#39;\u0026#39;\n        def linreg(X, w, b):\n            return torch.matmul(X, w) + b\n        \u0026#39;\u0026#39;\u0026#39;\n        code_1 = Code(code=code_text_1,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head_2 = Text(\u0026#39;Define the Loss Function\u0026#39;)\n        head_2.set_color(BLUE)\n        code_text_2 = \u0026#39;\u0026#39;\u0026#39;\n        def squared_loss(y_hat, y):\n            return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n        \u0026#39;\u0026#39;\u0026#39;\n\n\n        code_2 = Code(code=code_text_2,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head = VGroup(head_1,code_1,head_2,code_2)\n        head.arrange(DOWN,buff=1)\n        self.play(Write(head))\n        self.wait(2)\n        self.play(Unwrite(head))\n\n        # 展示MSE--------------------------------------------------------------------------------------------    \n        head = MathTex(r\u0026#34;Lose~Function~MSE~:(y_i - \\hat{y}_i)^2\u0026#34;)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        self.play(Write(head))\n        axes = Axes(\n            x_range=[-10, 10, 2.5],\n            y_range=[0, 100, 20],\n            x_length=10,\n            y_length=5,\n            axis_config={\u0026#34;color\u0026#34;: GREEN},\n        )\n        \n        # 定义MSE函数\n        mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10])\n        mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10])\n        # 将元素添加到场景中\n        self.play(Create(axes),Create(mse_curve))\n        self.wait(2)\n        self.play(Uncreate(head))\n        head = Text(\u0026#34;The MSE Derivative indicates that loss will be increasing as it increase\u0026#34;,font_size=30)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        self.play(Write(head),Create(mse_der))\n        self.wait(3)\n        self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes))\n\n\n        # 展示SGD--------------------------------------------------------------------------------------------\n        head = Text(\u0026#34;Now Conduct the Optimization Method\u0026#34;)\n        head.move_to(UP*3)\n        head.set_color(BLUE)\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        def sgd(params, lr, batch_size):\n        with torch.no_grad():\n            for param in params:\n                param -= lr * param.grad / batch_size\n                param.grad.zero_()\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head_2 = Text(\u0026#39;Apply this optimization method for each batch\u0026#39;)\n        head_2.set_color(BLUE)\n        sgd = MathTex(r\u0026#34;(w,b)\\leftarrow (w,b)-\\eta g\u0026#34;)\n        main = VGroup(head,code,head_2,sgd)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(2)\n        self.play(Uncreate(main),run_time=0.1)\n        \n        # 计算梯度--------------------------------------------------------------------------------------------\n        head = Text(\u0026#34;Now Calculate the Gradient\u0026#34;)\n        head.set_color(BLUE)\n        head.move_to(UP*3)\n        grad = MathTex(r\u0026#34;\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w} \\left( y_i - (w x_i + b) \\right)^2\u0026#34;)\n        grad_1 = MathTex(r\u0026#34;= \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - (wx_i + b)) \\cdot (-x_i)\u0026#34;)\n        grad_2 = MathTex(r\u0026#34;= -\\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#34;)\n\n        main = VGroup(head,grad,grad_1,grad_2)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(2)\n        self.play(Uncreate(main),run_time=0.01)\n        head = Text(\u0026#34;Then apllies the formula for 1000/10=100 Times\u0026#34;,font_size=45)\n        head.set_color(BLUE)\n        grad = MathTex(r\u0026#39;w := w + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i) \\cdot x_i\u0026#39;)\n        grad_1 = MathTex(r\u0026#34;b := b + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\hat{y}_i)\u0026#34;)\n        main = VGroup(head,grad,grad_1)\n        main.arrange(DOWN,buff=0.7)\n        self.play(Write(main))\n        self.wait(3)\n        self.play(Uncreate(main),run_time=0.01)\n\n        # 总结--------------------------------------------------------------------------------------------\n        code_text = \u0026#39;\u0026#39;\u0026#39;\n        lr = 0.03\n        num_epochs = 3\n        net = linreg\n        loss = squared_loss\n\n        for epoch in range(num_epochs):\n            for X, y in data_iter(batch_size, features, labels):\n                l = loss(net(X, w, b), y)\n                l.sum().backward()\n                sgd([w, b], lr, batch_size)\n            with torch.no_grad():\n                train_l = loss(net(features, w, b), labels)\n                print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}\u0026#39;)\n        \u0026#39;\u0026#39;\u0026#39;\n        code = Code(code=code_text,insert_line_no=False,language=\u0026#34;Python\u0026#34;,font=\u0026#34;Monospace\u0026#34;)\n        head = Text(\u0026#34;Then applies the whole process in epochs and that\u0026#39;s linear regression\u0026#34;,font_size=30)\n        main = VGroup(head,code)\n        main.arrange(DOWN,buff=1)\n        self.play(Write(main))\n---\n\u003c/code\u003e\u003c/pre\u003e","title":"Linear Regression","type":"docs"},{"content":"","date":"Dec 27 2021","externalUrl":null,"permalink":"/tags/wangyiyun/","section":"Tags","summary":"","title":"Wangyiyun","type":"tags"},{"content":"\r","date":"Dec 27 2021","externalUrl":null,"permalink":"/blogs/wangyiyun/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/wangyiyun/1_hu17702828048897762413.jpg 330w,\r\n        /blogs/wangyiyun/1_hu360113685819379410.jpg 660w,\r\n        /blogs/wangyiyun/1_hu13608319589208325737.jpg 1024w,\r\n        /blogs/wangyiyun/1_hu6717855714386122734.jpg 2x\"\r\n        src=\"/blogs/wangyiyun/1_hu360113685819379410.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"网易云年度总结","type":"blogs"},{"content":"","date":"Dec 26 2021","externalUrl":null,"permalink":"/tags/shanghai/","section":"Tags","summary":"","title":"Shanghai","type":"tags"},{"content":"\r","date":"Dec 26 2021","externalUrl":null,"permalink":"/blogs/theband/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/theband/1_hu12727092779758697027.jpg 330w,\r\n        /blogs/theband/1_hu6728190700059376434.jpg 660w,\r\n        /blogs/theband/1_hu5333335250471856356.jpg 1024w,\r\n        /blogs/theband/1_hu8413573821294788432.jpg 2x\"\r\n        src=\"/blogs/theband/1_hu6728190700059376434.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"外滩2021","type":"blogs"},{"content":"\r","date":"Nov 1 2021","externalUrl":null,"permalink":"/blogs/desk2021/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FoxCsgo/FoxCsgo-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"2021 桌搭","type":"blogs"},{"content":"","date":"Nov 1 2021","externalUrl":null,"permalink":"/tags/csgp/","section":"Tags","summary":"","title":"Csgp","type":"tags"},{"content":"\r","date":"Oct 12 2021","externalUrl":null,"permalink":"/blogs/5e600/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg\r\n        class=\"my-0 rounded-md\"\r\n        loading=\"lazy\"\r\n        srcset=\"\r\n        /blogs/5e600/1_hu9666354151374759163.jpg 330w,\r\n        /blogs/5e600/1_hu16441751272499849929.jpg 660w,\r\n        /blogs/5e600/1_hu13471086248097805267.jpg 1024w,\r\n        /blogs/5e600/1_hu2577903920060232791.jpg 2x\"\r\n        src=\"/blogs/5e600/1_hu16441751272499849929.jpg\"\r\n        alt=\"Img\"\r\n      /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"5e 600分对决","type":"blogs"},{"content":"","date":"Oct 12 2021","externalUrl":null,"permalink":"/tags/desk/","section":"Tags","summary":"","title":"Desk","type":"tags"},{"content":"\r","date":"Sep 3 2021","externalUrl":null,"permalink":"/blogs/foxcsgo/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FoxCsgo/FoxCsgo-1.png\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"沙漠之狐","type":"blogs"},{"content":"","date":"Apr 28 2020","externalUrl":null,"permalink":"/tags/qiuqiu/","section":"Tags","summary":"","title":"Qiuqiu","type":"tags"},{"content":"","date":"Apr 28 2020","externalUrl":null,"permalink":"/tags/zhuzi/","section":"Tags","summary":"","title":"Zhuzi","type":"tags"},{"content":"\r","date":"Apr 28 2020","externalUrl":null,"permalink":"/blogs/qiuqiu/young/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Qiuqiu/Young/Young-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"小时候","type":"blogs"},{"content":"\r","date":"Apr 28 2020","externalUrl":null,"permalink":"/blogs/zhuzi/young/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-2.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-3.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-4.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\n\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/Zhuzi/Young/Young-5.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"小时候","type":"blogs"},{"content":"","date":"Jan 25 2020","externalUrl":null,"permalink":"/blogs/qiuqiu/","section":"Blogs","summary":"","title":"球球","type":"blogs"},{"content":"\r","date":"Dec 21 2018","externalUrl":null,"permalink":"/blogs/fundationchristams/","section":"Blogs","summary":"\u003cp\u003e\r\n    \u003cfigure\u003e\r\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" src=\"/blog/FundationChristams/FundationChristams-1.jpg\" alt=\"Img\" /\u003e\r\n      \r\n    \u003c/figure\u003e\r\n\u003c/p\u003e","title":"Fundation Christams","type":"blogs"},{"content":"","date":"Dec 21 2018","externalUrl":null,"permalink":"/blogs/zhuzi/","section":"Blogs","summary":"","title":"竹子","type":"blogs"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Special thanks to those for thier invaluable contributions\n","externalUrl":null,"permalink":"/credits/","section":"Credits","summary":"\u003cp\u003eSpecial thanks to those for thier invaluable contributions\u003c/p\u003e","title":"Credits","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/faker/","section":"Credits","summary":"","title":"Faker","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/ss/","section":"Credits","summary":"","title":"SS","type":"credits"},{"content":"123\nRecord some thinking\n","externalUrl":null,"permalink":"/thoughts/thethreebodyproblem/","section":"Thoughts","summary":"\u003cp\u003e123\u003c/p\u003e\n\u003cp\u003eRecord some thinking\u003c/p\u003e","title":"The Three Body Problem","type":"thoughts"},{"content":"","externalUrl":null,"permalink":"/thoughts/","section":"Thoughts","summary":"","title":"Thoughts","type":"thoughts"},{"content":"","externalUrl":null,"permalink":"/credits/%E6%B3%95%E8%80%81/","section":"Credits","summary":"","title":"法老","type":"credits"},{"content":"","externalUrl":null,"permalink":"/credits/%E9%82%93%E7%B4%AB%E6%A3%8B/","section":"Credits","summary":"","title":"邓紫棋","type":"credits"}]