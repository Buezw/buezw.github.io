<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Science on Buezwqwg</title>
    <link>https://buezw.github.io/tags/computer-science/</link>
    <description>Recent content in Computer Science on Buezwqwg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>youremail@example.com (Buezwqwg)</managingEditor>
    <webMaster>youremail@example.com (Buezwqwg)</webMaster>
    <copyright>© 2024 Buezwqwg</copyright>
    <lastBuildDate>Mon, 15 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://buezw.github.io/tags/computer-science/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>D2L 3.1 Linear Regression</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Last Edit 4/15/24&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.2 Object-Oriented Design for Implementation</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/</guid>
      <description>&lt;p&gt;从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.3 A concise implementation of linear regression</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/</guid>
      <description>&lt;p&gt;本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.4 Softmax Regression</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/</guid>
      <description>&lt;p&gt;回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.5 Image classification datasets</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/</guid>
      <description>&lt;p&gt;MNIST数据集 (&lt;a href=&#34;https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90&#34;title=&#34;LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., &amp;amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.&#34; target=&#34;_blank&#34;&gt;LeCun &lt;em&gt;et al.&lt;/em&gt;, 1998&lt;/a&gt;) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (&lt;a href=&#34;https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189&#34;title=&#34;Xiao, H., Rasul, K., &amp;amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.&#34; target=&#34;_blank&#34;&gt;Xiao &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;)。&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.6 Implementation of softmax regression from scratch</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/</guid>
      <description>&lt;h2 class=&#34;relative group&#34;&gt;3.6.1 初始化模型参数 
    &lt;div id=&#34;361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量&lt;/li&gt;
&lt;li&gt;在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;初始化权重w 
    &lt;div id=&#34;%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 class=&#34;relative group&#34;&gt;3.6.2 定义softmax操作 
    &lt;div id=&#34;362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现softmax操作由三个步骤组成&lt;/li&gt;
&lt;li&gt;对每个项求幂&lt;/li&gt;
&lt;li&gt;对每一行求和，得到其规范化常数&lt;/li&gt;
&lt;li&gt;每一行除以其规范化常数，保持结果的和为1
$$softmax(X)&lt;em&gt;{ij}=\frac{exp(X&lt;/em&gt;{ij})}{\sum_kexp(X_{ik})}$$&lt;/li&gt;
&lt;li&gt;分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自&lt;a href=&#34;https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29&#34; target=&#34;_blank&#34;&gt;统计物理学&lt;/a&gt;中一个模拟粒子群分布的方程&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;keepdim=True: 在进行张量操作时，保持原始张量的维度&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.1 MultilayerPerceptron</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/</guid>
      <description>&lt;p&gt;在了解多层感知机前，需要先了解[[Perceptron 感知机]]&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.1 MultilayerPerceptron</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/</guid>
      <description>&lt;p&gt;在了解多层感知机前，需要先了解[[Perceptron 感知机]]&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.3 Simple Implementation of Multilayer Perceptron</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/</guid>
      <description>&lt;p&gt;通过更高级的API进一步简洁训练过程&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;4.3.1 模型 
    &lt;div id=&#34;431-%E6%A8%A1%E5%9E%8B&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#431-%E6%A8%A1%E5%9E%8B&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;初始化神经网络&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction=&amp;#39;none&amp;#39;)
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
&lt;/code&gt;&lt;/pre&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.4 Model Selection, Underfitting, and Overfitting</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/</guid>
      <description>&lt;p&gt;深度学习的目的是发现Pattern，即做到模型的Generalization 泛化&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/feature.png" />
    </item>
    
    <item>
      <title>Chapter 3. Linear Neural Network</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/</guid>
      <description>&lt;hr&gt;</description>
      
    </item>
    
    <item>
      <title>Chapter 4. Multilayer Perceptron</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/</guid>
      <description>&lt;hr&gt;</description>
      
    </item>
    
    <item>
      <title>Dive Into Deep Learning</title>
      <link>https://buezw.github.io/docs/d2l_dive-into-deep-learning/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>https://buezw.github.io/docs/d2l_dive-into-deep-learning/</guid>
      <description>&lt;p&gt;《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. &lt;a href=&#34;https://zh-v2.d2l.ai/index.html&#34; target=&#34;_blank&#34;&gt;https://zh-v2.d2l.ai/index.html&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;‌ 
    &lt;div id=&#34;heading&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#heading&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;</description>
      
    </item>
    
  </channel>
</rss>
