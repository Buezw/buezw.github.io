<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Science on Buezwqwg</title>
    <link>http://localhost:1313/tags/computer-science/</link>
    <description>Recent content in Computer Science on Buezwqwg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>youremail@example.com (Buezwqwg)</managingEditor>
    <webMaster>youremail@example.com (Buezwqwg)</webMaster>
    <copyright>© 2024 Buezwqwg</copyright>
    <lastBuildDate>Mon, 04 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/computer-science/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Regression</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/linearregression/</link>
      <pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/linearregression/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Last Edit: 11/4/2024&lt;/p&gt;
&lt;/blockquote&gt;
&lt;video width=&#34;640&#34; height=&#34;360&#34; controls&gt;
  &lt;source src=&#34;LinearRegression.mp4&#34; type=&#34;video/mp4&#34;&gt;
  Your browser does not support the video tag.
&lt;/video&gt;
&lt;p&gt;Full Code is Provided&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-from&#34; data-lang=&#34;from&#34;&gt;import numpy as np
import torch
import random

class LinearRegression(Scene):
    def construct(self):
        def data_generator(w,b,num):
            X = torch.normal(0, 1, (num, len(w)))
            y = torch.matmul(X, w) + b
            y += torch.normal(0, 0.01, y.shape)
            return X, y.reshape((-1, 1))

        true_w = torch.tensor([2,-3.4])
        true_b = 4.2
        features, labels = data_generator(true_w,true_b,1000)
        normal_data = features[:,[0]].numpy()
        #plt.hist(normal_data,bins=100,density=True,color=&amp;#39;lightblue&amp;#39;)  
         

        head = Text(&amp;#34;Linear Regression - Buezwqwg&amp;#34;)
        head.set_color(BLUE)
        self.play(Create(head))

        head_0 = Text(&amp;#34;In one process of Linear Regression, there bascially includes 5 steps&amp;#34;,font_size=30)
        self.play(Uncreate(head),Write(head_0))
        self.play(head_0.animate.move_to(UP*3.5))
        head_1 = Text(&amp;#34;1. Initial Parameters&amp;#34;,font_size=30)
        head_2 = Text(&amp;#34;2. Defining Model and Loss Function&amp;#34;,font_size=30)
        head_3 = Text(&amp;#34;3. Optimization&amp;#34;,font_size=30)
        head_4 = Text(&amp;#34;4. Loop&amp;#34;,font_size=30)
        head = VGroup(head_1,head_2,head_3,head_4)
        head.arrange(DOWN)
        self.play(Write(head))

        # --------------------------------------------------------------------------------------------

        head_5 = Text(&amp;#34;In this animate, we start with generating the data&amp;#34;,font_size=30)
        head_5.move_to(UP*3.5)
        self.play(Uncreate(head),Uncreate(head_0),Write(head_5))
        code_text = &amp;#39;&amp;#39;&amp;#39;
        def data_generator(w, b, num):
            X = torch.normal(0, 1, (num, len(w)))
            y = torch.matmul(X, w) + b
            y += torch.normal(0, 0.01, y.shape)
            return X, y.reshape((-1, 1))
            
        true_w = torch.tensor([2,-3.4])
        true_b = 4.2
        features, labels = data_generator(true_w,true_b,1000)
        &amp;#39;&amp;#39;&amp;#39;
        code = Code(code=code_text,insert_line_no=False,language=&amp;#34;Python&amp;#34;,font=&amp;#34;Monospace&amp;#34;)
        self.play(Write(code),Uncreate(head_5),run_time=3)
        self.wait(3)
        self.play(Unwrite(code))
        axes = Axes(
            x_range=[-4, 4, 1],
            y_range=[0, 0.5, 0.1],
            axis_config={&amp;#34;color&amp;#34;: BLUE},
        ).add_coordinates()

        # 正态分布函数 y = (1/sqrt(2*pi)) * exp(-x^2 / 2)
        normal_curve = axes.plot(
            lambda x: (1 / (2 * PI) ** 0.5) * np.exp(-x**2 / 2),
            color=YELLOW
        )

        # 绘制均值为0的竖线
        mean_line = DashedLine(
            start=axes.c2p(0, 0),
            end=axes.c2p(0, (1 / (2 * PI) ** 0.5)),
            color=RED
        )

        # 添加图形和标注
        self.play(Create(axes))
        self.play(Create(normal_curve), Create(mean_line))
        
        # 标注均值和标准差
        mean_label = MathTex(r&amp;#34;\mu=0&amp;#34;).next_to(mean_line, DOWN)
        std_label = MathTex(r&amp;#34;\sigma=1&amp;#34;).next_to(normal_curve, UP, buff=0.5)
        self.play(Write(mean_label), Write(std_label))

        # 展示最终效果
        self.wait(2)
        self.play(Unwrite(mean_label),Unwrite(std_label),Uncreate(axes),Uncreate(normal_curve),Uncreate(mean_line))

        # --------------------------------------------------------------------------------------------

        head = Text(&amp;#34;Displaying the distribution of features&amp;#34;)
        feature_one = features[:,[0]].tolist()
        feature_two = features[:,[1]].tolist()
        labels = labels.tolist()
        axes_1 = Axes(
            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=5,  # x轴的长度
            y_length=5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        axes_2 = Axes(
            x_range=[min(feature_two)[0], max(feature_two)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=5,  # x轴的长度
            y_length=5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        axes = VGroup(axes_1,axes_2)
        axes.arrange(RIGHT,buff=1)
        self.play(Create(axes))

        points_1 = []
        for i in range(len(labels)):
            dot_position = axes_1.coords_to_point(feature_one[i][0], labels[i][0])
            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_1 = [Create(dot) for dot in points_1]
        points_2 = []
        for i in range(len(labels)):
            dot_position = axes_2.coords_to_point(feature_two[i][0], labels[i][0])
            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_2 = [Create(dot) for dot in points_2]
        self.play(Succession(*animations_1, lag_ratio=0.005),Succession(*animations_2, lag_ratio=0.005))

        # 抽取样本-------------------------------------------------------------------------------------------- 

        head = Text(&amp;#39;Shuffle the data and divided into samples(batches)&amp;#39;,font_size=30)
        self.play(Uncreate(axes),Write(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)))
        head.move_to(UP*3.5)
        code_text = &amp;#39;&amp;#39;&amp;#39;
        def data_iter(batch_size,features,labels):
            num = len(features)
            index = list(range(num))
            random.shuffle(index)
            for i in range(0,num,batch_size):
                batch_index = torch.tensor(index[i:min(i+batch_size,num)])
                yield features[batch_index], labels[batch_index]
        &amp;#39;&amp;#39;&amp;#39;
        code = Code(code=code_text,insert_line_no=False,language=&amp;#34;Python&amp;#34;,font=&amp;#34;Monospace&amp;#34;)
        self.play(Write(code))
        self.wait(2)
        self.play(Uncreate(code),Unwrite(head))

        def data_iter(batch_size,features,labels):
            num = len(features)
            index = list(range(num))
            random.shuffle(index)
            for i in range(0,num,batch_size):
                batch_index = torch.tensor(index[i:min(i+batch_size,num)])
                return batch_index.tolist()



        axes_1 = Axes(
            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=2.5,  # x轴的长度
            y_length=2.5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        

        axes_2 = Axes(
            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=2.5,  # x轴的长度
            y_length=2.5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        
        axes_3 = Axes(
            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=2.5,  # x轴的长度
            y_length=2.5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        axes_4 = Axes(
            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=2.5,  # x轴的长度
            y_length=2.5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        axes_5 = Axes(
            x_range=[min(feature_one)[0], max(feature_one)[0], 1],  # x轴范围：从-5到5，步长为1
            y_range=[min(labels)[0], max(labels)[0], 5],  # y轴范围：从-3到3，步长为1
            x_length=2.5,  # x轴的长度
            y_length=2.5,  # y轴的长度
            axis_config={&amp;#34;color&amp;#34;: BLUE},  # 坐标轴的颜色
        )
        axes = VGroup(axes_1,axes_2,axes_3,axes_4,axes_5)
        axes.arrange(RIGHT)

        sample_1 = data_iter(10,features,labels)
        points_1 = []
        for i in sample_1:
            dot_position = axes_1.coords_to_point(features[i][0],labels[i][0])
            points_1.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_1 = [Create(dot) for dot in points_1]

        sample_2 = data_iter(10,features,labels)
        points_2 = []
        for i in sample_2:
            dot_position = axes_2.coords_to_point(features[i][0],labels[i][0])
            points_2.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_2 = [Create(dot) for dot in points_2]

        sample_3 = data_iter(10,features,labels)
        points_3 = []
        for i in sample_3:
            dot_position = axes_3.coords_to_point(features[i][0],labels[i][0])
            points_3.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_3 = [Create(dot) for dot in points_3]

        sample_4 = data_iter(10,features,labels)
        points_4 = []
        for i in sample_4:
            dot_position = axes_4.coords_to_point(features[i][0],labels[i][0])
            points_4.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_4 = [Create(dot) for dot in points_4]

        sample_5 = data_iter(10,features,labels)
        points_5 = []
        for i in sample_5:
            dot_position = axes_5.coords_to_point(features[i][0],labels[i][0])
            points_5.append(Dot(point=dot_position, radius=0.05, color=YELLOW))
        animations_5 = [Create(dot) for dot in points_5]   
        head = Text(&amp;#34;Display five of Sample Batches (Batch Size = 10)&amp;#34;,font_size=30)
        head.set_color(BLUE)
        head.move_to(UP*2.5)
        self.play(Write(head))
        self.play(Create(axes),Succession(*animations_1, lag_ratio=0.05),Succession(*animations_2, lag_ratio=0.05),Succession(*animations_3, lag_ratio=0.05),Succession(*animations_4, lag_ratio=0.05),Succession(*animations_5, lag_ratio=0.05))
        self.wait(3)
        self.play(Uncreate(axes),Uncreate(head),Uncreate(VGroup(*points_1)),Uncreate(VGroup(*points_2)),Uncreate(VGroup(*points_3)),Uncreate(VGroup(*points_4)),Uncreate(VGroup(*points_5)))

        # 定义模型-------------------------------------------------------------------------------------------- 

        head_1 = Text(&amp;#39;Define the Function&amp;#39;)
        head_1.set_color(BLUE)
        code_text_1 = &amp;#39;&amp;#39;&amp;#39;
        def linreg(X, w, b):
            return torch.matmul(X, w) + b
        &amp;#39;&amp;#39;&amp;#39;
        code_1 = Code(code=code_text_1,insert_line_no=False,language=&amp;#34;Python&amp;#34;,font=&amp;#34;Monospace&amp;#34;)
        head_2 = Text(&amp;#39;Define the Loss Function&amp;#39;)
        head_2.set_color(BLUE)
        code_text_2 = &amp;#39;&amp;#39;&amp;#39;
        def squared_loss(y_hat, y):
            return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
        &amp;#39;&amp;#39;&amp;#39;


        code_2 = Code(code=code_text_2,insert_line_no=False,language=&amp;#34;Python&amp;#34;,font=&amp;#34;Monospace&amp;#34;)
        head = VGroup(head_1,code_1,head_2,code_2)
        head.arrange(DOWN,buff=1)
        self.play(Write(head))
        self.wait(2)
        self.play(Unwrite(head))

        # 展示MSE--------------------------------------------------------------------------------------------    
        head = MathTex(r&amp;#34;Lose~Function~MSE~:(y_i - \hat{y}_i)^2&amp;#34;)
        head.set_color(BLUE)
        head.move_to(UP*3)
        self.play(Write(head))
        axes = Axes(
            x_range=[-10, 10, 2.5],
            y_range=[0, 100, 20],
            x_length=10,
            y_length=5,
            axis_config={&amp;#34;color&amp;#34;: GREEN},
        )
        
        # 定义MSE函数
        mse_curve = axes.plot(lambda x: (x**2), color=BLUE, x_range=[-10, 10])
        mse_der = axes.plot(lambda x: (2*x), color=RED, x_range=[-10, 10])
        # 将元素添加到场景中
        self.play(Create(axes),Create(mse_curve))
        self.wait(2)
        self.play(Uncreate(head))
        head = Text(&amp;#34;The MSE Derivative indicates that loss will be increasing as it increase&amp;#34;,font_size=30)
        head.set_color(BLUE)
        head.move_to(UP*3)
        self.play(Write(head),Create(mse_der))
        self.wait(3)
        self.play(Uncreate(head),Uncreate(mse_der),Uncreate(mse_curve),Uncreate(axes))


        # 展示SGD--------------------------------------------------------------------------------------------
        head = Text(&amp;#34;Now Conduct the Optimization Method&amp;#34;)
        head.move_to(UP*3)
        head.set_color(BLUE)
        code_text = &amp;#39;&amp;#39;&amp;#39;
        def sgd(params, lr, batch_size):
        with torch.no_grad():
            for param in params:
                param -= lr * param.grad / batch_size
                param.grad.zero_()
        &amp;#39;&amp;#39;&amp;#39;
        code = Code(code=code_text,insert_line_no=False,language=&amp;#34;Python&amp;#34;,font=&amp;#34;Monospace&amp;#34;)
        head_2 = Text(&amp;#39;Apply this optimization method for each batch&amp;#39;)
        head_2.set_color(BLUE)
        sgd = MathTex(r&amp;#34;(w,b)\leftarrow (w,b)-\eta g&amp;#34;)
        main = VGroup(head,code,head_2,sgd)
        main.arrange(DOWN,buff=0.7)
        self.play(Write(main))
        self.wait(2)
        self.play(Uncreate(main),run_time=0.1)
        
        # 计算梯度--------------------------------------------------------------------------------------------
        head = Text(&amp;#34;Now Calculate the Gradient&amp;#34;)
        head.set_color(BLUE)
        head.move_to(UP*3)
        grad = MathTex(r&amp;#34;\frac{\partial \text{MSE}}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial w} \left( y_i - (w x_i + b) \right)^2&amp;#34;)
        grad_1 = MathTex(r&amp;#34;= \frac{1}{n} \sum_{i=1}^{n} 2(y_i - (wx_i + b)) \cdot (-x_i)&amp;#34;)
        grad_2 = MathTex(r&amp;#34;= -\frac{1}{n} \sum_{i=1}^{n} 2(y_i - \hat{y}_i) \cdot x_i&amp;#34;)

        main = VGroup(head,grad,grad_1,grad_2)
        main.arrange(DOWN,buff=0.7)
        self.play(Write(main))
        self.wait(2)
        self.play(Uncreate(main),run_time=0.01)
        head = Text(&amp;#34;Then apllies the formula for 1000/10=100 Times&amp;#34;,font_size=45)
        head.set_color(BLUE)
        grad = MathTex(r&amp;#39;w := w + \eta \cdot \frac{1}{n} \sum_{i=1}^{n} 2(y_i - \hat{y}_i) \cdot x_i&amp;#39;)
        grad_1 = MathTex(r&amp;#34;b := b + \eta \cdot \frac{1}{n} \sum_{i=1}^{n} 2(y_i - \hat{y}_i)&amp;#34;)
        main = VGroup(head,grad,grad_1)
        main.arrange(DOWN,buff=0.7)
        self.play(Write(main))
        self.wait(3)
        self.play(Uncreate(main),run_time=0.01)

        # 总结--------------------------------------------------------------------------------------------
        code_text = &amp;#39;&amp;#39;&amp;#39;
        lr = 0.03
        num_epochs = 3
        net = linreg
        loss = squared_loss

        for epoch in range(num_epochs):
            for X, y in data_iter(batch_size, features, labels):
                l = loss(net(X, w, b), y)
                l.sum().backward()
                sgd([w, b], lr, batch_size)
            with torch.no_grad():
                train_l = loss(net(features, w, b), labels)
                print(f&amp;#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}&amp;#39;)
        &amp;#39;&amp;#39;&amp;#39;
        code = Code(code=code_text,insert_line_no=False,language=&amp;#34;Python&amp;#34;,font=&amp;#34;Monospace&amp;#34;)
        head = Text(&amp;#34;Then applies the whole process in epochs and that&amp;#39;s linear regression&amp;#34;,font_size=30)
        main = VGroup(head,code)
        main.arrange(DOWN,buff=1)
        self.play(Write(main))
---
&lt;/code&gt;&lt;/pre&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/linearregression/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.1 Linear Regression</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Last Edit 4/15/24&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regression 回归，是能为一个或多个自变量与因变量之间关系建模的一种方式&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.1_linearregression/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.2 Object-Oriented Design for Implementation</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/</guid>
      <description>&lt;p&gt;从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.2_object-orienteddesignforimplementation/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.3 A concise implementation of linear regression</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/</guid>
      <description>&lt;p&gt;本节将介绍如何通过使用深度学习框架来简洁地实现[[3.2_Object-OrientedDesignforImplementation]]中的线性回归模型&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.3_syntheticregressiondat/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.4 Softmax Regression</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/</guid>
      <description>&lt;p&gt;回归可以用于预测_多少_的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.4_softmaxregression/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.5 Image classification datasets</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/</guid>
      <description>&lt;p&gt;MNIST数据集 (&lt;a href=&#34;https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90&#34;title=&#34;LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., &amp;amp; others. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.&#34; target=&#34;_blank&#34;&gt;LeCun &lt;em&gt;et al.&lt;/em&gt;, 1998&lt;/a&gt;) 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集 (&lt;a href=&#34;https://zh-v2.d2l.ai/chapter_references/zreferences.html#id189&#34;title=&#34;Xiao, H., Rasul, K., &amp;amp; Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.&#34; target=&#34;_blank&#34;&gt;Xiao &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;)。&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.5_imageclassificationdatasets/feature.png" />
    </item>
    
    <item>
      <title>D2L 3.6 Implementation of softmax regression from scratch</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/</guid>
      <description>&lt;h2 class=&#34;relative group&#34;&gt;3.6.1 初始化模型参数 
    &lt;div id=&#34;361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#361-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是28×28的图像。 本节将展平每个图像，把它们看作长度为784的向量&lt;/li&gt;
&lt;li&gt;在3.5中，我们选择了一个拥有10个类别的数据集，所以softmax网络的输出维度为10&lt;/li&gt;
&lt;/ul&gt;


&lt;h3 class=&#34;relative group&#34;&gt;初始化权重w 
    &lt;div id=&#34;%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8Dw&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;与线性回归一样，我们使用正态分布初始化权重w，偏置初始化为0&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 class=&#34;relative group&#34;&gt;3.6.2 定义softmax操作 
    &lt;div id=&#34;362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#362-%E5%AE%9A%E4%B9%89softmax%E6%93%8D%E4%BD%9C&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现softmax操作由三个步骤组成&lt;/li&gt;
&lt;li&gt;对每个项求幂&lt;/li&gt;
&lt;li&gt;对每一行求和，得到其规范化常数&lt;/li&gt;
&lt;li&gt;每一行除以其规范化常数，保持结果的和为1
$$softmax(X)&lt;em&gt;{ij}=\frac{exp(X&lt;/em&gt;{ij})}{\sum_kexp(X_{ik})}$$&lt;/li&gt;
&lt;li&gt;分母或规范化常数，有时也称为_配分函数_（其对数称为对数-配分函数）。 该名称来自&lt;a href=&#34;https://en.wikipedia.org/wiki/Partition_function_%28statistical_mechanics%29&#34; target=&#34;_blank&#34;&gt;统计物理学&lt;/a&gt;中一个模拟粒子群分布的方程&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;keepdim=True: 在进行张量操作时，保持原始张量的维度&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/3.6_implementationofsoftmaxregressionfromscratch/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.1 MultilayerPerceptron</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/</guid>
      <description>&lt;p&gt;在了解多层感知机前，需要先了解[[Perceptron 感知机]]&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.2_implementationofmultilayerperceptronfromscratch/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.1 MultilayerPerceptron</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/</guid>
      <description>&lt;p&gt;在了解多层感知机前，需要先了解[[Perceptron 感知机]]&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.3_simpleimplementationofmultilayerperceptron/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.3 Simple Implementation of Multilayer Perceptron</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/</guid>
      <description>&lt;p&gt;通过更高级的API进一步简洁训练过程&lt;/p&gt;


&lt;h1 class=&#34;relative group&#34;&gt;4.3.1 模型 
    &lt;div id=&#34;431-%E6%A8%A1%E5%9E%8B&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#431-%E6%A8%A1%E5%9E%8B&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h1&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;初始化神经网络&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction=&amp;#39;none&amp;#39;)
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
&lt;/code&gt;&lt;/pre&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.1_multilayerperceptron/feature.png" />
    </item>
    
    <item>
      <title>D2L 4.4 Model Selection, Underfitting, and Overfitting</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/</guid>
      <description>&lt;p&gt;深度学习的目的是发现Pattern，即做到模型的Generalization 泛化&lt;/p&gt;</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/4.4_modelselectionunderfittingandoverfitting/feature.png" />
    </item>
    
    <item>
      <title>Chapter 3. Linear Neural Network</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter3linearneuralnetwork/</guid>
      <description>&lt;hr&gt;</description>
      
    </item>
    
    <item>
      <title>Chapter 4. Multilayer Perceptron</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/d2l_chapter4multilayerperceptron/</guid>
      <description>&lt;hr&gt;</description>
      
    </item>
    
    <item>
      <title>Dive Into Deep Learning</title>
      <link>http://localhost:1313/docs/d2l_dive-into-deep-learning/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <author>youremail@example.com (Buezwqwg)</author>
      <guid>http://localhost:1313/docs/d2l_dive-into-deep-learning/</guid>
      <description>&lt;p&gt;《动手学深度学习》 — 动手学深度学习 2.0.0 documentation. (2023). Zh-V2.D2l.ai. &lt;a href=&#34;https://zh-v2.d2l.ai/index.html&#34; target=&#34;_blank&#34;&gt;https://zh-v2.d2l.ai/index.html&lt;/a&gt;&lt;/p&gt;


&lt;h2 class=&#34;relative group&#34;&gt;‌ 
    &lt;div id=&#34;heading&#34; class=&#34;anchor&#34;&gt;&lt;/div&gt;
    
    &lt;span
        class=&#34;absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100&#34;&gt;
        &lt;a class=&#34;group-hover:text-primary-300 dark:group-hover:text-neutral-700&#34;
            style=&#34;text-decoration-line: none !important;&#34; href=&#34;#heading&#34; aria-label=&#34;Anchor&#34;&gt;#&lt;/a&gt;
    &lt;/span&gt;        
    
&lt;/h2&gt;</description>
      
    </item>
    
  </channel>
</rss>
